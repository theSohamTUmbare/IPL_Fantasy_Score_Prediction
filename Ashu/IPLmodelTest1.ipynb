{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "640f08c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bcf724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    PLAYERS_SIZE: int = 0\n",
    "    CONTEXT_LEN: int = 16  ## predict the 21st match by the 20\n",
    "    PERFORMANCE_INPUT_DIM = 23\n",
    "    PERFORMANCE_EMBD_DIM: int = 128\n",
    "    \n",
    "    PLAYER_INPUT_DIM: int = 25   ## univpalyer\n",
    "    MATCH_INPUT_EMBD: int = 14   ## match_info\n",
    "    NUM_EPOCHS: int = 100\n",
    "    LEARNING_RATE: float = 1e-3\n",
    "    BATCH_SIZE: int = 32\n",
    "    DEVICE: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # MODEL_SAVE_PATH: str = \"/kaggle/working\"\n",
    "    # BASE_DIR: str = '/kaggle/input/flickr8k/'\n",
    "    # CROSSATT_NUM_HEADS: int = 8\n",
    "    CLS_INIT_STD: float = 0.02    ## <CLS> token initialized with std 0.02 from the mean=0\n",
    "    TEST_DATASET_SIZE: int = 180\n",
    "    IDLE_DEVICE: str = 'cpu'\n",
    "    ACCUMULATION_STEPS = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b568c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb947dc",
   "metadata": {},
   "source": [
    "## Defination of **Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01598c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "150b0391",
   "metadata": {},
   "source": [
    "## Data <br>\n",
    " Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a857bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "class PlayerMatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for autoregressive next-match prediction.\n",
    "\n",
    "    For each sample:\n",
    "      1. Given a player_id, retrieve universal player features from universal_player.csv.\n",
    "         (The 'player_id' and 'cricinfo_id' columns are dropped from the input features.)\n",
    "      2. Retrieve all matches for that player from a directory of player match files\n",
    "         (each file is named \"{player_id}.csv\" and contains the matches for that player,\n",
    "         already sorted by date).\n",
    "         The performance columns include:\n",
    "           batting_position, runs, balls, fours, sixes, strike_rate, overs, total_balls, dots,\n",
    "           maidens, conceded, fours_conceded, sixes_conceded, wickets, LBW, Bowled, noballs,\n",
    "           wides, economy_rate, catches, stumping, direct_hit, indirect_hit, strike_rate_fp,\n",
    "           batting_fp, bowling_fp, fielding_fp, total_fp.\n",
    "         We drop 'match_id' and the fantasy-breakdown columns\n",
    "         ('strike_rate_fp', 'batting_fp', 'bowling_fp', 'fielding_fp', 'total_fp') when forming the input vector.\n",
    "      3. Randomly sample a contiguous window of (context_len + 1) matches for this player.\n",
    "         The first context_len matches serve as input and matches 2 ... context_len+1 yield the target fantasy scores.\n",
    "      4. For each match in the window:\n",
    "         - Load the corresponding match players CSV from the match_players folder (file: '{match_id}.csv').\n",
    "         - Determine the player's team for that match and then separate player_ids into:\n",
    "              team1_ids: those belonging to the same team as the player,\n",
    "              team2_ids: those belonging to the other team.\n",
    "         - Retrieve their universal features from universal_player.csv.\n",
    "         - Retrieve match info from a single match_info CSV (by matching on match_id).\n",
    "      5. Return a dictionary containing:\n",
    "         - 'player_id': the player's id.\n",
    "         - 'univ_features': the player's universal features.\n",
    "         - 'context_matches': a numpy array of performance features for the context matches.\n",
    "         - 'target_scores': a numpy array of target fantasy scores (for matches 2 ... context_len+1).\n",
    "         - 'team1_players': list (per match) of team1 players' universal features.\n",
    "         - 'team2_players': list (per match) of team2 players' universal features.\n",
    "         - 'match_info': list (per match) of match info dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, universal_player_csv, player_matches_dir, match_players_dir, match_info_csv, context_len=25, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            universal_player_csv (str): Path to universal_player.csv.\n",
    "            player_matches_dir (str): Directory containing CSV files for each player's matches (named '{player_id}.csv').\n",
    "            match_players_dir (str): Directory containing match_players CSV files.\n",
    "            match_info_csv (str): Path to the CSV file containing match info for all matches.\n",
    "            context_len (int): Number of context matches to use as input \n",
    "                                (target will be matches 2 ... context_len+1).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.context_len = context_len\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load universal player features and set player_id as index.\n",
    "        self.univ_df = pd.read_csv(universal_player_csv)\n",
    "        self.univ_df = self.univ_df.set_index('player_id')\n",
    "        # Drop 'cricinfo_id' as it is redundant.\n",
    "        self.univ_features = self.univ_df.drop(columns=['cricinfo_id'], errors='ignore')\n",
    "\n",
    "        # Instead of a single file for player matches, we assume a directory where each player's\n",
    "        # matches are stored in a file named '{player_id}.csv'. We build a list of valid player_ids\n",
    "        # by checking which player match files exist and ensuring they have at least (context_len+1) rows.\n",
    "        self.player_matches_dir = player_matches_dir\n",
    "        self.player_ids = []\n",
    "        self.player_match_data = {}  # Key: player_id, Value: DataFrame of that player's matches.\n",
    "        for player_id in self.univ_features.index:\n",
    "            match_file = os.path.join(player_matches_dir, f\"{player_id}.csv\")\n",
    "            if os.path.exists(match_file):\n",
    "                df_matches = pd.read_csv(match_file)\n",
    "                # Assume the matches in this file are already sorted by date.\n",
    "                if len(df_matches) >= (self.context_len + 1):\n",
    "                    self.player_ids.append(player_id)\n",
    "                    self.player_match_data[player_id] = df_matches\n",
    "\n",
    "        self.match_players_dir = match_players_dir\n",
    "\n",
    "        # Load the single match_info CSV and set match_id as index for fast lookup.\n",
    "        self.match_info_df = pd.read_csv(match_info_csv)\n",
    "        self.match_info_df = self.match_info_df.set_index('match_id')\n",
    "        # Optionally drop columns not needed.\n",
    "        self.match_info_df = self.match_info_df.drop(columns=['team1', 'team2', 'toss_winner', \"toss_decision\", \"winner\"], errors='ignore')\n",
    "        #Todo don't just drop them we need to infer from them in sense of player's team ->winner 1 else 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.player_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a dictionary with:\n",
    "          - 'player_id': the player's id.\n",
    "          - 'univ_features': universal features for the player.\n",
    "          - 'context_matches': a numpy array of performance features for the context matches.\n",
    "          - 'target_scores': a numpy array of target fantasy scores (for matches 2 ... context_len+1).\n",
    "          - 'team1_players': list (per match) of team1 players' universal features.\n",
    "          - 'team2_players': list (per match) of team2 players' universal features.\n",
    "          - 'match_info': list (per match) of match info dictionaries.\n",
    "        \"\"\"\n",
    "        #ToDO not just total_fantasy score, rather a weighted sum of error for batting_bowling fp's, fielding_fp with lesser weightage to the overall total_fp  \n",
    "        # Select the player.\n",
    "        player_id = self.player_ids[idx]\n",
    "        # Retrieve the player's universal features as a numpy array.\n",
    "        player_univ = self.univ_features.loc[player_id].values.astype(float)\n",
    "        \n",
    "        # Load this player's matches DataFrame from the pre-loaded dictionary.\n",
    "        df_matches = self.player_match_data[player_id]\n",
    "        total_matches = len(df_matches)\n",
    "        # Randomly select a contiguous window of (context_len + 1) matches.\n",
    "        start_idx = np.random.randint(1, total_matches - self.context_len+1)\n",
    "        window_df = df_matches.iloc[start_idx-1 : start_idx + self.context_len]\n",
    "\n",
    "        # For input, drop the unused columns.\n",
    "        exclude_cols = [ 'teamname','match_id',\n",
    "                        'strike_rate_fp', 'batting_fp', 'bowling_fp', 'fielding_fp', 'total_fp']\n",
    "        # Extract target fantasy scores (from column 'total_fp') for matches 2 ... context_len+1.\n",
    "        target_scores = window_df.iloc[1:self.context_len+1][['batting_fp', 'bowling_fp', 'fielding_fp','total_fp']].values.astype(float)\n",
    "        #Todo account for all sub_fp's (stage -2) #Done\n",
    "        # Extract performance features for input matches.\n",
    "        context_matches = window_df.iloc[:self.context_len].drop(columns=exclude_cols, errors='ignore')\n",
    "        \n",
    "\n",
    "        # For each match in the window, retrieve team players and match info.\n",
    "        # team1_players_list = [] \n",
    "        team2_players_list = []\n",
    "        match_info_list = []\n",
    "        for _, match in window_df.iterrows():\n",
    "            match_id = match['match_id']\n",
    "            # Load match players file (expects a file named \"{match_id}.csv\").\n",
    "            match_players_file = os.path.join(self.match_players_dir, f\"{match_id}.csv\")\n",
    "            match_players_df = pd.read_csv(match_players_file)\n",
    "            ## series of team_name\n",
    "            player_team_series = match_players_df[\n",
    "                match_players_df['player_id'] == player_id]['Team']\n",
    "            #? rather we can refer it from window_df, there in the excluded_cols same as match['team_name']\n",
    "            if not player_team_series.empty:\n",
    "                player_team = player_team_series.iloc[0]\n",
    "            else:\n",
    "                player_team = match_players_df.iloc[0]['Team'] #?why's this, though it won't execute\n",
    "            \n",
    "            # Split player_ids into two groups based on the player's team.\n",
    "            #! we just need the team2 player_ids (we are omitting the intra-team interactions...)\n",
    "            team2_ids = match_players_df[\n",
    "                match_players_df['Team'] != player_team]['player_id'].tolist()\n",
    "\n",
    "            # Retrieve universal features for these players.\n",
    "            # team1_features = self.univ_features.reindex(team1_ids).dropna().values.astype(float)\n",
    "            team2_features = self.univ_features.reindex(team2_ids).dropna().values.astype(float)\n",
    "            # team1_players_list.append(team1_features)\n",
    "            team2_players_list.append(team2_features)\n",
    "            \n",
    "            # Retrieve match info using match_id from the single match_info DataFrame.\n",
    "            if match_id in self.match_info_df.index:\n",
    "                match_info_dict = self.match_info_df.loc[match_id].to_dict()\n",
    "            else:\n",
    "                match_info_dict = {}  #TODO how we are going to handle this scenario \n",
    "            match_info_list.append(match_info_dict)\n",
    "\n",
    "        # --- START OF CHANGES: convert to properly shaped torch.Tensors --\n",
    "        \n",
    "        # # universal features\n",
    "        univ_features = torch.tensor(player_univ, dtype=torch.float32)  # (feat_dim,)\n",
    "\n",
    "        # context matches\n",
    "        context_matches = context_matches.drop(columns = 'teamname',errors = 'ignore')\n",
    "        context_matches = torch.tensor(context_matches.values.astype(float),\n",
    "                                       dtype=torch.float32)  # (context_len, perf_dim)\n",
    "\n",
    "        # target scores\n",
    "        target_scores = torch.tensor(target_scores, dtype=torch.float32)  # (context_len,)\n",
    "\n",
    "        # team1 players: pad to max players across the window, then stack\n",
    "        # max1 = max(arr.shape[0] for arr in team1_players_list)\n",
    "        # feat_dim = team1_players_list[0].shape[1] if max1>0 else 0\n",
    "        # padded1 = [\n",
    "        #     np.pad(arr, ((0, max1 - arr.shape[0]), (0, 0)), mode='constant')\n",
    "        #     for arr in team1_players_list\n",
    "        # ]\n",
    "        # team1_players = torch.tensor(np.stack(padded1), dtype=torch.float32)\n",
    "        # shape: (context_len+1, max1, feat_dim)\n",
    "\n",
    "        # team2 players: same\n",
    "        max2 = max(arr.shape[0] for arr in team2_players_list)\n",
    "        feat_dim2 = team2_players_list[0].shape[1] if max2>0 else 0\n",
    "        padded2 = [\n",
    "            np.pad(arr, ((0, max2 - arr.shape[0]), (0, 0)), mode='constant')\n",
    "            for arr in team2_players_list\n",
    "        ]\n",
    "        team2_players = torch.tensor(np.stack(padded2), dtype=torch.float32)\n",
    "        # shape: (context_len+1, max2, feat_dim2)\n",
    "        #? wht's this\n",
    "\n",
    "        # match_info: convert list of dicts to array in fixed key order\n",
    "        keys = list(self.match_info_df.columns)\n",
    "        info_arr = np.stack([[d.get(k, 0.0) for k in keys] for d in match_info_list])\n",
    "        match_info = torch.tensor(info_arr, dtype=torch.float32)\n",
    "        # shape: (context_len+1, len(keys))\n",
    "\n",
    "        # --- END OF CHANGES ---\n",
    "\n",
    "        sample = {\n",
    "            'player_id': player_id,\n",
    "            'univ_features': univ_features,\n",
    "            'context_matches': context_matches,\n",
    "            'target_scores': target_scores,\n",
    "            # 'team1_players': team1_players[:, :11, :],\n",
    "            'team2_players': team2_players[:, :11, :],\n",
    "            'match_info': match_info\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "413eb98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define file paths (update these paths as needed for your folder structure)\n",
    "universal_player_csv = r'C:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\\cleaned_universal_player.csv'\n",
    "player_matches_dir = r'C:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\\Test_1\\Cleaned_Global_player_csvs'  # Contains files named like {player_id}.csv (each containing that player's matches)\n",
    "match_players_dir = r'C:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\\Test_1\\processed_GlobalMatchrecords'    # Contains files like {match_id}.csv\n",
    "match_info_csv = r'C:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\\Test_1\\cleaned_matchinfo_without_venue_with_updated_match_number.csv'               # Single CSV containing all match info\n",
    "\n",
    "# Define the context length (number of matches to use as context)\n",
    "# For example, if config.CONTEXT_LEN is defined in your config module:\n",
    "# config.CONTEXT_LEN = 5\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = PlayerMatchDataset(\n",
    "    universal_player_csv=universal_player_csv,\n",
    "    player_matches_dir=player_matches_dir,  # This parameter may be ignored if you use the directory version\n",
    "    match_players_dir=match_players_dir,\n",
    "    match_info_csv=match_info_csv,\n",
    "    context_len=config.CONTEXT_LEN\n",
    ")\n",
    "\n",
    "# def get_shape(lst):\n",
    "#     shape = []\n",
    "#     while isinstance(lst, list):\n",
    "#         shape.append(len(lst))\n",
    "#         if len(lst) == 0:\n",
    "#             break\n",
    "#         lst = lst[0]\n",
    "#     return tuple(shape)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce5ffd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player IDs: ['95fcbfb7', 'b61a3e1a', 'f05fe9b1', '5d77a96a', '3d8feaf8', '69e23303', '8dd02a98', 'ad286dcc', '955e8e86', '791c088d', '3560a786', 'fb0d68b4', '5451a2c1', 'f8dcfe4e', '36a0a88c', 'e635a5dd', 'a9d788e3', '1bdbf53b', '85ebf2be', '3c6ffae8', '6ceb94d0', '07687c15', 'ce2b42ae', 'b93a5d0b', '33ab1f1c', 'bdadf7da', '0d677597', '903560ed', 'f5180fe6', '74bace71', '1b04e02b', 'd8bee9a1']\n",
      "Universal features shape: torch.Size([32, 25])\n",
      "Context matches shape: torch.Size([32, 16, 23])\n",
      "Target scores shape: torch.Size([32, 16, 4])\n",
      "Number of matches in team2_players (per sample): torch.Size([32, 17, 11, 25])\n",
      "Number of matches in match_info (per sample): torch.Size([32, 17, 14])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Iterate through one batch to check the output shapes and values\n",
    "for batch in dataloader:\n",
    "    print(\"Player IDs:\", batch['player_id'])  # list of player_ids (length = batch_size)\n",
    "    \n",
    "    print(\"Universal features shape:\", (batch['univ_features']).shape)  # e.g., (batch_size, num_features)\n",
    "    \n",
    "    print(\"Context matches shape:\", batch['context_matches'].shape)  # e.g., (batch_size, context_len, performance_feature_dim)\n",
    "    \n",
    "    print(\"Target scores shape:\", batch['target_scores'].shape)      # e.g., (batch_size, context_len)\n",
    "    \n",
    "    # The following are lists of length (context_len+1); each element is a numpy array.\n",
    "    # team1_players_tensor = list_of_tensors_to_3d(batch['team1_players'])\n",
    "    # team1_players_tensor = team1_players_tensor.squeeze(2)\n",
    "    # print(\"Number of matches in team1_players (per sample):\", batch['team1_players'].shape)\n",
    "    \n",
    "    # team2_players_tensor = list_of_tensors_to_3d(batch['team2_players'])\n",
    "    # team2_players_tensor = team2_players_tensor.squeeze(2)\n",
    "    print(\"Number of matches in team2_players (per sample):\", batch['team2_players'].shape)\n",
    "    \n",
    "    # match_info_tensor = list_of_dicts_to_tensor(batch['match_info'])\n",
    "    print(\"Number of matches in match_info (per sample):\", (batch['match_info']).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13d666ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8c4d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_tensors_to_3d(tensor_list):\n",
    "    \"\"\"\n",
    "    Converts a list of tensors into a 3D tensor with shape:\n",
    "    (1, number_of_tensors, *inner_tensor_shape)\n",
    "    \n",
    "    Args:\n",
    "        tensor_list (list of torch.Tensor): List of tensors with identical shapes.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor with the new shape (1, len(tensor_list), inner dims...).\n",
    "    \"\"\"\n",
    "    # First stack the tensors along dimension 0: shape becomes (number_of_tensors, inner dims...)\n",
    "    stacked = torch.stack(tensor_list, dim=0)\n",
    "    # Add a leading dimension to obtain the final shape (1, number_of_tensors, inner dims...)\n",
    "    return stacked.unsqueeze(0)\n",
    "\n",
    "\n",
    "def list_of_dicts_to_tensor(data, key_order=None):\n",
    "    \"\"\"\n",
    "    Convert a list of dictionaries (each with tensor or numeric values) \n",
    "    into a 3D tensor of shape (1, number_of_dicts, number_of_keys).\n",
    "\n",
    "    Args:\n",
    "        data (list): List of dictionaries where each dictionary contains the same keys.\n",
    "        key_order (list, optional): Specific order of keys to extract from each dictionary.\n",
    "                                    If None, keys from the first dictionary are used.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A 3D tensor with shape (1, len(data), len(key_order)).\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        raise ValueError(\"The input data list is empty.\")\n",
    "    \n",
    "    # Use keys from the first dictionary if no order is specified.\n",
    "    if key_order is None:\n",
    "        key_order = list(data[0].keys())\n",
    "    \n",
    "    values_list = []\n",
    "    for d in data:\n",
    "        # Extract values in the specified order. Convert tensor values to scalar if necessary.\n",
    "        values = []\n",
    "        for key in key_order:\n",
    "            value = d[key]\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                # Assuming tensor is of shape (1,)\n",
    "                values.append(value.item())\n",
    "            else:\n",
    "                values.append(value)\n",
    "        values_list.append(values)\n",
    "    \n",
    "    # Convert the list of lists to a 2D tensor.\n",
    "    tensor_2d = torch.tensor(values_list)\n",
    "    # Add a new dimension at the beginning to make it 3D.\n",
    "    tensor_3d = tensor_2d.unsqueeze(0)\n",
    "    return tensor_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10252d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9a320",
   "metadata": {},
   "source": [
    "## MODEL **Architechure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34fc258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayerEmbedding(nn.Module):\n",
    "  \"\"\" HEre we are doing the Proj of the raw Player embedding into the PERFORMANCE_EMBD_DIM \"\"\"\n",
    "  def __init__(self, in_channels=config.PLAYER_INPUT_DIM, out_channels=config.PERFORMANCE_EMBD_DIM):\n",
    "    super().__init__()\n",
    "    self.proj = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # print(f\"In Player Embd  {x.shape}\")\n",
    "    # x: (B, PLAYER_INPUT_DIM) or flattened (B*T, PLAYER_INPUT_DIM)\n",
    "    return self.proj(x)   ## ( B/B*T, PERFORMANCE_EMBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adba455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! handle cross attention (also should it be applied using team2 and current player)\n",
    "class MatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a match-level embedding from team and match information.\n",
    "    performing cross attention from team_players performance -> current palyer performance \n",
    "    Expected input shapes (for T matches):\n",
    "      team1_players: (B, T, num_team1, PLAYER_INPUT_DIM) #!TODO we are passing it\n",
    "      team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
    "      #? TODO implement cross attention (including player X team-2 performance )\n",
    "      \n",
    "      match_info: (B, T, match_info_dim)\n",
    "    Output:\n",
    "      (B, T, PERFORMANCE_EMBD_DIM)\n",
    "    \"\"\"\n",
    "    def __init__(self, player_embedding_module, in_channels=(2*config.PERFORMANCE_EMBD_DIM + config.MATCH_INPUT_EMBD), out_channels=config.PERFORMANCE_EMBD_DIM):\n",
    "        super().__init__()\n",
    "        self.player = player_embedding_module\n",
    "        self.proj = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, team1_players, team2_players, match_info):\n",
    "        \"\"\"\n",
    "          1. Get player embeddings using self.player.\n",
    "          2. Sum (or pool) embeddings for each team.\n",
    "          3. Concatenate team representations with match_info.\n",
    "          4. Project the concatenated vector to obtain the final match embedding.\n",
    "        \"\"\"\n",
    "        B, T, num_team1, _ = team1_players.shape\n",
    "        # print(f\"In match embd {team1_players.shape} & {match_info.shape}, B {B}, T, {T}, nums_team1 {num_team1}\")\n",
    "        # Compute player embeddings\n",
    "        team1_flat = team1_players.reshape(B * T, num_team1, -1)  # (B*T, num_team1, PLAYER_INPUT_DIM)\n",
    "        team1_embeds = self.player(team1_flat)  # (B*T, num_team1, PERFORMANCE_EMBD_DIM)\n",
    "        team1_sum = team1_embeds.sum(dim=1)  # (B*T, PERFORMANCE_EMBD_DIM)\n",
    "        team1_sum = team1_sum.reshape(B, T, -1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
    "\n",
    "        B, T, num_team2, _ = team2_players.shape\n",
    "        team2_flat = team2_players.reshape(B * T, num_team2, -1)\n",
    "        team2_embeds = self.player(team2_flat)  # (B*T, num_team2, PERFORMANCE_EMBD_DIM)\n",
    "        team2_sum = team2_embeds.sum(dim=1)  # (B*T, PERFORMANCE_EMBD_DIM)\n",
    "        team2_sum = team2_sum.reshape(B, T, -1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
    "\n",
    "        # Concatenate team summaries with match-level info along last dimension.\n",
    "        # match_info: (B, T, match_info_dim)\n",
    "        fused = torch.cat([team1_sum, team2_sum, match_info], dim=-1)  # (B, T, 2*PERFORMANCE_EMBD_DIM + match_info_dim)\n",
    "        match_embedding = self.proj(fused)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
    "\n",
    "        return match_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines a sequence of player embeddings with a sequence of match embeddings to produce a sequence\n",
    "    of performance embeddings. For each time step:\n",
    "      1. Obtain the player's embedding from player_input (B, T, PLAYER_INPUT_DIM).\n",
    "      2. Obtain the match embedding using the MatchEmbedding module, which now expects team-level\n",
    "         inputs with a time dimension (B, T, ...).\n",
    "      3. Concatenate these embeddings (+moresulting in a vector of dimension 2 * PERFORMANCE_EMBD_DIM).\n",
    "      4. Project the concatenated vector to PERFORMANCE_EMBD_DIM.\n",
    "\n",
    "    Expected input shapes:\n",
    "      player_input: (B, T, PLAYER_INPUT_DIM)\n",
    "      team1_players: (B, T, num_team1, PLAYER_INPUT_DIM)\n",
    "      team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
    "      match_info: (B, T, match_info_dim)\n",
    "\n",
    "    Output:\n",
    "      (B, T, PERFORMANCE_EMBD_DIM)\n",
    "    \"\"\"\n",
    "    def __init__(self, player_embedding_module, match_embedding_module, out_channels=config.PERFORMANCE_EMBD_DIM):\n",
    "        super().__init__()\n",
    "        self.player_embedding_module = player_embedding_module\n",
    "        self.match_embedding_module = match_embedding_module\n",
    "        self.performance_proj = nn.Linear(config.PERFORMANCE_INPUT_DIM, out_channels)\n",
    "        # Linear layer to map concatenated [player_emb; match_emb] (dimension 2*PERFORMANCE_EMBD_DIM)\n",
    "        # to PERFORMANCE_EMBD_DIM.\n",
    "        self.proj = nn.Linear(3 * config.PERFORMANCE_EMBD_DIM, out_channels)\n",
    "\n",
    "    def forward(self, player_input, player_performance_input, team1_players, team2_players, match_info):\n",
    "        \"\"\"\n",
    "        player_input: (B, T, PLAYER_INPUT_DIM) - raw features for a specific player across T matches.\n",
    "        team1_players: (B, T, num_team1, PLAYER_INPUT_DIM) - raw features for team1 players per match.\n",
    "        team2_players: (B, T, num_team2, PLAYER_INPUT_DIM) - raw features for team2 players per match.\n",
    "        match_info: (B, T, match_info_dim) - extra normalized match information per match.\n",
    "        \"\"\"\n",
    "        B, T, _ = player_performance_input.shape\n",
    "        # print(f\"In performance embedding {player_input.shape} && {player_performance_input.shape}\")\n",
    "        # Compute player's embedding for each match time step.\n",
    "        # Reshape to (B*T, PLAYER_INPUT_DIM) so that the player_embedding_module can be applied, then reshape back.\n",
    "        player_emb = self.player_embedding_module(player_input.reshape(B, -1))  # (B, PERFORMANCE_EMBD_DIM)\n",
    "        player_emb = player_emb.unsqueeze(1).repeat(1, T, 1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
    "        #!why we are repeating this.....\n",
    "        player_performance_emb = self.performance_proj(player_performance_input.reshape(B * T, -1))  # (B*T, PERFORMANCE_INPUT_DIM) => (B*T, PERFORMANCE_EMBD_DIM)\n",
    "        player_performance_emb = player_performance_emb.reshape(B, T, -1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
    "\n",
    "        # Compute match embedding across T time steps.\n",
    "        # Ensure that the match_embedding_module expects inputs with a time dimension.\n",
    "        match_emb = self.match_embedding_module(team1_players, team2_players, match_info)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
    "\n",
    "        # Concatenate the player's embedding and match embedding for each time step.\n",
    "        combined = torch.cat([player_emb, player_performance_emb, match_emb], dim=-1)  # (B, T, 3 * PERFORMANCE_EMBD_DIM)\n",
    "        #! what is player_emb, and player_performance_emb (diffence....)\n",
    "        # Project the concatenated vector back to PERFORMANCE_EMBD_DIM.\n",
    "        performance_emb = self.proj(combined)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
    "        #! performance_proj -> performance_embd_dim  and the combined with the match_ifo is also project to the performance_embd_dim \n",
    "        #TODO should we use another higher dimesion \n",
    "        return performance_emb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
