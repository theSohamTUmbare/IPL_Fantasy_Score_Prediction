{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10876ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8abce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    PLAYERS_SIZE: int = 0\n",
    "    CONTEXT_LEN: int = 16  ## predict the 21st match by the 20\n",
    "    PERFORMANCE_INPUT_DIM = 23\n",
    "    PERFORMANCE_EMBD_DIM: int = 128\n",
    "    \n",
    "    PLAYER_INPUT_DIM: int = 25   ## univpalyer\n",
    "    MATCH_INPUT_EMBD: int = 14   ## match_info\n",
    "    NUM_EPOCHS: int = 100\n",
    "    LEARNING_RATE: float = 1e-3\n",
    "    BATCH_SIZE: int = 128\n",
    "    DEVICE: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # MODEL_SAVE_PATH: str = \"/kaggle/working\"\n",
    "    # BASE_DIR: str = '/kaggle/input/flickr8k/'\n",
    "    # CROSSATT_NUM_HEADS: int = 8\n",
    "    CLS_INIT_STD: float = 0.02    ## <CLS> token initialized with std 0.02 from the mean=0\n",
    "    TEST_DATASET_SIZE: int = 180\n",
    "    IDLE_DEVICE: str = 'cpu'\n",
    "    ACCUMULATION_STEPS = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24453239",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539a741",
   "metadata": {},
   "source": [
    "## Defination of **Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf47a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4cf5dc7",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3643010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PlayerMatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for autoregressive next-match prediction.\n",
    "\n",
    "    For each sample:\n",
    "      1. Given a player_id, retrieve universal player features from universal_player.csv.\n",
    "         (The 'player_id' and 'cricinfo_id' columns are dropped from the input features.)\n",
    "      2. Retrieve all matches for that player from a directory of player match files\n",
    "         (each file is named \"{player_id}.csv\" and contains the matches for that player,\n",
    "         already sorted by date).\n",
    "         The performance columns include:\n",
    "           batting_position, runs, balls, fours, sixes, strike_rate, overs, total_balls, dots,\n",
    "           maidens, conceded, fours_conceded, sixes_conceded, wickets, LBW, Bowled, noballs,\n",
    "           wides, economy_rate, catches, stumping, direct_hit, indirect_hit, strike_rate_fp,\n",
    "           batting_fp, bowling_fp, fielding_fp, total_fp.\n",
    "         We drop 'match_id' and the fantasy-breakdown columns\n",
    "         ('strike_rate_fp', 'batting_fp', 'bowling_fp', 'fielding_fp', 'total_fp') when forming the input vector.\n",
    "      3. Randomly sample a contiguous window of (context_len + 1) matches for this player.\n",
    "         The first context_len matches serve as input and matches 2 ... context_len+1 yield the target fantasy scores.\n",
    "      4. For each match in the window:\n",
    "         - Load the corresponding match players CSV from the match_players folder (file: '{match_id}.csv').\n",
    "         - Determine the player's team for that match and then separate player_ids into:\n",
    "              team1_ids: those belonging to the same team as the player,\n",
    "              team2_ids: those belonging to the other team.\n",
    "         - Retrieve their universal features from universal_player.csv.\n",
    "         - Retrieve match info from a single match_info CSV (by matching on match_id).\n",
    "      5. Return a dictionary containing:\n",
    "         - 'player_id': the player's id.\n",
    "         - 'univ_features': the player's universal features.\n",
    "         - 'context_matches': a numpy array of performance features for the context matches.\n",
    "         - 'target_scores': a numpy array of target fantasy scores (for matches 2 ... context_len+1).\n",
    "         - 'team1_players': list (per match) of team1 players' universal features.\n",
    "         - 'team2_players': list (per match) of team2 players' universal features.\n",
    "         - 'match_info': list (per match) of match info dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, universal_player_csv, player_matches_dir, match_players_dir, match_info_csv, context_len=25, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            universal_player_csv (str): Path to universal_player.csv.\n",
    "            player_matches_dir (str): Directory containing CSV files for each player's matches (named '{player_id}.csv').\n",
    "            match_players_dir (str): Directory containing match_players CSV files.\n",
    "            match_info_csv (str): Path to the CSV file containing match info for all matches.\n",
    "            context_len (int): Number of context matches to use as input \n",
    "                                (target will be matches 2 ... context_len+1).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.context_len = context_len\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load universal player features and set player_id as index.\n",
    "        self.univ_df = pd.read_csv(universal_player_csv)\n",
    "        self.univ_df = self.univ_df.set_index('player_id')\n",
    "        # Drop 'cricinfo_id' as it is redundant.\n",
    "        self.univ_features = self.univ_df.drop(columns=['cricinfo_id'], errors='ignore')\n",
    "\n",
    "        # Instead of a single file for player matches, we assume a directory where each player's\n",
    "        # matches are stored in a file named '{player_id}.csv'. We build a list of valid player_ids\n",
    "        # by checking which player match files exist and ensuring they have at least (context_len+1) rows.\n",
    "        self.player_matches_dir = player_matches_dir\n",
    "        self.player_ids = []\n",
    "        self.player_match_data = {}  # Key: player_id, Value: DataFrame of that player's matches.\n",
    "        for player_id in self.univ_features.index:\n",
    "            match_file = os.path.join(player_matches_dir, f\"{player_id}.csv\")\n",
    "            if os.path.exists(match_file):\n",
    "                df_matches = pd.read_csv(match_file)\n",
    "                # Assume the matches in this file are already sorted by date.\n",
    "                if len(df_matches) >= (self.context_len + 1):\n",
    "                    self.player_ids.append(player_id)\n",
    "                    self.player_match_data[player_id] = df_matches\n",
    "\n",
    "        self.match_players_dir = match_players_dir\n",
    "\n",
    "        # Load the single match_info CSV and set match_id as index for fast lookup.\n",
    "        self.match_info_df = pd.read_csv(match_info_csv)\n",
    "        self.match_info_df = self.match_info_df.set_index('match_id')\n",
    "        # Optionally drop columns not needed.\n",
    "        self.match_info_df = self.match_info_df.drop(columns=['team1', 'team2', 'toss_winner', \"toss_decision\", \"winner\"], errors='ignore')\n",
    "        #Todo don't just drop them we need to infer from them in sense of player's team ->winner 1 else 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.player_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a dictionary with:\n",
    "          - 'player_id': the player's id.\n",
    "          - 'univ_features': universal features for the player.\n",
    "          - 'context_matches': a numpy array of performance features for the context matches.\n",
    "          - 'target_scores': a numpy array of target fantasy scores (for matches 2 ... context_len+1).\n",
    "          - 'team1_players': list (per match) of team1 players' universal features.\n",
    "          - 'team2_players': list (per match) of team2 players' universal features.\n",
    "          - 'match_info': list (per match) of match info dictionaries.\n",
    "        \"\"\"\n",
    "        #ToDO not just total_fantasy score, rather a weighted sum of error for batting_bowling fp's, fielding_fp with lesser weightage to the overall total_fp  \n",
    "        # Select the player.\n",
    "        player_id = self.player_ids[idx]\n",
    "        # Retrieve the player's universal features as a numpy array.\n",
    "        player_univ = self.univ_features.loc[player_id].values.astype(float)\n",
    "        \n",
    "        # Load this player's matches DataFrame from the pre-loaded dictionary.\n",
    "        df_matches = self.player_match_data[player_id]\n",
    "        total_matches = len(df_matches)\n",
    "        # Randomly select a contiguous window of (context_len + 1) matches.\n",
    "        start_idx = np.random.randint(1, total_matches - self.context_len+1)\n",
    "        window_df = df_matches.iloc[start_idx-1 : start_idx + self.context_len]\n",
    "\n",
    "        # For input, drop the unused columns.\n",
    "        exclude_cols = ['teamname', 'match_id',\n",
    "                        'strike_rate_fp', 'batting_fp', 'bowling_fp', 'fielding_fp', 'total_fp']\n",
    "        # Extract target fantasy scores (from column 'total_fp') for matches 2 ... context_len+1.\n",
    "        target_scores = window_df.iloc[1:self.context_len+1][['batting_fp', 'bowling_fp', 'fielding_fp',]].values.astype(float)\n",
    "        #Todo account for all sub_fp's (stage -2)\n",
    "        # Extract performance features for input matches.\n",
    "        context_matches = window_df.iloc[:self.context_len].drop(columns=exclude_cols, errors='ignore')\n",
    "\n",
    "        # For each match in the window, retrieve team players and match info.\n",
    "        # team1_players_list = [] \n",
    "        team2_players_list = []\n",
    "        match_info_list = []\n",
    "        for _, match in window_df.iterrows():\n",
    "            match_id = match['match_id']\n",
    "            # Load match players file (expects a file named \"{match_id}.csv\").\n",
    "            match_players_file = os.path.join(self.match_players_dir, f\"{int(match_id)}.csv\")\n",
    "            match_players_df = pd.read_csv(match_players_file)\n",
    "            \n",
    "            # Determine the team of the sample's player in this match.\n",
    "            player_team_series = match_players_df[\n",
    "                match_players_df['player_id'] == player_id]['Team']\n",
    "            #? rather we can refer it from window_df, there in the excluded_cols same as match['team_name']\n",
    "            if not player_team_series.empty:\n",
    "                player_team = player_team_series.iloc[0]\n",
    "            else:\n",
    "                player_team = match_players_df.iloc[0]['Team'] #?why's this, though it won't execute\n",
    "            \n",
    "            # Split player_ids into two groups based on the player's team.\n",
    "            # team1_ids = match_players_df[\n",
    "            #     match_players_df['Team'] == player_team]['player_id'].tolist()\n",
    "            # #! we just need the team2 player_ids (we are omitting the intra-team interactions...)\n",
    "            team2_ids = match_players_df[\n",
    "                match_players_df['Team'] != player_team]['player_id'].tolist()\n",
    "\n",
    "            # Retrieve universal features for these players.\n",
    "            # team1_features = self.univ_features.reindex(team1_ids).dropna().values.astype(float)\n",
    "            team2_features = self.univ_features.reindex(team2_ids).dropna().values.astype(float)\n",
    "            # team1_players_list.append(team1_features)\n",
    "            team2_players_list.append(team2_features)\n",
    "            \n",
    "            # Retrieve match info using match_id from the single match_info DataFrame.\n",
    "            if match_id in self.match_info_df.index:\n",
    "                match_info_dict = self.match_info_df.loc[match_id].to_dict()\n",
    "            else:\n",
    "                match_info_dict = { }\n",
    "            match_info_list.append(match_info_dict)\n",
    "            #? wht's this \n",
    "\n",
    "        # --- START OF CHANGES: convert to properly shaped torch.Tensors ---\n",
    "        import torch\n",
    "\n",
    "        # universal features\n",
    "        univ_features = torch.tensor(player_univ, dtype=torch.float32)  # (feat_dim,)\n",
    "\n",
    "        # context matches\n",
    "        context_matches = torch.tensor(context_matches.values.astype(float),\n",
    "                                       dtype=torch.float32)  # (context_len, perf_dim)\n",
    "\n",
    "        # target scores\n",
    "        target_scores = torch.tensor(target_scores, dtype=torch.float32)  # (context_len,)\n",
    "\n",
    "        # team1 players: pad to max players across the window, then stack\n",
    "        # max1 = max(arr.shape[0] for arr in team1_players_list)\n",
    "        # feat_dim = team1_players_list[0].shape[1] if max1>0 else 0\n",
    "        # padded1 = [\n",
    "        #     np.pad(arr, ((0, max1 - arr.shape[0]), (0, 0)), mode='constant')\n",
    "        #     for arr in team1_players_list\n",
    "        # ]\n",
    "        # team1_players = torch.tensor(np.stack(padded1), dtype=torch.float32)\n",
    "        # # shape: (context_len+1, max1, feat_dim)\n",
    "\n",
    "        # team2 players: same\n",
    "        max2 = max(arr.shape[0] for arr in team2_players_list)\n",
    "        feat_dim2 = team2_players_list[0].shape[1] if max2>0 else 0\n",
    "        padded2 = [\n",
    "            np.pad(arr, ((0, max2 - arr.shape[0]), (0, 0)), mode='constant')\n",
    "            for arr in team2_players_list\n",
    "        ]\n",
    "        team2_players = torch.tensor(np.stack(padded2), dtype=torch.float32)\n",
    "        # shape: (context_len+1, max2, feat_dim2)\n",
    "        #? wht's this\n",
    "\n",
    "        # match_info: convert list of dicts to array in fixed key order\n",
    "        keys = list(self.match_info_df.columns)\n",
    "        info_arr = np.stack([[d.get(k, 0.0) for k in keys] for d in match_info_list])\n",
    "        match_info = torch.tensor(info_arr, dtype=torch.float32)\n",
    "        # shape: (context_len+1, len(keys))\n",
    "\n",
    "        # --- END OF CHANGES ---\n",
    "\n",
    "        sample = {\n",
    "            'player_id': player_id,\n",
    "            'univ_features': univ_features,\n",
    "            'context_matches': context_matches,\n",
    "            'target_scores': target_scores,\n",
    "            # 'team1_players': team1_players[:, :11, :],\n",
    "            'team2_players': team2_players[:, :11, :],\n",
    "            'match_info': match_info\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c409bc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2354"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define file paths (update these paths as needed for your folder structure)\n",
    "universal_player_csv = r'C:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\\Test_1\\cleaned_universal_player.csv'\n",
    "player_matches_dir = r'C:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\\Test_1\\Processed_Player_records'  # Contains files named like {player_id}.csv (each containing that player's matches)\n",
    "match_players_dir = r'C:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\\Test_1\\processed_GlobalMatchrecords'    # Contains files like {match_id}.csv\n",
    "match_info_csv = r'C:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\\Test_1\\cleaned_matchinfo_without_venue_with_updated_match_number.csv'               # Single CSV containing all match info\n",
    "\n",
    "# Define the context length (number of matches to use as context)\n",
    "# For example, if config.CONTEXT_LEN is defined in your config module:\n",
    "# config.CONTEXT_LEN = 5\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = PlayerMatchDataset(\n",
    "    universal_player_csv=universal_player_csv,\n",
    "    player_matches_dir=player_matches_dir,  # This parameter may be ignored if you use the directory version\n",
    "    match_players_dir=match_players_dir,\n",
    "    match_info_csv=match_info_csv,\n",
    "    context_len=config.CONTEXT_LEN\n",
    ")\n",
    "\n",
    "# def get_shape(lst):\n",
    "#     shape = []\n",
    "#     while isinstance(lst, list):\n",
    "#         shape.append(len(lst))\n",
    "#         if len(lst) == 0:\n",
    "#             break\n",
    "#         lst = lst[0]\n",
    "#     return tuple(shape)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "len(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92299cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player IDs: ['3fca55af', '6cc4d322', '02ba80a0', '839b6d2e', 'f54c8804', '6c19c6e5', '8f6dd463', 'b23d3ec5', 'f7e9a562', 'afa7e784', 'fdb8ed0d', '92aeac25', 'f892fcf9', 'e741ed8f', '365229a8', '2494ff9e', '11f56b14', '7cb2d9d2', '13d0b3d6', '352e589f', 'ff3f6fc1', '23eeb873', '1be70c88', 'ffe699c0', '818c9e0d', '6a26221c', '9ccb6e52', 'f8035c2a', '773cec26', '9061a703', '7be8a0ed', '88209c84']\n",
      "Universal features shape: torch.Size([32, 25])\n",
      "Context matches shape: torch.Size([32, 16, 23])\n",
      "Target scores shape: torch.Size([32, 16, 3])\n",
      "Number of matches in team2_players (per sample): torch.Size([32, 17, 11, 25])\n",
      "Number of matches in match_info (per sample): torch.Size([32, 17, 14])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "for batch in dataloader:\n",
    "    print(\"Player IDs:\", batch['player_id'])  # list of player_ids (length = batch_size)\n",
    "    \n",
    "    print(\"Universal features shape:\", (batch['univ_features']).shape)  # e.g., (batch_size, num_features)\n",
    "    \n",
    "    print(\"Context matches shape:\", batch['context_matches'].shape)  # e.g., (batch_size, context_len, performance_feature_dim)\n",
    "    \n",
    "    print(\"Target scores shape:\", batch['target_scores'].shape)      # e.g., (batch_size, context_len)\n",
    "    \n",
    "    # The following are lists of length (context_len+1); each element is a numpy array.\n",
    "    # team1_players_tensor = list_of_tensors_to_3d(batch['team1_players'])\n",
    "    # team1_players_tensor = team1_players_tensor.squeeze(2)\n",
    "    # print(\"Number of matches in team1_players (per sample):\", batch['team1_players'].shape)\n",
    "    \n",
    "    # team2_players_tensor = list_of_tensors_to_3d(batch['team2_players'])\n",
    "    # team2_players_tensor = team2_players_tensor.squeeze(2)\n",
    "    print(\"Number of matches in team2_players (per sample):\", batch['team2_players'].shape)\n",
    "    \n",
    "    # match_info_tensor = list_of_dicts_to_tensor(batch['match_info'])\n",
    "    print(\"Number of matches in match_info (per sample):\", (batch['match_info']).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17b89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_tensors_to_3d(tensor_list):\n",
    "    \"\"\"\n",
    "    Converts a list of tensors into a 3D tensor with shape:\n",
    "    (1, number_of_tensors, *inner_tensor_shape)\n",
    "    \n",
    "    Args:\n",
    "        tensor_list (list of torch.Tensor): List of tensors with identical shapes.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor with the new shape (1, len(tensor_list), inner dims...).\n",
    "    \"\"\"\n",
    "    # First stack the tensors along dimension 0: shape becomes (number_of_tensors, inner dims...)\n",
    "    stacked = torch.stack(tensor_list, dim=0)\n",
    "    # Add a leading dimension to obtain the final shape (1, number_of_tensors, inner dims...)\n",
    "    return stacked.unsqueeze(0)\n",
    "\n",
    "\n",
    "def list_of_dicts_to_tensor(data, key_order=None):\n",
    "    \"\"\"\n",
    "    Convert a list of dictionaries (each with tensor or numeric values) \n",
    "    into a 3D tensor of shape (1, number_of_dicts, number_of_keys).\n",
    "\n",
    "    Args:\n",
    "        data (list): List of dictionaries where each dictionary contains the same keys.\n",
    "        key_order (list, optional): Specific order of keys to extract from each dictionary.\n",
    "                                    If None, keys from the first dictionary are used.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A 3D tensor with shape (1, len(data), len(key_order)).\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        raise ValueError(\"The input data list is empty.\")\n",
    "    \n",
    "    # Use keys from the first dictionary if no order is specified.\n",
    "    if key_order is None:\n",
    "        key_order = list(data[0].keys())\n",
    "    \n",
    "    values_list = []\n",
    "    for d in data:\n",
    "        # Extract values in the specified order. Convert tensor values to scalar if necessary.\n",
    "        values = []\n",
    "        for key in key_order:\n",
    "            value = d[key]\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                # Assuming tensor is of shape (1,)\n",
    "                values.append(value.item())\n",
    "            else:\n",
    "                values.append(value)\n",
    "        values_list.append(values)\n",
    "    \n",
    "    # Convert the list of lists to a 2D tensor.\n",
    "    tensor_2d = torch.tensor(values_list)\n",
    "    # Add a new dimension at the beginning to make it 3D.\n",
    "    tensor_3d = tensor_2d.unsqueeze(0)\n",
    "    return tensor_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37731161",
   "metadata": {},
   "source": [
    "## MODEL **Architechure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9a370ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayerEmbedding(nn.Module):\n",
    "  \"\"\" HEre we are doing the Proj of the raw Player embedding into the PERFORMANCE_EMBD_DIM \"\"\"\n",
    "  def __init__(self, in_channels=config.PLAYER_INPUT_DIM, out_channels=config.PERFORMANCE_EMBD_DIM):\n",
    "    super().__init__()\n",
    "    self.proj = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # print(f\"In Player Embd  {x.shape}\")\n",
    "    # x: (B, PLAYER_INPUT_DIM) or flattened (B*T, PLAYER_INPUT_DIM)\n",
    "    return self.proj(x)   ## ( B/B*T, PERFORMANCE_EMBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "39d43842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CrossAttention(nn.Module):\n",
    "#     def __init__(self, d_embed=config.PERFORMANCE_EMBD_DIM, d_cross=config.PERFORMANCE_EMBD_DIM, n_heads=8, in_proj_bias=True, out_proj_bias=True):\n",
    "#         super().__init__()\n",
    "#         # Initialize linear layers for query, key, and value projections.\n",
    "#         # q_proj: projects the query input with shape (batch_size, seq_length_q=1, d_embed)\n",
    "#         self.q_proj   = nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n",
    "#         # k_proj: projects the key input with shape (batch_size, seq_length_kv=11, d_cross)\n",
    "#         self.k_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
    "#         # v_proj: projects the value input with shape (batch_size, seq_length_kv=11, d_cross)\n",
    "#         self.v_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
    "#         # out_proj: projects the concatenated multi-head outputs back to the original embedding dimension.\n",
    "#         self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "#         self.n_heads = n_heads                     # Number of attention heads.\n",
    "#         self.d_head = d_embed // n_heads           # Dimensionality per head.\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         \"\"\"\n",
    "#         Compute cross-attention between two sequences.\n",
    "\n",
    "#         Parameters:\n",
    "#             x (Tensor): Query input tensor of shape (batch_size, seq_length_q=1, d_embed)\n",
    "#             y (Tensor): Key/Value input tensor of shape (batch_size, seq_length_kv=11, d_cross)\n",
    "\n",
    "#         Returns:\n",
    "#             Tensor: Output tensor after applying cross-attention, with shape (batch_size, seq_length_q=1, d_embed)\n",
    "#         \"\"\"\n",
    "#         # Save original input shape.\n",
    "#         input_shape = x.shape  # (batch_size, seq_length_q=1, d_embed)\n",
    "#         batch_size, sequence_length, d_embed = input_shape\n",
    "\n",
    "#         # Define intermediate shape for splitting into multiple heads.\n",
    "#         # New shape: (batch_size, seq_length, n_heads, d_head)\n",
    "#         interim_shape = (batch_size, -1, self.n_heads, self.d_head)\n",
    "\n",
    "#         # Apply linear projections:\n",
    "#         # q: project queries from x.\n",
    "#         q = self.q_proj(x)  # (batch_size, seq_length_q=1, d_embed)\n",
    "#         # k: project keys from y.\n",
    "#         k = self.k_proj(y)  # (batch_size, seq_length_kv=11, d_embed)\n",
    "#         # v: project values from y.\n",
    "#         v = self.v_proj(y)  # (batch_size, seq_length_kv=11, d_embed)\n",
    "\n",
    "#         # Reshape projections to separate attention heads.\n",
    "#         # Transform shape to (batch_size, n_heads, seq_length, d_head)\n",
    "#         q = q.view(interim_shape).transpose(1, 2)\n",
    "#         k = k.view(interim_shape).transpose(1, 2)\n",
    "#         v = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "#         # Compute attention scores using scaled dot-product:\n",
    "#         # Multiply queries with transposed keys.\n",
    "#         # Resulting shape: (batch_size, n_heads, seq_length_q=1, seq_length_kv=11)\n",
    "#         weight = q @ k.transpose(-1, -2)\n",
    "\n",
    "#         # Scale the scores by the square root of head dimension to stabilize gradients.\n",
    "#         weight /= math.sqrt(self.d_head)\n",
    "\n",
    "#         # Normalize the attention weights using softmax along the key dimension.\n",
    "#         weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "#         # Compute the weighted sum over the values.\n",
    "#         # Output shape: (batch_size, n_heads, seq_length_q=1, d_head)\n",
    "#         output = weight @ v\n",
    "\n",
    "#         # Reorder dimensions to combine multiple heads:\n",
    "#         # Transpose back to (batch_size, seq_length_q=1, n_heads, d_head) and make contiguous.\n",
    "#         output = output.transpose(1, 2).contiguous()\n",
    "\n",
    "#         # Merge the multi-head outputs to recover original embedding dimension.\n",
    "#         # Final shape: (batch_size, seq_length_q=1, d_embed)\n",
    "#         output = output.view(input_shape)\n",
    "\n",
    "#         # Apply the final output projection.\n",
    "#         output = self.out_proj(output)\n",
    "\n",
    "#         return output  # Return the final cross-attention output.\n",
    "\n",
    "# given this cross attention module, i want to apply to calculate the cross attention from team2 to team1 players embeddings...\n",
    "# #! handle cross attention (also should it be applied using team2 and current player)\n",
    "# class MatchEmbedding(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Computes a match-level embedding from team and match information.\n",
    "#     performing cross attention from team_players performance -> current palyer performance \n",
    "#     Expected input shapes (for T matches):\n",
    "#       team1_players: (B, T, num_team1, PLAYER_INPUT_DIM) #!TODO we are passing it\n",
    "#       team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
    "#       #? TODO implement cross attention (including player X team-2 performance )\n",
    "      \n",
    "#       match_info: (B, T, match_info_dim)\n",
    "#     Output:\n",
    "#       (B, T, PERFORMANCE_EMBD_DIM)\n",
    "#     \"\"\"\n",
    "#     def __init__(self, player_embedding_module, in_channels=(2*config.PERFORMANCE_EMBD_DIM + config.MATCH_INPUT_EMBD), out_channels=config.PERFORMANCE_EMBD_DIM):\n",
    "#         super().__init__()\n",
    "#         self.player = player_embedding_module\n",
    "#         self.proj = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, team1_players, team2_players, match_info):\n",
    "#         \"\"\"\n",
    "#           1. Get player embeddings using self.player.\n",
    "#           2. Sum (or pool) embeddings for each team.\n",
    "#           3. Concatenate team representations with match_info.\n",
    "#           4. Project the concatenated vector to obtain the final match embedding.\n",
    "#         \"\"\"\n",
    "#         B, T, num_team1, _ = team1_players.shape\n",
    "#         # print(f\"In match embd {team1_players.shape} & {match_info.shape}, B {B}, T, {T}, nums_team1 {num_team1}\")\n",
    "#         # Compute player embeddings\n",
    "#         team1_flat = team1_players.reshape(B * T, num_team1, -1)  # (B*T, num_team1, PLAYER_INPUT_DIM)\n",
    "#         team1_embeds = self.player(team1_flat)  # (B*T, num_team1, PERFORMANCE_EMBD_DIM)\n",
    "#         team1_sum = team1_embeds.sum(dim=1)  # (B*T, PERFORMANCE_EMBD_DIM)\n",
    "#         team1_sum = team1_sum.reshape(B, T, -1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
    "\n",
    "#         B, T, num_team2, _ = team2_players.shape\n",
    "#         team2_flat = team2_players.reshape(B * T, num_team2, -1)\n",
    "#         team2_embeds = self.player(team2_flat)  # (B*T, num_team2, PERFORMANCE_EMBD_DIM)\n",
    "#         team2_sum = team2_embeds.sum(dim=1)  # (B*T, PERFORMANCE_EMBD_DIM)\n",
    "#         team2_sum = team2_sum.reshape(B, T, -1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
    "\n",
    "#         # Concatenate team summaries with match-level info along last dimension.\n",
    "#         # match_info: (B, T, match_info_dim)\n",
    "#         fused = torch.cat([team1_sum, team2_sum, match_info], dim=-1)  # (B, T, 2*PERFORMANCE_EMBD_DIM + match_info_dim)\n",
    "#         match_embedding = self.proj(fused)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
    "\n",
    "#         return match_embedding \n",
    "# in this  MatchEmbedding module instead of duing a normal addition over all player in each team and creating team1_sum,team2_sum and then just concatenating them with match_info to create fused embedding of shape  (B, T, 2*PERFORMANCE_EMBD_DIM + match_info_dim)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4cdcf795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_embed=config.PERFORMANCE_EMBD_DIM, d_cross=config.PERFORMANCE_EMBD_DIM, n_heads=8, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        # Initialize linear layers for query, key, and value projections.\n",
    "        # q_proj: projects the query input with shape (batch_size, seq_length_q=1, d_embed)\n",
    "        self.q_proj   = nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n",
    "        # k_proj: projects the key input with shape (batch_size, seq_length_kv=11, d_cross)\n",
    "        self.k_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
    "        # v_proj: projects the value input with shape (batch_size, seq_length_kv=11, d_cross)\n",
    "        self.v_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
    "        # out_proj: projects the concatenated multi-head outputs back to the original embedding dimension.\n",
    "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "        self.n_heads = n_heads                     # Number of attention heads.\n",
    "        self.d_head = d_embed // n_heads           # Dimensionality per head.\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute cross-attention between two sequences.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): Query input tensor of shape (batch_size, seq_length_q=1, d_embed)\n",
    "            y (Tensor): Key/Value input tensor of shape (batch_size, seq_length_kv=11, d_cross)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying cross-attention, with shape (batch_size, seq_length_q=1, d_embed)\n",
    "        \"\"\"\n",
    "        # Save original input shape.\n",
    "        input_shape = x.shape  # (batch_size, seq_length_q=1, d_embed)\n",
    "        batch_size, sequence_length, d_embed = input_shape\n",
    "\n",
    "        # Define intermediate shape for splitting into multiple heads.\n",
    "        # New shape: (batch_size, seq_length, n_heads, d_head)\n",
    "        interim_shape = (batch_size, -1, self.n_heads, self.d_head)\n",
    "\n",
    "        # Apply linear projections:\n",
    "        # q: project queries from x.\n",
    "        q = self.q_proj(x)  # (batch_size, seq_length_q=1, d_embed)\n",
    "        # k: project keys from y.\n",
    "        k = self.k_proj(y)  # (batch_size, seq_length_kv=11, d_embed)\n",
    "        # v: project values from y.\n",
    "        v = self.v_proj(y)  # (batch_size, seq_length_kv=11, d_embed)\n",
    "\n",
    "        # Reshape projections to separate attention heads.\n",
    "        # Transform shape to (batch_size, n_heads, seq_length, d_head)\n",
    "        q = q.view(interim_shape).transpose(1, 2)\n",
    "        k = k.view(interim_shape).transpose(1, 2)\n",
    "        v = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores using scaled dot-product:\n",
    "        # Multiply queries with transposed keys.\n",
    "        # Resulting shape: (batch_size, n_heads, seq_length_q=1, seq_length_kv=11)\n",
    "        weight = q @ k.transpose(-1, -2)\n",
    "\n",
    "        # Scale the scores by the square root of head dimension to stabilize gradients.\n",
    "        weight /= math.sqrt(self.d_head)\n",
    "\n",
    "        # Normalize the attention weights using softmax along the key dimension.\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "        # Compute the weighted sum over the values.\n",
    "        # Output shape: (batch_size, n_heads, seq_length_q=1, d_head)\n",
    "        output = weight @ v\n",
    "\n",
    "        # Reorder dimensions to combine multiple heads:\n",
    "        # Transpose back to (batch_size, seq_length_q=1, n_heads, d_head) and make contiguous.\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "\n",
    "        # Merge the multi-head outputs to recover original embedding dimension.\n",
    "        # Final shape: (batch_size, seq_length_q=1, d_embed)\n",
    "        output = output.view(input_shape)\n",
    "\n",
    "        # Apply the final output projection.\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output  # Return the final cross-attention output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "36589dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Computes a match-level embedding by refining a player's performance embedding \n",
    "        with cross-attention over the opposing team (team2) players' performance embeddings \n",
    "        and then fusing with match-level information. # we'll do this step later seperately\n",
    "    \"\"\"\n",
    "    def __init__(self,cross_attention_module : CrossAttention,\n",
    "                 in_channels = config.PERFORMANCE_EMBD_DIM + config.MATCH_INPUT_EMBD,\n",
    "                 out_channels = config.PERFORMANCE_EMBD_DIM):\n",
    "        super(ApplyCrossAttention,self).__init__()\n",
    "        self.cross_attention = cross_attention_module\n",
    "        self.proj = nn.Linear(in_channels,out_channels)\n",
    "    def forward(self,player,team2_players,match_info):\n",
    "        \"\"\"\n",
    "        expected inputed are already passed through the playerEmbedding module\n",
    "        Args:\n",
    "            player (Tensor): shape (B,T, PERFORMANCE_EMBD_DIM) #we have to copy it for (B,T,PLAYER_INPUT_DIM) already provided\n",
    "            team2_players (Tensor): shape (B,T,num_team2,PERFORMANCE_EMBD_DIM)\n",
    "            match_info (Tensor): shape (B,T,match_info_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: The final match embedding, with shape (B, T, PERFORMANCE_EMBD_DIM)\n",
    "        \"\"\"\n",
    "        # print(player.shape)\n",
    "        # print(team2_players.shape)\n",
    "        assert player.shape[-1] == config.PERFORMANCE_EMBD_DIM, 'dim mismatch!...'\n",
    "        B,T,num_team2,_ = team2_players.shape\n",
    "        assert num_team2 == 11 , 'no of players in team2 should we 11...'\n",
    "        # player_embed = self.player(player.unsqueeze(1))  # -> (B, 1, PERFORMANCE_EMBD_DIM)\n",
    "        # player_embed_expanded = player_embed.expand(-1,T,-1)\n",
    "        player_query = player.reshape(B*T,1,-1) #(B*T,1,PERFORMANCE_EMBD_DIM) \n",
    "        \n",
    "        team2_embeds = team2_players.reshape(B*T,num_team2,-1) # -> (B*T, num_team2, PERFORMANCE_EMBD_DIM) \n",
    "        \n",
    "        #TODO apply cross attention\n",
    "        attened = self.cross_attention(player_query,team2_embeds)\n",
    "        attened = attened.squeeze(1)  # -> (B*T, PERFORMANCE_EMBD_DIM)\n",
    "        attened = attened.view(B,T,-1)\n",
    "        \n",
    "        fused = torch.cat([attened,match_info],dim=-1) # -> (B, T, PERFORMANCE_EMBD_DIM + match_info_dim) T is actually config.CONTEXT_LEN + 1\n",
    "        match_embedding = self.proj(fused)  # -> (B, T, PERFORMANCE_EMBD_DIM)\n",
    "        return match_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ca1fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        # This combines the Wq, Wk and Wv matrices into one matrix\n",
    "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
    "        # This one represents the Wo matrix\n",
    "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "        self.n_heads = n_heads   ## how many heads u want ?\n",
    "        self.d_head = d_embed // n_heads   ## the original embedding get divided in the all heads equally\n",
    "   \n",
    "\n",
    "    def forward(self, x, causal_mask=False):\n",
    "\n",
    "        # x: # (Batch_Size, Seq_Len, Dim)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim)\n",
    "        input_shape = x.shape\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim)\n",
    "        batch_size, sequence_length, d_embed = input_shape\n",
    "\n",
    "        # (Batch_Size, Seq_Len, H, Dim / H)\n",
    "        qkv_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim * 3) -> 3 tensor of shape (Batch_Size, Seq_Len, Dim)\n",
    "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
    "        q = q.view(qkv_shape).transpose(1, 2)\n",
    "        k = k.view(qkv_shape).transpose(1, 2)\n",
    "        v = v.view(qkv_shape).transpose(1, 2)\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len, Dim / H) @ (Batch_Size, H, Dim / H, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
    "        weight = q @ k.transpose(-1, -2)\n",
    "\n",
    "        if causal_mask:\n",
    "            # Mask where the upper triangle (above the principal diagonal) is 1\n",
    "            mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
    "            # Fill the upper triangle with -inf\n",
    "            weight.masked_fill_(mask, -torch.inf)\n",
    "\n",
    "        # Divide by d_k (Dim / H).\n",
    "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
    "        weight /= math.sqrt(self.d_head)\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len, Seq_Len) @ (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
    "        output = weight @ v\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, Seq_Len, H, Dim / H)\n",
    "        output = output.transpose(1, 2)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, Seq_Len, Dim)\n",
    "        output = output.reshape(input_shape)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e459ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class a_layer(nn.Module):\n",
    "    \"\"\" A Single Transformer Layer/Block \"\"\"\n",
    "\n",
    "    def __init__(self, n_head: int, n_embd: int):\n",
    "        super().__init__()\n",
    "        # Pre-attention norm\n",
    "        self.layernorm_1 = nn.LayerNorm(n_embd)\n",
    "        # Self attention\n",
    "        self.attention = SelfAttention(n_head, n_embd)\n",
    "        # Pre-FNN norm\n",
    "        self.layernorm_2 = nn.LayerNorm(n_embd)\n",
    "        # Feedforward layer\n",
    "        self.linear_1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.linear_2 = nn.Linear(4 * n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (Batch_Size, Seq_Len, Dim)\n",
    "        residue = x\n",
    "\n",
    "        ### SELF ATTENTION ###\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x = self.layernorm_1(x)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x = self.attention(x, causal_mask=True)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) + (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x += residue\n",
    "\n",
    "        ### FEEDFORWARD LAYER ###\n",
    "        # Apply a feedforward layer where the hidden dimension is 4 times the embedding dimension.\n",
    "\n",
    "        residue = x\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x = self.layernorm_2(x)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, 4 * Dim)\n",
    "        x = self.linear_1(x)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, 4 * Dim) -> (Batch_Size, Seq_Len, 4 * Dim)\n",
    "        x = x * torch.sigmoid(1.702 * x)   # QuickGELU activation function found best for this work\n",
    "\n",
    "        # (Batch_Size, Seq_Len, 4 * Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) + (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x += residue\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3a70ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NextFormPredictor1(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts a sequence of performance embeddings autoregressively.\n",
    "    applies loss at each time step. \n",
    "    \"\"\"\n",
    "    def __init__(self,player_embedding_module,cross_attention_module,cross_Attention_embedding_module,fantasy_score_prediction_module,custom_loss = None,embedding_dim = config.PERFORMANCE_EMBD_DIM ,num_layers=6, n_head=8):\n",
    "        super(NextFormPredictor1,self).__init__()\n",
    "        self.player_embedding_module = player_embedding_module\n",
    "        self.cross_attention_module = cross_attention_module\n",
    "        self.cross_attended_embedding = cross_Attention_embedding_module(self.cross_attention_module)\n",
    "        \n",
    "        self.performance_proj = nn.Linear(config.PERFORMANCE_INPUT_DIM, embedding_dim)\n",
    "        # self.token_embedding = Performance_embedding_module(\n",
    "        #     self.player_embedding_module,\n",
    "        #     self.cross_attended_embedding\n",
    "        # )\n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(config.CONTEXT_LEN, embedding_dim)\n",
    "        self.layers = nn.ModuleList([a_layer(n_head=n_head, n_embd=embedding_dim) for _ in range(num_layers)])\n",
    "        self.layernorm = nn.LayerNorm(embedding_dim)\n",
    "        self.out_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.fantasy_score_prediction_module = fantasy_score_prediction_module\n",
    "        # self.custom_loss = custom_loss\n",
    "        self.proj1 = nn.Linear(3*embedding_dim,embedding_dim)\n",
    "        self.proj2 = nn.Linear(2*embedding_dim,embedding_dim)\n",
    "    def forward(self,player_input,player_performance,team_2,match_info,target = None):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            player_input (Tensor): shape #(B,PLAYER_INPUT_DIM)\n",
    "            player_performance (Tensor): (B,T + 1,PLAYER_PERPROMANCE_DIM)\n",
    "            team_2 (Tensor): (B,T + 1,team2_size,PLAYER_INPUT_DIM)\n",
    "            match_info (Tensor): (B,T + 1,match_info_dim)\n",
    "            target (Tensor, optional): (B,T,_). Defaults to None.\n",
    "        \"\"\"\n",
    "        B, T, _ = match_info.shape #correct it !...\n",
    "        #! T is equal to config.CONTEXT_LEN + 1 \n",
    "        assert T == config.CONTEXT_LEN + 1, 'context_len + 1 is not 17'\n",
    "        player_embed = self.player_embedding_module(player_input) #(B,PERFORMANCE_EMBD_DIM)\n",
    "        player_embed = player_embed.unsqueeze(1).repeat(1,T,1)   # (B,T, PERFORMANCE_EMBD_DIM)\n",
    "        team_2_embed = self.player_embedding_module(team_2) #(B,T,team_2_size,PLAYER_INPUT_DIM) - > (B,T,team_2_size,PLAYER_EMBD_DIM)\n",
    "        attended_embedding = self.cross_attended_embedding(player_embed,team_2_embed,match_info) # (B,T, PERFORMANCE_EMBD_DIM)\n",
    "        \n",
    "        player_performance_embd = self.performance_proj(player_performance.reshape(B*config.CONTEXT_LEN,-1)) # (B*T, PERFORMANCE_INPUT_DIM) => (B*T, PERFORMANCE_EMBD_DIM)\n",
    "        player_performance_embd = player_performance_embd.reshape(B,Config.CONTEXT_LEN,-1) #(B,T,PERFORMANCE_EMBD_DIM) \n",
    "        fused = torch.cat([player_embed[:,:config.CONTEXT_LEN,:],attended_embedding[:,:config.CONTEXT_LEN,:],player_performance_embd[:,:config.CONTEXT_LEN,:]],dim=-1)\n",
    "        # print(fused.shape)\n",
    "        x = self.proj1(fused)\n",
    "        pos_ids = torch.arange(config.CONTEXT_LEN, device=config.DEVICE).unsqueeze(0).expand(B,config.CONTEXT_LEN)  # (B, T)\n",
    "        x = x + self.pos_embedding(pos_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.layernorm(x)\n",
    "        perf_emb = self.out_proj(x)  # (B, T, embedding_dim)\n",
    "        # print(\"perf_emb: shape\")\n",
    "        # print(perf_emb.shape)\n",
    "        pred_fantasy = None\n",
    "        # loss = None\n",
    "        if target is not None:\n",
    "            pred_pref = perf_emb[:,:config.CONTEXT_LEN,:]  # (B, config.Context_len, embedding_dim)\n",
    "            Bp,Tp,D = pred_pref.shape #!Tp ->config.CONTEXT_LEN\n",
    "            # pred_pref = pred_pref.reshape(Bp * Tp, D)\n",
    "            target_match_embd = torch.cat([player_embed[:,1:,:],attended_embedding[:,1:,:]]) # (B,T,2*PERFORMANCE_EMBD_DIM) attended_embd includes the match_info itself as cancatinated \n",
    "            target_match_embd = target_match_embd.view(Bp*Tp,-1)\n",
    "            target_match_embd = self.proj2(target_match_embd) # (Bp,Tp,2*PERFORMANCE_EMBD_DIM) -> (Bp,Tp,PERFORMANCE_EMBD_DIM)\n",
    "            target_match_embd = target_match_embd.reshape(Bp,Tp,-1)\n",
    "            # print(\"target_match_embd: shape\")\n",
    "            # print(target_match_embd.shape)\n",
    "            # print(\"player_embed_context_len: shape\")\n",
    "            # print(player_embed[:,1:,:].shape)\n",
    "            \n",
    "            pred_fantasy = self.fantasy_score_prediction_module(pred_pref,player_embed[:,1:,:],target_match_embd)  # we are back and forth giving player_embd till now we used it or passed it 3 times  \n",
    "            #TODO need to handle if the shape is (B,T,3) 3 refers to batting_fp,bowling_fp,fielding_fp and total_fp is just some of these \n",
    "            \n",
    "            # pred_fantasy_flat = pred_fantasy.view(-1,3) # (B,T,3) ->(B*T,)\n",
    "            # target_flat = target.view(-1,3) #(B*T,)\n",
    "            # loss = self.custom_loss(pred_fantasy,target)\n",
    "            \n",
    "        return pred_fantasy \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "83f74f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with the dropout layers\n",
    "class FantasyScorePrediction(nn.Module):\n",
    "    \"\"\"Predict the Fantasy Score by the <CLS> embedding, the next match_embedding, and the player_embedding.\"\"\"\n",
    "    def __init__(self, embedding_dim=config.PERFORMANCE_EMBD_DIM):\n",
    "        super().__init__()\n",
    "        self.proj1 = nn.Linear(3 * config.PERFORMANCE_EMBD_DIM, 2048)\n",
    "        self.proj2 = nn.Linear(2048, 512)\n",
    "        self.proj3 = nn.Linear(512, 512)\n",
    "        self.proj4 = nn.Linear(512, 256)\n",
    "        self.proj5 = nn.Linear(256, 256)\n",
    "        self.proj6 = nn.Linear(256, 128)\n",
    "        self.proj7 = nn.Linear(128, 128)\n",
    "        self.proj8 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, pred_pref, target_player_embd, target_match_embd): \n",
    "        # Concatenate the embeddings along the last dimension\n",
    "        # print(f\"NI fantasy {pred_pref.shape} & {target_player_embd.shape} & {target_match_embd.shape}\")\n",
    "        x = torch.cat([pred_pref, target_player_embd, target_match_embd], dim=-1)\n",
    "        # print(f\"/n x.shape ->{x.shape} \")\n",
    "        x = F.gelu(self.proj1(x))\n",
    "        x = F.gelu(self.proj2(x))\n",
    "        x = F.gelu(self.proj3(x))\n",
    "        x = F.gelu(self.proj4(x))\n",
    "        x = F.gelu(self.proj5(x))\n",
    "        x = F.gelu(self.proj6(x))\n",
    "        x = F.gelu(self.proj7(x))\n",
    "\n",
    "\n",
    "        x = self.proj8(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc82c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO considering equal weightage to each time step...\n",
    "def custom_loss_component_fps(pred_fantasy: torch.Tensor,target_components: torch.Tensor,target_total: torch.Tensor = None,gamma : float = 0.5,weights :torch.Tensor= None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    computes a weighted loss for multi-component fantasy score prediction.\n",
    "    By including a term that directly compares the sum of the predictions with the\n",
    "    overall target (i.e. total_fp), you ensure that the model learns to get the “big picture” right. \n",
    "    At the same time, the separate losses help the model capture the underlying \n",
    "    structure: where batting and bowling (which are supported by richer\n",
    "    historical data and have more influence) get more weight than fielding.\n",
    "    Args:\n",
    "        pred_fantasy (torch.Tensor): Model prediction of shape (B,T,3), where\n",
    "                        pred_fantasy[:,:,0] -> bating_fp,\n",
    "                        pred_fantasy[:,:1] -> bowling_fp,\n",
    "                        pred_fantasy[:,:2] -> fielding_fp...\n",
    "        target_components (torch.Tensor): Ground truth for the components, shape (B,T,3), with same ordering\n",
    "        target_total (torch.Tensor,optional) : Ground truth total fantasy points of shape (B,T).\n",
    "                                               If not provided, it is computed as sum(target_components, dim=1).\n",
    "        gamma (float,optional) : Trade-off hyperparameter between total and component losses.\n",
    "                                value E (0,1) and default is 0.5.\n",
    "        weights (torch.Tensor,optional) :  weights (torch.Tensor, optional): Weights to be applied to each time step's loss.\n",
    "            Expected shape is (T,). If None, each time step is equally weighted.\n",
    "    Returns : \n",
    "    total_loss (torch.Tensor) : Scalar loss combining total and component-level errors.\n",
    "\n",
    "    \"\"\"\n",
    "    B,T,_ = pred_fantasy.shape\n",
    "    pred_total = pred_fantasy.sum(dim = -1) #(B,T)\n",
    "    \n",
    "    if target_total is None:\n",
    "        target_total = target_components.sum(dim = -1) #(B,T)\n",
    "    abs_error_total = torch.abs(pred_total - target_total) #(B,T)\n",
    "    loss_total = (pred_total - target_total)**2 #(B,T)\n",
    "    comp_weights = torch.tensor([0.4,0.4,0.2],device=pred_fantasy.device, dtype = pred_fantasy.dtype)\n",
    "    loss_components = (((pred_fantasy - target_components)**2)*comp_weights).sum(dim=-1)\n",
    "    combined_loss = gamma * loss_total + (1 - gamma)*loss_components       #(B,T) \n",
    "    if weights is not  None:        \n",
    "        # weights is expected to have shape (T,)\n",
    "\n",
    "        assert weights.dim() == 1 and weights.shape[0] == T, \"weights must be a 1D tensor of shape (T,)\"\n",
    "        combined_loss = combined_loss*weights   \n",
    "    total_loss = combined_loss.mean()\n",
    "    return total_loss\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4b261bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_asymmetric_loss(pred, target, delta=30.0, penalty=2.0, extra_factor=1.5):\n",
    "    \"\"\"\n",
    "    Asymmetric Huber-style loss that emphasizes errors above 10.\n",
    "    \n",
    "    Parameters:\n",
    "        pred (Tensor): Model predictions (floating point)\n",
    "        target (Tensor): Target values (integers)\n",
    "        delta (float): Transition point from quadratic to linear penalty.\n",
    "        penalty (float): Factor to amplify the loss when pred < target.\n",
    "        extra_factor (float): Additional factor to scale the loss in the linear region, if desired.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Mean loss value.\n",
    "    \"\"\"\n",
    "    error = pred - target\n",
    "    abs_err = error.abs()\n",
    "    \n",
    "    # Compute the quadratic (small error) region\n",
    "    loss_small = torch.where(\n",
    "        error < 0,\n",
    "        penalty * 0.5 * error.pow(2),\n",
    "        0.5 * error.pow(2)\n",
    "    )\n",
    "    \n",
    "    # Compute the linear (large error) region with optional extra scaling\n",
    "    base_loss_large = torch.where(\n",
    "        error < 0,\n",
    "        penalty * (delta * (abs_err - 0.5 * delta)),\n",
    "        delta * (abs_err - 0.5 * delta)\n",
    "    )\n",
    "    \n",
    "    # Optionally apply an extra factor to errors that are above the desired threshold\n",
    "    loss_large = torch.where(\n",
    "        abs_err > delta,\n",
    "        extra_factor * base_loss_large,\n",
    "        base_loss_large\n",
    "    )\n",
    "\n",
    "    # for err in abs_err:\n",
    "    #     if err <= delta:\n",
    "    #         print(\"yEs\")\n",
    "    #     else:\n",
    "    #         print(\"Np\")\n",
    "\n",
    "    loss = torch.where(abs_err <= delta, loss_small, loss_large)\n",
    "    # print(abs_err.mean())\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#TODO considering equal weightage to each time step...\n",
    "def custom_loss_component_fps_accounting_for_outliers(\n",
    "    pred_fantasy: torch.Tensor,\n",
    "    target_components: torch.Tensor,\n",
    "    target_total: torch.Tensor = None,\n",
    "    gamma : float = 0.5,weights :torch.Tensor= None,\n",
    "    epoch: int = 0,\n",
    "    total_epoches : int = 100,\n",
    "    decay_rate : float = 0.05, # decides how fast the threshold converges..\n",
    "    upper_threshold : float = 200,\n",
    "    min_threshold : float = 100,\n",
    "    \n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    computes a weighted loss for multi-component fantasy score prediction.\n",
    "    By including a term that directly compares the sum of the predictions with the\n",
    "    overall target (i.e. total_fp), you ensure that the model learns to get the “big picture” right. \n",
    "    At the same time, the separate losses help the model capture the underlying \n",
    "    structure: where batting and bowling (which are supported by richer\n",
    "    historical data and have more influence) get more weight than fielding.\n",
    "    Args:\n",
    "        pred_fantasy (torch.Tensor): Model prediction of shape (B,T,3), where\n",
    "                        pred_fantasy[:,:,0] -> bating_fp,\n",
    "                        pred_fantasy[:,:1] -> bowling_fp,\n",
    "                        pred_fantasy[:,:2] -> fielding_fp...\n",
    "        target_components (torch.Tensor): Ground truth for the components, shape (B,T,3), with same ordering\n",
    "        target_total (torch.Tensor,optional) : Ground truth total fantasy points of shape (B,T).\n",
    "                                               If not provided, it is computed as sum(target_components, dim=1).\n",
    "        gamma (float,optional) : Trade-off hyperparameter between total and component losses.\n",
    "                                value E (0,1) and default is 0.5.\n",
    "        weights (torch.Tensor,optional) :  weights (torch.Tensor, optional): Weights to be applied to each time step's loss.\n",
    "            Expected shape is (T,). If None, each time step is equally weighted.\n",
    "    Returns : \n",
    "    total_loss (torch.Tensor) : Scalar loss combining total and component-level errors.\n",
    "\n",
    "    \"\"\"\n",
    "    B,T,_ = pred_fantasy.shape\n",
    "    pred_total = pred_fantasy.sum(dim = -1) #(B,T)\n",
    "    \n",
    "    if target_total is None:\n",
    "        target_total = target_components.sum(dim = -1) #(B,T)\n",
    "    error = pred_total - target_total\n",
    "    abs_error_total = torch.abs(pred_total - target_total) #(B,T)\n",
    "    # effective_threshold = math.exp(-decay_rate*epoch) # it should start as inf(consider,upper_threshold = 250) and gradualy decrease toward min_threshold as epoch -> total_epoch\n",
    "    effective_threshold = (upper_threshold - min_threshold) * math.exp(-decay_rate * epoch) + min_threshold\n",
    "    # For errors that exceed the threshold reduce their panelity \n",
    "    # otherwise keep their weighting at 1.\n",
    "    weight_total = torch.where(\n",
    "        abs_error_total > effective_threshold,\n",
    "        torch.exp(-decay_rate * (abs_error_total - effective_threshold)),\n",
    "        torch.ones_like(abs_error_total)        \n",
    "    )\n",
    "    loss_total = (pred_total - target_total)**2 #(B,T)\n",
    "    comp_weights = torch.tensor([0.4,0.4,0.2],device=pred_fantasy.device, dtype = pred_fantasy.dtype)\n",
    "    loss_components = (((pred_fantasy - target_components)**2)*comp_weights).sum(dim=-1)\n",
    "    combined_loss = gamma * loss_total + (1 - gamma)*loss_components       #(B,T) \n",
    "    combined_loss = combined_loss*weight_total\n",
    "    if weights is not None:        \n",
    "        # weights is expected to have shape (T,)\n",
    "        assert weights.dim() == 1 and weights.shape[0] == T, \"weights must be a 1D tensor of shape (T,)\"\n",
    "        combined_loss = combined_loss*weights   \n",
    "    total_loss = combined_loss.mean()\n",
    "    return error,total_loss\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "15266654",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_embedding_module = PlayerEmbedding(\n",
    "    in_channels=config.PLAYER_INPUT_DIM,\n",
    "    out_channels=config.PERFORMANCE_EMBD_DIM\n",
    ")\n",
    "\n",
    "# Note: NextFormPredictor expects the *class* MatchEmbedding so that inside it\n",
    "# it can do `match_embedding_module(player_embedding_module)`\n",
    "cross_attention_module = CrossAttention()\n",
    "cross_attention_embedding_module = ApplyCrossAttention\n",
    "fantasy_score_prediction_module = FantasyScorePrediction(\n",
    "    embedding_dim=config.PERFORMANCE_EMBD_DIM\n",
    ")\n",
    "custom_loss = custom_loss_component_fps_accounting_for_outliers\n",
    "\n",
    "model = NextFormPredictor1(\n",
    "    player_embedding_module=player_embedding_module,\n",
    "    cross_attention_module= cross_attention_module,\n",
    "    cross_Attention_embedding_module=cross_attention_embedding_module,\n",
    "    fantasy_score_prediction_module=fantasy_score_prediction_module,\n",
    "    # custom_loss=custom_loss,\n",
    "    embedding_dim=config.PERFORMANCE_EMBD_DIM,\n",
    "    num_layers=6,\n",
    "    n_head=8\n",
    ").to(config.DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def train(model, dataloader, start_epoch=0):\n",
    "    model.to(config.DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    epoch_losses = []\n",
    "    losses = []\n",
    "    for epoch in range(start_epoch, config.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Wrap dataloader with tqdm for progress bar\n",
    "        loop = tqdm(dataloader, leave=True)\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{config.NUM_EPOCHS}]\")\n",
    "\n",
    "        for batch in loop:\n",
    "            # Move tensors to device\n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = v.to(config.DEVICE)\n",
    "\n",
    "            # Unpack batch\n",
    "            player_univ   = batch['univ_features']\n",
    "            context_feats = batch['context_matches']\n",
    "            # team1_players = batch['team1_players']\n",
    "            team2_players = batch['team2_players']\n",
    "            match_info    = batch['match_info']\n",
    "            target_scores = batch['target_scores']\n",
    "            \n",
    "            # print(player_univ.shape)\n",
    "            # print(context_feats.shape)\n",
    "            # print(team1_players.shape)\n",
    "            # print(team2_players.shape)\n",
    "            # print(match_info.shape)\n",
    "            # print(target_scores.shape)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_fantasy = model(\n",
    "                player_univ,\n",
    "                context_feats,\n",
    "                # team1_players,\n",
    "                team2_players,\n",
    "                match_info,\n",
    "                target_scores\n",
    "            )\n",
    "            loss = custom_loss(pred_fantasy,target_scores,epoch=epoch,total_epoches=config.NUM_EPOCHS,min_threshold=50)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "            # Update tqdm progress bar with current loss\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Indivudal Training Losses ')\n",
    "        plt.grid(True)\n",
    "        plt.show()     \n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{config.NUM_EPOCHS} — Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return epoch_losses,losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a9556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/100]:  99%|█████████▊| 73/74 [03:24<00:02,  2.68s/it, loss=1.06e+3]"
     ]
    }
   ],
   "source": [
    "lossi = train(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c654058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3728515"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aab018",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6e48f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0395ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94940dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e47da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f570b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49098c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc70e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686a24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3030c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe254d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e164af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7c3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fbf45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
