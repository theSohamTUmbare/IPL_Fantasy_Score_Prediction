{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ckUw9JF0JC6"
      },
      "source": [
        "# The BASELINE model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70CeiU1w0QDJ"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p4YEOS3nzBVQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.nn as nn\n",
        "from typing import List\n",
        "from torch import Tensor\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve_zbfbzfLo3"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "G9htvclHfODH",
        "outputId": "5bfe8edc-6c51-4af3-ddc8-611cf1ee85e3"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    PLAYERS_SIZE: int = 0\n",
        "    CONTEXT_LEN: int = 16  ## predict the 21st match by the 20\n",
        "    PERFORMANCE_INPUT_DIM = 23\n",
        "    PERFORMANCE_EMBD_DIM: int = 128\n",
        "    PLAYER_INPUT_DIM: int = 25   ## univpalyer\n",
        "    MATCH_INPUT_EMBD: int = 14   ## match_info\n",
        "    NUM_EPOCHS: int = 100\n",
        "    LEARNING_RATE: float = 1e-3\n",
        "    BATCH_SIZE: int = 128\n",
        "    DEVICE: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # MODEL_SAVE_PATH: str = \"/kaggle/working\"\n",
        "    # BASE_DIR: str = '/kaggle/input/flickr8k/'\n",
        "    # CROSSATT_NUM_HEADS: int = 8\n",
        "    CLS_INIT_STD: float = 0.02    ## <CLS> token initialized with std 0.02 from the mean=0\n",
        "    TEST_DATASET_SIZE: int = 180\n",
        "    IDLE_DEVICE: str = 'cpu'\n",
        "    ACCUMULATION_STEPS = 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "grAYxNeZSjLo"
      },
      "outputs": [],
      "source": [
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFI4JDxB0VTy"
      },
      "source": [
        "## Defination of **Losses**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62S8Cr5Tol_7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0fTYTZmzgtp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48MZrwcv0luw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CO-J50z0mWY"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMwXN5x41ycH"
      },
      "source": [
        "#### Data Processing (Normalization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "5XptNBgk0lr8"
      },
      "outputs": [],
      "source": [
        "## INPUT\n",
        "# Universal Player embedding\n",
        "# Match Situation Embedding\n",
        "# Form embedding --> attention to be performed on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "bJEzVOF7IDG0"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'venues.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m venues_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvenues.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m venues_df.head()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\umbar\\OneDrive\\Documents\\NLP\\my_GPT\\gptvenv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\umbar\\OneDrive\\Documents\\NLP\\my_GPT\\gptvenv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\umbar\\OneDrive\\Documents\\NLP\\my_GPT\\gptvenv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\umbar\\OneDrive\\Documents\\NLP\\my_GPT\\gptvenv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\umbar\\OneDrive\\Documents\\NLP\\my_GPT\\gptvenv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'venues.csv'"
          ]
        }
      ],
      "source": [
        "venues_df = pd.read_csv(\"venues.csv\")\n",
        "venues_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbmPp-tF1ms8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def handle_missing_data(df, missing_handling):\n",
        "    for col, method in missing_handling.items():\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Column '{col}' not found in the DataFrame.\")\n",
        "        if method not in ['mean', 'median', 'mode', 'drop']:\n",
        "            raise ValueError(\"Invalid method for handling missing data\")\n",
        "        if method in ['mean', 'median'] and not pd.is_numeric_dtype(df[col]):\n",
        "            raise ValueError(f\"Column '{col}' is not numerical and cannot be imputed with {method}.\")\n",
        "        if method == 'mean':\n",
        "            df[col].fillna(df[col].mean(), inplace=True)\n",
        "        elif method == 'median':\n",
        "            df[col].fillna(df[col].median(), inplace=True)\n",
        "        elif method == 'mode':\n",
        "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "        elif method == 'drop':\n",
        "            df.dropna(subset=[col], inplace=True)\n",
        "    return df\n",
        "\n",
        "def clean_data(df, operations):\n",
        "    for op in operations:\n",
        "        if op['type'] == 'remove_duplicates':\n",
        "            df.drop_duplicates(inplace=True)\n",
        "        elif op['type'] == 'convert_type':\n",
        "            col = op['column']\n",
        "            new_type = op['new_type']\n",
        "            df[col] = df[col].astype(new_type)\n",
        "        elif op['type'] == 'drop_columns':\n",
        "            cols = op['columns']\n",
        "            df.drop(columns=cols, inplace=True)\n",
        "        elif op['type'] == 'rename_columns':\n",
        "            mapping = op['mapping']\n",
        "            df.rename(columns=mapping, inplace=True)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown operation type: {op['type']}\")\n",
        "    return df\n",
        "\n",
        "def scale_numerical_columns(df, numerical_cols, method='min_max'):\n",
        "    if method == 'min_max':\n",
        "        scaler = preprocessing.MinMaxScaler()\n",
        "    elif method == 'standard':\n",
        "        scaler = preprocessing.StandardScaler()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid scaling method\")\n",
        "    for col in numerical_cols:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Column '{col}' not found in the DataFrame.\")\n",
        "        if not pd.is_numeric_dtype(df[col]):\n",
        "            raise ValueError(f\"Column '{col}' is not numerical and cannot be scaled.\")\n",
        "        df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n",
        "    return df\n",
        "\n",
        "def encode_categorical_columns(df, categorical_cols, method='one_hot'):\n",
        "    if method == 'one_hot':\n",
        "        df = pd.get_dummies(df, columns=categorical_cols)\n",
        "    elif method == 'label':\n",
        "        for col in categorical_cols:\n",
        "            if col not in df.columns:\n",
        "                raise ValueError(f\"Column '{col}' not found in the DataFrame.\")\n",
        "            if df[col].dtype == 'object':\n",
        "                df[col] = pd.factorize(df[col])[0]\n",
        "            else:\n",
        "                raise ValueError(f\"Column '{col}' is not categorical and cannot be label encoded.\")\n",
        "    else:\n",
        "        raise ValueError(\"Invalid encoding method\")\n",
        "    return df\n",
        "\n",
        "def preprocess_csv(input_file, output_file, missing_handling=None, cleaning_operations=None, numerical_cols=None, categorical_cols=None, scale_numerical=False, encode_categorical=False):\n",
        "    df = pd.read_csv(input_file)\n",
        "    if missing_handling:\n",
        "        df = handle_missing_data(df, missing_handling)\n",
        "    if cleaning_operations:\n",
        "        df = clean_data(df, cleaning_operations)\n",
        "    if scale_numerical:\n",
        "        if not numerical_cols:\n",
        "            raise ValueError(\"numerical_cols must be provided if scale_numerical is True\")\n",
        "        df = scale_numerical_columns(df, numerical_cols)\n",
        "    if encode_categorical:\n",
        "        if not categorical_cols:\n",
        "            raise ValueError(\"categorical_cols must be provided if encode_categorical is True\")\n",
        "        df = encode_categorical_columns(df, categorical_cols)\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "# # Example usage\n",
        "# input_file = 'data.csv'\n",
        "# output_file = 'preprocessed_data.csv'\n",
        "# missing_handling = {'age': 'mean', 'income': 'median'}\n",
        "# cleaning_operations = [{'type': 'remove_duplicates'}, {'type': 'convert_type', 'column': 'age', 'new_type': 'int'}, {'type': 'drop_columns', 'columns': ['id']}]\n",
        "# numerical_cols = ['age', 'income']\n",
        "# categorical_cols = ['gender', 'country']\n",
        "# preprocess_csv(input_file, output_file, missing_handling, cleaning_operations, numerical_cols, categorical_cols, scale_numerical=True, encode_categorical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM9gq79B169b"
      },
      "outputs": [],
      "source": [
        "dfp = pd.read_csv(\"players.csv\")\n",
        "dfp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOlbJjhkWZRn"
      },
      "outputs": [],
      "source": [
        "dfm = pd.read_csv(\"matches.csv\")\n",
        "dfm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4Brx9GIWZOq"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaling = MinMaxScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeWLLvTWxrWI"
      },
      "outputs": [],
      "source": [
        "scaling.fit_transform(dfp[[\"all col except the occpuation one hot\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnstBTxxxrSs"
      },
      "outputs": [],
      "source": [
        "scaling.fit_transform(dfm['all col'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QocbPHpgB_4G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6nQ70N3B_0q",
        "outputId": "5f1b7f3b-1308-419b-c59f-6b54766f4569"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('updated_players.csv')\n",
        "\n",
        "# Display the DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "print(df.head())  # Show the first few rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irD5MfDwB_xn",
        "outputId": "26e1901b-0d62-42e8-baf4-4d8a320cd745"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = df.drop(columns=['batting_style', 'bowling_style', 'name'])\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(\"\\nModified DataFrame:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJSmh9hwB_u6",
        "outputId": "7480a6c3-0175-45ef-b9a5-29d304cb18ca"
      },
      "outputs": [],
      "source": [
        "# Get the list of all column names\n",
        "columns = df.columns.tolist()\n",
        "\n",
        "# Print the column names\n",
        "print(columns)\n",
        "len(columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSTPUFikNb2m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNEpXiwQJpWK",
        "outputId": "1c003988-c2d5-4a6c-ef20-6c0c74a029da"
      },
      "outputs": [],
      "source": [
        "# Define lists of positions for the various groups (using lowercase for comparison)\n",
        "allrounder_positions = ['allrounder', 'batting allrounder', 'bowling allrounder']\n",
        "batter_positions = ['batter', 'wicketkeeper batter', 'middle-order batter', 'opening batter', 'top-order batter']\n",
        "bowler_positions = ['bowler']\n",
        "\n",
        "# Define the values to skip: do not override if player_position is \"Wicketkeeper\" or blank\n",
        "skip_positions = ['wicketkeeper', '']\n",
        "\n",
        "# Create a lowercase version of the player_position column (and strip any extra spaces)\n",
        "df['player_position_clean'] = df['player_position'].str.lower().str.strip()\n",
        "\n",
        "# Process all-rounders:\n",
        "mask_allrounder = ~df['player_position_clean'].isin(skip_positions) & df['player_position_clean'].isin(allrounder_positions)\n",
        "df.loc[mask_allrounder, ['all_rounder', 'batting', 'bowling']] = [True, False, False]\n",
        "\n",
        "# Process batters:\n",
        "mask_batter = ~df['player_position_clean'].isin(skip_positions) & df['player_position_clean'].isin(batter_positions)\n",
        "df.loc[mask_batter, ['batting', 'bowling', 'all_rounder']] = [True, False, False]\n",
        "\n",
        "# Process bowlers:\n",
        "mask_bowler = ~df['player_position_clean'].isin(skip_positions) & (df['player_position_clean'] == 'bowler')\n",
        "df.loc[mask_bowler, ['bowling', 'batting', 'all_rounder']] = [True, False, False]\n",
        "\n",
        "# (Optional) Drop the helper column if you don't need it anymore\n",
        "df.drop(columns=['player_position_clean'], inplace=True)\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq4Q5SfIJptI",
        "outputId": "266d1ced-8e9d-45a2-dd92-877120928567"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['player_position'], inplace=True)\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQyna7pKB_r4",
        "outputId": "0e35e53a-88f8-4e07-a346-289286b22631"
      },
      "outputs": [],
      "source": [
        "null_counts = df.isnull().sum()\n",
        "\n",
        "# Print the results\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsDaCkT0JpTa",
        "outputId": "c04d0402-196b-4933-b519-6dacc6196f6c"
      },
      "outputs": [],
      "source": [
        "if 'strike rate in all matches combined' in df.columns:\n",
        "    df['strike rate in all matches combined'] = df['strike rate in all matches combined'].fillna(0)\n",
        "\n",
        "if 'age' in df.columns:\n",
        "    median_age = df['age'].median()\n",
        "    df['age'] = df['age'].fillna(median_age)\n",
        "\n",
        "# For boolean columns: Fill missing values with False\n",
        "for col in ['batting', 'bowling', 'all_rounder']:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].fillna(False).infer_objects(copy=False)\n",
        "\n",
        "if 'cricinfo_id' in df.columns:\n",
        "    df['cricinfo_id'] = df['cricinfo_id'].fillna(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "X6oum16QPQt6",
        "outputId": "9bfe109e-42fe-4e1d-b91f-23bbfff64141"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb7mcOg1PQrg",
        "outputId": "de7c48c6-eef1-4fe5-8985-babceef5ae95"
      },
      "outputs": [],
      "source": [
        "# ----- Step 3: Data Type Conversion -----\n",
        "# Define numeric columns (adjust as needed)\n",
        "numeric_cols = [ 'matches', 'strike rate in all matches combined', 'Total Runs Scored in all matches combined',\n",
        "                'no. of fours', 'no. of sixes', 'highest runs scored in a match', 'Balls Played till now',\n",
        "                'no. of dot_balls', 'maiden_overs', 'run conceded', 'Average Economy Rate', 'no. of wickets',\n",
        "                'no. of balls thrown', 'highest wickets taken in a match', 'total centuries',\n",
        "                'total halfcenturies', 'total catches', 'total stumps', 'total direct_runouts',\n",
        "                'total indirect_runouts', 'total avg fp', 'age']\n",
        "\n",
        "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "len(numeric_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EiDU_BTPQos"
      },
      "outputs": [],
      "source": [
        "# Ensure boolean columns are of type bool\n",
        "bool_cols = ['batting', 'bowling', 'all_rounder']\n",
        "df[bool_cols] = df[bool_cols].astype(bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy8BF5voRSlk",
        "outputId": "7d050f2e-c585-4c16-c2e0-88bfb290a9e6"
      },
      "outputs": [],
      "source": [
        "# ----- Step 4: Handle Duplicates -----\n",
        "print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())\n",
        "df = df.drop_duplicates()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "Cz6c7XjQRShU",
        "outputId": "bd947baf-a7e1-4ac2-80c0-0ce49d85771a"
      },
      "outputs": [],
      "source": [
        "# Distribution of 'age'\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(df['age'], kde=True, bins=20)\n",
        "plt.title('Age Distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8mMdr4_RSen"
      },
      "outputs": [],
      "source": [
        "if 'age' in df.columns:\n",
        "    median_age = df['age'].median()\n",
        "    # Replace 0 values with median\n",
        "    df['age'] = df['age'].replace(0, median_age)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "b7Ie5B0YSbFn",
        "outputId": "afa0d355-b396-4785-e001-4ad466a270c1"
      },
      "outputs": [],
      "source": [
        "# Boxplot for 'Total Runs Scored in all matches combined'\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(x=df['Total Runs Scored in all matches combined'])\n",
        "plt.title('Boxplot for Total Runs Scored')\n",
        "plt.xlabel('Total Runs Scored')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vxn2F0s4SbAA",
        "outputId": "68961806-b3d8-4a63-f15b-e077115daace"
      },
      "outputs": [],
      "source": [
        "# Correlation heatmap for numeric columns\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je7LbN1cSa9G",
        "outputId": "12c947ca-5ba8-4c54-97cd-2d7b2ac7700c"
      },
      "outputs": [],
      "source": [
        "# # ----- Step 7: Data Normalization/Scaling -----\n",
        "# # Scale the numeric columns using StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "# df_scaled = df.copy()\n",
        "# df_scaled[numeric_cols] = scaler.fit_transform(df_scaled[numeric_cols])\n",
        "# print(\"\\nSample of scaled numeric features:\")\n",
        "# print(df_scaled[numeric_cols].head())\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = df.copy()\n",
        "df_scaled[numeric_cols] = scaler.fit_transform(df_scaled[numeric_cols])\n",
        "\n",
        "print(\"\\nSample of normalized numeric features:\")\n",
        "print(df_scaled[numeric_cols].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "CP5C90KTSa3_",
        "outputId": "c0319c4e-f6f7-42d1-84d3-ee1b029961c7"
      },
      "outputs": [],
      "source": [
        "# Distribution of 'age'\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(df_scaled['age'], kde=True, bins=20)\n",
        "plt.title('Age Distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "kwxz_NMYS6zx",
        "outputId": "4077f3bf-2a1c-4dc1-fd27-7440ac28a323"
      },
      "outputs": [],
      "source": [
        "# Boxplot for 'Total Runs Scored in all matches combined'\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(x=df_scaled['Total Runs Scored in all matches combined'])\n",
        "plt.title('Boxplot for Total Runs Scored')\n",
        "plt.xlabel('Total Runs Scored')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cTR_VfdZS6wu",
        "outputId": "9815dc73-85c3-4b60-b305-df0136646a00"
      },
      "outputs": [],
      "source": [
        "# Correlation heatmap for numeric columns\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgRVjBMUUIAq",
        "outputId": "a06146a3-4727-42b7-db74-1bc65e93715b"
      },
      "outputs": [],
      "source": [
        "selected_features = ['player_id', 'cricinfo_id'] + numeric_cols + ['batting', 'bowling', 'all_rounder']\n",
        "df_selected = df_scaled[selected_features]\n",
        "print(\"\\nSelected features for further analysis/modeling:\")\n",
        "print(df_selected.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXTlBUZzT_D7",
        "outputId": "2d3c838e-1619-4bde-e571-08024aa76edc"
      },
      "outputs": [],
      "source": [
        "df_selected.to_csv('cleaned_universal_player.csv', index=False)\n",
        "print(\"\\nCleaned data saved to 'cleaned_data.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRb7i7qSP2TO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7ZhZ_PyP2Qh",
        "outputId": "d54d0c5c-b2a8-40a5-ccf7-eeecb8b12113"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/4a8a2e3b.csv')\n",
        "\n",
        "# Display the DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "print(df.head())  # Show the first few rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4muzci4zT_BG",
        "outputId": "e2fccfc7-4126-4982-abf1-a21f4f33c134"
      },
      "outputs": [],
      "source": [
        "# Get the list of all column names\n",
        "columns = df.columns.tolist()\n",
        "\n",
        "# Print the column names\n",
        "print(columns)\n",
        "len(columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZp723IyQ-Li",
        "outputId": "ac42e644-b495-4886-b921-09fd53885e84"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=['date', 'event', 'teamname', 'overs_bowled'])\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(\"\\nModified DataFrame:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHdKy20IQ-I-",
        "outputId": "afef84d8-b9f8-49f5-f33b-fa97826416bb"
      },
      "outputs": [],
      "source": [
        "null_counts = df.isnull().sum()\n",
        "\n",
        "# Print the results\n",
        "print(\"Null values in each column:\")\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wYYok6gRifc"
      },
      "outputs": [],
      "source": [
        "# if 'overs_bowled' in df.columns:\n",
        "#     df['overs_bowled'] = df['overs_bowled'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "OuC5GHvpS0kP",
        "outputId": "5294aa27-ed53-474e-fca6-9a33056edd9b"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dtcV84bS0hr",
        "outputId": "a6da27c5-853f-41a4-b1ed-70973cafb8ee"
      },
      "outputs": [],
      "source": [
        "# ----- Step 3: Data Type Conversion -----\n",
        "# Define numeric columns (adjust as needed)\n",
        "numeric_cols = [ 'batting_position', 'runs','balls', 'fours', 'sixes', 'strike_rate',\n",
        "                'overs', 'total_balls', 'dots', 'maidens',\n",
        "                'conceded', 'fours_conceded', 'sixes_conceded',\n",
        "                'wickets', 'LBW', 'Bowled', 'noballs',\n",
        "                'wides', 'economy_rate', 'catches', 'stumping', 'direct_hit', 'indirect_hit']\n",
        "\n",
        "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "len(numeric_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a98Hs_AYS0ey"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Normalize the numeric columns\n",
        "scaler = MinMaxScaler()\n",
        "df_normalized = df.copy()\n",
        "df_normalized[numeric_cols] = scaler.fit_transform(df[numeric_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "niJbt5s7S0b9",
        "outputId": "a7d04d63-f205-4e0d-b207-347a48dd5ca7"
      },
      "outputs": [],
      "source": [
        "# Correlation Matrix\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "9KXNVTK2Q-GE",
        "outputId": "78423929-cec7-49e1-8bd8-81b49ae5ce69"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "selected_features = ['match_id'] + numeric_cols + ['strike_rate_fp', 'batting_fp', 'bowling_fp', 'fielding_fp', 'total_fp']\n",
        "df_selected = df_normalized[selected_features]\n",
        "print(\"\\nSelected features for further analysis/modeling:\")\n",
        "df_selected.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTDz4JwkXQEp",
        "outputId": "09708e56-4e0d-4146-cdbe-37c8bb28f6ff"
      },
      "outputs": [],
      "source": [
        "df_selected.to_csv('cleaned_4a8a2e3b.csv', index=False)\n",
        "print(\"\\nCleaned data saved to 'cleaned_data.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG-EyS8UXQA6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS9nhysUXP-K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCZK5_NJsV1o"
      },
      "outputs": [],
      "source": [
        "## WEATHER FECTHING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw7HrRUVtF-R",
        "outputId": "76734ac0-4b98-450e-d520-4b48b5624424"
      },
      "outputs": [],
      "source": [
        "!pip install openmeteo_requests\n",
        "!pip install retry-requests\n",
        "!pip install requests_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7sYZzZFsVys"
      },
      "outputs": [],
      "source": [
        "import openmeteo_requests\n",
        "import requests_cache\n",
        "import pandas as pd\n",
        "from retry_requests import retry\n",
        "import time\n",
        "from tqdm import tqdm  # Import tqdm for progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc82pfJjsVv4",
        "outputId": "18ca7242-6212-4455-b2ef-44c0d6e9ed44"
      },
      "outputs": [],
      "source": [
        "# Load match data\n",
        "matches_df = pd.read_csv(\"/content/updated_matches_info1.csv\")\n",
        "\n",
        "# Setup the Open-Meteo API client with cache and retry on error\n",
        "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
        "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
        "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
        "\n",
        "# Weather API URL\n",
        "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
        "\n",
        "# List to store weather data\n",
        "weather_data = []\n",
        "\n",
        "# Use tqdm to show progress over the DataFrame rows\n",
        "for index, row in tqdm(matches_df.iterrows(), total=matches_df.shape[0], desc=\"Processing matches\"):\n",
        "    match_id = row[\"match_id\"]\n",
        "    latitude = row[\"venue_lat\"]\n",
        "    longitude = row[\"venue_long\"]\n",
        "    match_date = row[\"match_date\"]\n",
        "\n",
        "    # Skip rows with missing lat/lon\n",
        "    if pd.isna(latitude) or pd.isna(longitude):\n",
        "        continue\n",
        "\n",
        "    params = {\n",
        "        \"latitude\": latitude,\n",
        "        \"longitude\": longitude,\n",
        "        \"start_date\": match_date,\n",
        "        \"end_date\": match_date,\n",
        "        \"daily\": \"apparent_temperature_mean\",\n",
        "        \"hourly\": [\"relative_humidity_2m\", \"apparent_temperature\", \"precipitation\", \"is_day\",\n",
        "                   \"temperature_2m_spread\", \"temperature_2m\"]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        responses = openmeteo.weather_api(url, params=params)\n",
        "        response = responses[0]\n",
        "\n",
        "        # Process daily data\n",
        "        daily = response.Daily()\n",
        "        daily_apparent_temperature_mean = daily.Variables(0).ValuesAsNumpy()[0]\n",
        "\n",
        "        # Process hourly data\n",
        "        hourly = response.Hourly()\n",
        "        hourly_relative_humidity_2m = hourly.Variables(0).ValuesAsNumpy().mean()\n",
        "        hourly_apparent_temperature = hourly.Variables(1).ValuesAsNumpy().mean()\n",
        "        hourly_precipitation = hourly.Variables(2).ValuesAsNumpy().sum()\n",
        "        hourly_is_day = hourly.Variables(3).ValuesAsNumpy().mean()\n",
        "        hourly_temperature_2m_spread = hourly.Variables(4).ValuesAsNumpy().mean()\n",
        "        hourly_temperature_2m = hourly.Variables(5).ValuesAsNumpy().mean()\n",
        "\n",
        "        weather_data.append([\n",
        "            match_id, match_date, latitude, longitude, daily_apparent_temperature_mean,\n",
        "            hourly_relative_humidity_2m, hourly_apparent_temperature,\n",
        "            hourly_precipitation, hourly_is_day, hourly_temperature_2m_spread,\n",
        "            hourly_temperature_2m\n",
        "        ])\n",
        "\n",
        "        # Delay to avoid hitting API limits\n",
        "        time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for match {match_id}: {e}\")\n",
        "\n",
        "# Convert to DataFrame and save\n",
        "weather_df = pd.DataFrame(weather_data, columns=[\n",
        "    \"match_id\", \"match_date\", \"latitude\", \"longitude\", \"daily_apparent_temperature_mean\",\n",
        "    \"hourly_relative_humidity_2m\", \"hourly_apparent_temperature\",\n",
        "    \"hourly_precipitation\", \"hourly_is_day\", \"hourly_temperature_2m_spread\",\n",
        "    \"hourly_temperature_2m\"\n",
        "])\n",
        "\n",
        "weather_df.to_csv(\"weather.csv\", index=False)\n",
        "print(\"Weather data saved to weather.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "o7R2TPJXsVtO",
        "outputId": "9e20e19a-48ee-4cf2-b228-e004e201005b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('cleaned_universal_player.csv')\n",
        "\n",
        "# Display the DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "df.head()  # Show the first few rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "FshYtMc3CCZU",
        "outputId": "2dba4968-456b-4c46-aa90-4f98a7eb0900"
      },
      "outputs": [],
      "source": [
        "df[['batting', 'bowling', 'all_rounder']] = df[['batting', 'bowling', 'all_rounder']].astype(int)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqCC73NTCCV6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2TJiVlGsVno"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wWrj0-o1655"
      },
      "outputs": [],
      "source": [
        "## Concatation of the Input Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eljnjnGk2VDz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFOJQcoY2Z6d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB5JsauI219m"
      },
      "source": [
        "#### Create the DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZZm6eBO2lsV"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from torch.utils.data import Dataset\n",
        "\n",
        "# class PlayerMatchDataset(Dataset):\n",
        "#     \"\"\"\n",
        "#     Dataset for autoregressive next-match prediction.\n",
        "\n",
        "#     For each sample:\n",
        "#       1. Given a player_id, retrieve universal player features from universal_player.csv.\n",
        "#          (We drop the 'player_id' and 'cricinfo_id' from the input features.)\n",
        "#       2. Retrieve all matches for that player from player_matches.csv (sorted by date).\n",
        "#          The performance columns include:\n",
        "#            batting_position, runs, balls, fours, sixes, strike_rate, overs, total_balls, dots,\n",
        "#            maidens, conceded, fours_conceded, sixes_conceded, wickets, LBW, Bowled, noballs,\n",
        "#            wides, economy_rate, catches, stumping, direct_hit, indirect_hit, strike_rate_fp,\n",
        "#            batting_fp, bowling_fp, fielding_fp, total_fp.\n",
        "#          We drop 'match_id' and the fantasy-breakdown columns\n",
        "#          ('strike_rate_fp', 'batting_fp', 'bowling_fp', 'fielding_fp', 'total_fp') when forming the input vector.\n",
        "#       3. Randomly sample a contiguous window of (context_len + 1) matches from this player.\n",
        "#          The first context_len matches serve as input and matches 2...context_len+1 yield the target fantasy scores.\n",
        "#       4. For each match in the window:\n",
        "#          - Load the corresponding match_players CSV from the match_players folder (file: '{match_id}.csv').\n",
        "#          - Separate the player_ids into team1 and team2 (based on the 'Team' column).\n",
        "#          - Retrieve their universal features from universal_player.csv.\n",
        "#          - Load the match info from the match_info folder (file: '{match_id}_info.csv').\n",
        "#       5. Return a dictionary containing:\n",
        "#          - 'univ_features': the player's universal features.\n",
        "#          - 'context_matches': an array of performance features for the context matches.\n",
        "#          - 'target_scores': an array of target fantasy scores.\n",
        "#          - 'team1_players': list of team1 players' universal features for each match.\n",
        "#          - 'team2_players': list of team2 players' universal features for each match.\n",
        "#          - 'match_info': list of match info dictionaries for each match.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, universal_player_csv, player_matches_csv, match_players_dir, match_info_dir, context_len=5, transform=None):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             universal_player_csv (str): Path to universal_player.csv.\n",
        "#             player_matches_csv (str): Path to player_matches.csv.\n",
        "#             match_players_dir (str): Directory containing match_players CSV files.\n",
        "#             match_info_dir (str): Directory containing match info CSV files.\n",
        "#             context_len (int): Number of context matches to use as input (the target will be matches 2 ... context_len+1).\n",
        "#             transform (callable, optional): Optional transform to be applied on a sample.\n",
        "#         \"\"\"\n",
        "#         self.context_len = context_len\n",
        "#         self.transform = transform\n",
        "\n",
        "#         # Load universal player features and set player_id as index.\n",
        "#         self.univ_df = pd.read_csv(universal_player_csv)\n",
        "#         self.univ_df = self.univ_df.set_index('player_id')\n",
        "#         # Drop cricinfo_id since it is redundant.\n",
        "#         self.univ_features = self.univ_df.drop(columns=['cricinfo_id'], errors='ignore')\n",
        "\n",
        "#         # Load player matches and sort by date.\n",
        "#         self.matches_df = pd.read_csv(player_matches_csv)\n",
        "#         self.matches_df['date'] = pd.to_datetime(self.matches_df['date'])\n",
        "#         self.matches_df = self.matches_df.sort_values(by='date')\n",
        "\n",
        "#         # Build an index: for each player_id, list the row indices in matches_df.\n",
        "#         self.player_match_indices = {}\n",
        "#         for player_id, group in self.matches_df.groupby('player_id'):\n",
        "#             indices = group.index.tolist()\n",
        "#             # Only include players with at least (context_len + 1) matches.\n",
        "#             if len(indices) >= (self.context_len + 1):\n",
        "#                 self.player_match_indices[player_id] = indices\n",
        "\n",
        "#         self.match_players_dir = match_players_dir\n",
        "#         self.match_info_dir = match_info_dir\n",
        "\n",
        "#         # Valid player_ids (those with enough matches)\n",
        "#         self.player_ids = list(self.player_match_indices.keys())\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.player_ids)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         \"\"\"\n",
        "#         Returns a dictionary with:\n",
        "#         - 'player_id': the player's id.\n",
        "#         - 'univ_features': universal features for the player.\n",
        "#         - 'context_matches': performance features for the first context_len matches.\n",
        "#         - 'target_scores': fantasy scores from matches 2 to context_len+1.\n",
        "#         - 'team1_players': for each match, universal features for team1 players.\n",
        "#         - 'team2_players': for each match, universal features for team2 players.\n",
        "#         - 'match_info': match-specific info for each match.\n",
        "#         \"\"\"\n",
        "#         # Select the player\n",
        "#         player_id = self.player_ids[idx]\n",
        "#         # Get the player's universal features (as a numpy array of floats)\n",
        "#         player_univ = self.univ_features.loc[player_id].values.astype(float)\n",
        "#         print(player_univ)\n",
        "        \n",
        "#         # Get indices for all matches of this player.\n",
        "#         match_indices = self.player_match_indices[player_id]\n",
        "#         # Randomly select a contiguous window of (context_len + 1) matches.\n",
        "#         start_idx = np.random.randint(0, len(match_indices) - self.context_len)\n",
        "#         selected_indices = match_indices[start_idx : start_idx + self.context_len + 1]\n",
        "#         selected_matches = self.matches_df.loc[selected_indices]\n",
        "\n",
        "#         # For input, we drop the columns that are not used: 'match_id' and fantasy-breakdown columns.\n",
        "#         exclude_cols = ['teamname', 'match_id', 'strike_rate_fp', 'batting_fp', 'bowling_fp', 'fielding_fp', 'total_fp']\n",
        "#         # Target fantasy scores (assumed to be in column 'total_fp') from matches 2 ... context_len+1.\n",
        "#         target_scores = selected_matches.iloc[1:self.context_len+1]['total_fp'].values.astype(float)\n",
        "#         context_matches = selected_matches.iloc[:self.context_len].drop(columns=exclude_cols, errors='ignore')\n",
        "\n",
        "#         # For each match in the window, retrieve team player ids and match info.\n",
        "#         team1_players_list = [] \n",
        "#         team2_players_list = []\n",
        "#         match_info_list = []\n",
        "#         for _, match in selected_matches.iterrows():\n",
        "#             match_id = match['match_id']\n",
        "#             # Load match players file (expects a file named \"{match_id}.csv\")\n",
        "#             match_players_file = os.path.join(self.match_players_dir, f\"{match_id}.csv\")\n",
        "#             match_players_df = pd.read_csv(match_players_file)\n",
        "            \n",
        "#             # Determine the player's team for this match.\n",
        "#             player_team_series = match_players_df[match_players_df['player_id'] == player_id]['Team']\n",
        "#             if not player_team_series.empty:\n",
        "#                 player_team = player_team_series.iloc[0]\n",
        "#             else:\n",
        "#                 # If player's team is not found, default to first team (or handle as needed).\n",
        "#                 player_team = match_players_df.iloc[0]['Team']\n",
        "            \n",
        "#             # Use the player's team name to split the teams.\n",
        "#             team1_ids = match_players_df[match_players_df['Team'] == player_team]['player_id'].tolist()\n",
        "#             team2_ids = match_players_df[match_players_df['Team'] != player_team]['player_id'].tolist()\n",
        "\n",
        "#             # Retrieve universal features for these players.\n",
        "#             team1_features = self.univ_features.reindex(team1_ids).dropna().values.astype(float)\n",
        "#             team2_features = self.univ_features.reindex(team2_ids).dropna().values.astype(float)\n",
        "\n",
        "#             team1_players_list.append(team1_features)\n",
        "#             print(team1_players_list.size())\n",
        "#             team2_players_list.append(team2_features)\n",
        "#             print(len(team2_players_list))\n",
        "            \n",
        "#             # Load match info file (expects a file named \"{match_id}_info.csv\")\n",
        "#             match_info_file = os.path.join(self.match_info_dir, f\"{match_id}_info.csv\")\n",
        "#             match_info_df = pd.read_csv(match_info_file)\n",
        "#             match_info_list.append(match_info_df.to_dict(orient='list'))\n",
        "#             print(len(match_info_list))\n",
        "\n",
        "#         sample = {\n",
        "#             'player_id': player_id,\n",
        "#             'univ_features': player_univ,                         # Universal features for the player.\n",
        "#             'context_matches': context_matches.values.astype(float),  # Performance features for context matches.\n",
        "#             'target_scores': target_scores,                       # Target fantasy scores.\n",
        "#             'team1_players': team1_players_list,                  # List (per match) of team1 players' universal features.\n",
        "#             'team2_players': team2_players_list,                  # List (per match) of team2 players' universal features.\n",
        "#             'match_info': match_info_list                         # List (per match) of match info dictionaries.\n",
        "#         }\n",
        "\n",
        "#         if self.transform:\n",
        "#             sample = self.transform(sample)\n",
        "#         return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PlayerMatchDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for autoregressive next-match prediction.\n",
        "\n",
        "    For each sample:\n",
        "      1. Given a player_id, retrieve universal player features from universal_player.csv.\n",
        "         (The 'player_id' and 'cricinfo_id' columns are dropped from the input features.)\n",
        "      2. Retrieve all matches for that player from a directory of player match files\n",
        "         (each file is named \"{player_id}.csv\" and contains the matches for that player,\n",
        "         already sorted by date).\n",
        "         The performance columns include:\n",
        "           batting_position, runs, balls, fours, sixes, strike_rate, overs, total_balls, dots,\n",
        "           maidens, conceded, fours_conceded, sixes_conceded, wickets, LBW, Bowled, noballs,\n",
        "           wides, economy_rate, catches, stumping, direct_hit, indirect_hit, strike_rate_fp,\n",
        "           batting_fp, bowling_fp, fielding_fp, total_fp.\n",
        "         We drop 'match_id' and the fantasy-breakdown columns\n",
        "         ('strike_rate_fp', 'batting_fp', 'bowling_fp', 'fielding_fp', 'total_fp') when forming the input vector.\n",
        "      3. Randomly sample a contiguous window of (context_len + 1) matches for this player.\n",
        "         The first context_len matches serve as input and matches 2 ... context_len+1 yield the target fantasy scores.\n",
        "      4. For each match in the window:\n",
        "         - Load the corresponding match players CSV from the match_players folder (file: '{match_id}.csv').\n",
        "         - Determine the player's team for that match and then separate player_ids into:\n",
        "              team1_ids: those belonging to the same team as the player,\n",
        "              team2_ids: those belonging to the other team.\n",
        "         - Retrieve their universal features from universal_player.csv.\n",
        "         - Retrieve match info from a single match_info CSV (by matching on match_id).\n",
        "      5. Return a dictionary containing:\n",
        "         - 'player_id': the player's id.\n",
        "         - 'univ_features': the player's universal features.\n",
        "         - 'context_matches': a numpy array of performance features for the context matches.\n",
        "         - 'target_scores': a numpy array of target fantasy scores (for matches 2 ... context_len+1).\n",
        "         - 'team1_players': list (per match) of team1 players' universal features.\n",
        "         - 'team2_players': list (per match) of team2 players' universal features.\n",
        "         - 'match_info': list (per match) of match info dictionaries.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, universal_player_csv, player_matches_dir, match_players_dir, match_info_csv, context_len=25, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            universal_player_csv (str): Path to universal_player.csv.\n",
        "            player_matches_dir (str): Directory containing CSV files for each player's matches (named '{player_id}.csv').\n",
        "            match_players_dir (str): Directory containing match_players CSV files.\n",
        "            match_info_csv (str): Path to the CSV file containing match info for all matches.\n",
        "            context_len (int): Number of context matches to use as input \n",
        "                                (target will be matches 2 ... context_len+1).\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.context_len = context_len\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load universal player features and set player_id as index.\n",
        "        self.univ_df = pd.read_csv(universal_player_csv)\n",
        "        self.univ_df = self.univ_df.set_index('player_id')\n",
        "        # Drop 'cricinfo_id' as it is redundant.\n",
        "        self.univ_features = self.univ_df.drop(columns=['cricinfo_id'], errors='ignore')\n",
        "\n",
        "        # Instead of a single file for player matches, we assume a directory where each player's\n",
        "        # matches are stored in a file named '{player_id}.csv'. We build a list of valid player_ids\n",
        "        # by checking which player match files exist and ensuring they have at least (context_len+1) rows.\n",
        "        self.player_matches_dir = player_matches_dir\n",
        "        self.player_ids = []\n",
        "        self.player_match_data = {}  # Key: player_id, Value: DataFrame of that player's matches.\n",
        "        for player_id in self.univ_features.index:\n",
        "            match_file = os.path.join(player_matches_dir, f\"{player_id}.csv\")\n",
        "            if os.path.exists(match_file):\n",
        "                df_matches = pd.read_csv(match_file)\n",
        "                # Assume the matches in this file are already sorted by date.\n",
        "                if len(df_matches) >= (self.context_len + 1):\n",
        "                    self.player_ids.append(player_id)\n",
        "                    self.player_match_data[player_id] = df_matches\n",
        "\n",
        "        self.match_players_dir = match_players_dir\n",
        "\n",
        "        # Load the single match_info CSV and set match_id as index for fast lookup.\n",
        "        self.match_info_df = pd.read_csv(match_info_csv)\n",
        "        self.match_info_df = self.match_info_df.set_index('match_id')\n",
        "        # Optionally drop columns not needed.\n",
        "        self.match_info_df = self.match_info_df.drop(columns=['team1', 'team2', 'toss_winner', \"toss_decision\", \"winner\"], errors='ignore')\n",
        "        #Todo don't just drop them we need to infer from them in sense of player's team ->winner 1 else 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.player_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a dictionary with:\n",
        "          - 'player_id': the player's id.\n",
        "          - 'univ_features': universal features for the player.\n",
        "          - 'context_matches': a numpy array of performance features for the context matches.\n",
        "          - 'target_scores': a numpy array of target fantasy scores (for matches 2 ... context_len+1).\n",
        "          - 'team1_players': list (per match) of team1 players' universal features.\n",
        "          - 'team2_players': list (per match) of team2 players' universal features.\n",
        "          - 'match_info': list (per match) of match info dictionaries.\n",
        "        \"\"\"\n",
        "        #ToDO not just total_fantasy score, rather a weighted sum of error for batting_bowling fp's, fielding_fp with lesser weightage to the overall total_fp  \n",
        "        # Select the player.\n",
        "        player_id = self.player_ids[idx]\n",
        "        # Retrieve the player's universal features as a numpy array.\n",
        "        player_univ = self.univ_features.loc[player_id].values.astype(float)\n",
        "        \n",
        "        # Load this player's matches DataFrame from the pre-loaded dictionary.\n",
        "        df_matches = self.player_match_data[player_id]\n",
        "        total_matches = len(df_matches)\n",
        "        # Randomly select a contiguous window of (context_len + 1) matches.\n",
        "        start_idx = np.random.randint(1, total_matches - self.context_len+1)\n",
        "        window_df = df_matches.iloc[start_idx-1 : start_idx + self.context_len]\n",
        "\n",
        "        # For input, drop the unused columns.\n",
        "        exclude_cols = ['teamname', 'match_id',\n",
        "                        'strike_rate_fp', 'batting_fp', 'bowling_fp', 'fielding_fp', 'total_fp']\n",
        "        # Extract target fantasy scores (from column 'total_fp') for matches 2 ... context_len+1.\n",
        "        target_scores = window_df.iloc[1:self.context_len+1]['total_fp'].values.astype(float)\n",
        "        #Todo account for all sub_fp's (stage -2)\n",
        "        # Extract performance features for input matches.\n",
        "        context_matches = window_df.iloc[:self.context_len].drop(columns=exclude_cols, errors='ignore')\n",
        "\n",
        "        # For each match in the window, retrieve team players and match info.\n",
        "        team1_players_list = [] \n",
        "        team2_players_list = []\n",
        "        match_info_list = []\n",
        "        for _, match in window_df.iterrows():\n",
        "            match_id = match['match_id']\n",
        "            # Load match players file (expects a file named \"{match_id}.csv\").\n",
        "            match_players_file = os.path.join(self.match_players_dir, f\"{int(match_id)}.csv\")\n",
        "            match_players_df = pd.read_csv(match_players_file)\n",
        "            \n",
        "            # Determine the team of the sample's player in this match.\n",
        "            player_team_series = match_players_df[\n",
        "                match_players_df['player_id'] == player_id]['Team']\n",
        "            #? rather we can refer it from window_df, there in the excluded_cols same as match['team_name']\n",
        "            if not player_team_series.empty:\n",
        "                player_team = player_team_series.iloc[0]\n",
        "            else:\n",
        "                player_team = match_players_df.iloc[0]['Team'] #?why's this, though it won't execute\n",
        "            \n",
        "            # Split player_ids into two groups based on the player's team.\n",
        "            team1_ids = match_players_df[\n",
        "                match_players_df['Team'] == player_team]['player_id'].tolist()\n",
        "            #! we just need the team2 player_ids (we are omitting the intra-team interactions...)\n",
        "            team2_ids = match_players_df[\n",
        "                match_players_df['Team'] != player_team]['player_id'].tolist()\n",
        "\n",
        "            # Retrieve universal features for these players.\n",
        "            team1_features = self.univ_features.reindex(team1_ids).dropna().values.astype(float)\n",
        "            team2_features = self.univ_features.reindex(team2_ids).dropna().values.astype(float)\n",
        "            team1_players_list.append(team1_features)\n",
        "            team2_players_list.append(team2_features)\n",
        "            \n",
        "            # Retrieve match info using match_id from the single match_info DataFrame.\n",
        "            if match_id in self.match_info_df.index:\n",
        "                match_info_dict = self.match_info_df.loc[match_id].to_dict()\n",
        "            else:\n",
        "                match_info_dict = { }\n",
        "            match_info_list.append(match_info_dict)\n",
        "            #? wht's this \n",
        "\n",
        "        # --- START OF CHANGES: convert to properly shaped torch.Tensors ---\n",
        "        import torch\n",
        "\n",
        "        # universal features\n",
        "        univ_features = torch.tensor(player_univ, dtype=torch.float32)  # (feat_dim,)\n",
        "\n",
        "        # context matches\n",
        "        context_matches = torch.tensor(context_matches.values.astype(float),\n",
        "                                       dtype=torch.float32)  # (context_len, perf_dim)\n",
        "\n",
        "        # target scores\n",
        "        target_scores = torch.tensor(target_scores, dtype=torch.float32)  # (context_len,)\n",
        "\n",
        "        # team1 players: pad to max players across the window, then stack\n",
        "        max1 = max(arr.shape[0] for arr in team1_players_list)\n",
        "        feat_dim = team1_players_list[0].shape[1] if max1>0 else 0\n",
        "        padded1 = [\n",
        "            np.pad(arr, ((0, max1 - arr.shape[0]), (0, 0)), mode='constant')\n",
        "            for arr in team1_players_list\n",
        "        ]\n",
        "        team1_players = torch.tensor(np.stack(padded1), dtype=torch.float32)\n",
        "        # shape: (context_len+1, max1, feat_dim)\n",
        "\n",
        "        # team2 players: same\n",
        "        max2 = max(arr.shape[0] for arr in team2_players_list)\n",
        "        feat_dim2 = team2_players_list[0].shape[1] if max2>0 else 0\n",
        "        padded2 = [\n",
        "            np.pad(arr, ((0, max2 - arr.shape[0]), (0, 0)), mode='constant')\n",
        "            for arr in team2_players_list\n",
        "        ]\n",
        "        team2_players = torch.tensor(np.stack(padded2), dtype=torch.float32)\n",
        "        # shape: (context_len+1, max2, feat_dim2)\n",
        "        #? wht's this\n",
        "\n",
        "        # match_info: convert list of dicts to array in fixed key order\n",
        "        keys = list(self.match_info_df.columns)\n",
        "        info_arr = np.stack([[d.get(k, 0.0) for k in keys] for d in match_info_list])\n",
        "        match_info = torch.tensor(info_arr, dtype=torch.float32)\n",
        "        # shape: (context_len+1, len(keys))\n",
        "\n",
        "        # --- END OF CHANGES ---\n",
        "\n",
        "        sample = {\n",
        "            'player_id': player_id,\n",
        "            'univ_features': univ_features,\n",
        "            'context_matches': context_matches,\n",
        "            'target_scores': target_scores,\n",
        "            'team1_players': team1_players[:, :11, :],\n",
        "            'team2_players': team2_players[:, :11, :],\n",
        "            'match_info': match_info\n",
        "        }\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s-TlMJET2-Oq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define file paths (update these paths as needed for your folder structure)\n",
        "universal_player_csv = r'C:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\\Test_1\\cleaned_universal_player.csv'\n",
        "player_matches_dir = r'C:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\\Test_1\\Processed_Player_records'  # Contains files named like {player_id}.csv (each containing that player's matches)\n",
        "match_players_dir = r'C:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\\Test_1\\processed_GlobalMatchrecords'    # Contains files like {match_id}.csv\n",
        "match_info_csv = r'C:\\Users\\kumar\\IPL_Fantasy_Score_Prediction\\Ashu\\Test_1\\cleaned_matchinfo_without_venue_with_updated_match_number.csv'               # Single CSV containing all match info\n",
        "\n",
        "# Define the context length (number of matches to use as context)\n",
        "# For example, if config.CONTEXT_LEN is defined in your config module:\n",
        "# config.CONTEXT_LEN = 5\n",
        "\n",
        "# Initialize the dataset\n",
        "dataset = PlayerMatchDataset(\n",
        "    universal_player_csv=universal_player_csv,\n",
        "    player_matches_dir=player_matches_dir,  # This parameter may be ignored if you use the directory version\n",
        "    match_players_dir=match_players_dir,\n",
        "    match_info_csv=match_info_csv,\n",
        "    context_len=config.CONTEXT_LEN\n",
        ")\n",
        "\n",
        "# def get_shape(lst):\n",
        "#     shape = []\n",
        "#     while isinstance(lst, list):\n",
        "#         shape.append(len(lst))\n",
        "#         if len(lst) == 0:\n",
        "#             break\n",
        "#         lst = lst[0]\n",
        "#     return tuple(shape)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "\n",
        "# # Iterate through one batch to check the output shapes and values\n",
        "# for batch in dataloader:\n",
        "#     print(\"Player IDs:\", batch['player_id'])  # list of player_ids (length = batch_size)\n",
        "    \n",
        "#     print(\"Universal features shape:\", (batch['univ_features']).shape)  # e.g., (batch_size, num_features)\n",
        "    \n",
        "#     print(\"Context matches shape:\", batch['context_matches'].shape)  # e.g., (batch_size, context_len, performance_feature_dim)\n",
        "    \n",
        "#     print(\"Target scores shape:\", batch['target_scores'])      # e.g., (batch_size, context_len)\n",
        "    \n",
        "#     # The following are lists of length (context_len+1); each element is a numpy array.\n",
        "#     # team1_players_tensor = list_of_tensors_to_3d(batch['team1_players'])\n",
        "#     # team1_players_tensor = team1_players_tensor.squeeze(2)\n",
        "#     print(\"Number of matches in team1_players (per sample):\", batch['team1_players'].shape)\n",
        "    \n",
        "#     # team2_players_tensor = list_of_tensors_to_3d(batch['team2_players'])\n",
        "#     # team2_players_tensor = team2_players_tensor.squeeze(2)\n",
        "#     print(\"Number of matches in team2_players (per sample):\", batch['team2_players'].shape)\n",
        "    \n",
        "#     # match_info_tensor = list_of_dicts_to_tensor(batch['match_info'])\n",
        "#     print(\"Number of matches in match_info (per sample):\", (batch['match_info']).shape)\n",
        "#     break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2354"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Player IDs: ['1f28fdfd', 'a756e61a', 'eb0f2802', '07c8a2f4', 'c9c0fe50', '498aacac', '7d15170b', 'ea3ffd1b', 'ee1b6c27', 'c41043e6', '99b75528', 'c63671e9', '45ca12a7', '0ea1455a', 'dca8273d', 'fd96de7c', '025a092b', '9c7d7cda', '6a71ba3a', '6aed7e79', '9f701298', '42172c3c', '1747ea18', 'bb18be76', '4312e0fd', '6a685375', '39e9f0c9', '616a8149', '39ed0d2f', '85d8110a', '5b3acc53', 'c96311d4', 'd76ab352', '7c7d63a2', '69d03465', '7c503806', '2baad3bd', '1efb8a28', 'f56cd097', 'f3ca5af8', 'ccc04a61', '5b7ab5a9', '1798078c', 'b552a935', '1786a21b', '1c5c3dce', '4663bd23', 'a0d66374', 'b91083bd', '3957a70a', 'c31adb5c', '93fabc71', 'ade90de7', '85b3fab2', '8db7f47f', '5d60560f', '51a60c16', '97b0e4ff', '779b7f4d', '6cc4d322', 'b52ffbbd', 'cb461421', 'd7a57f75', '47b4ad09', '28131839', '0a67aec0', '1a156c88', '3812d56b', 'a83c44b8', '23638956', 'dc4686e6', '3a7ea906', '59792462', '57590dbb', 'a343262c', '14157abf', '2498e163', 'b8704508', '9ed95736', 'cdf59953', '11614d87', '7a8bd078', '7081848a', '0861a8d6', '063b3673', '7050a1e7', 'e412cb64', '596982e6', 'fe5b4367', '9d3940b3', '72275237', 'e7de4f6b', '8e6827f7', '59ddd811', '0f21680d', 'eeeebf82', '6a2fadd0', '02b93a83', '650d5e49', 'a1f1829d', 'a00e1842', '8088632c', '6ea2710f', 'f64eed39', '1ca51098', 'c374f37f', 'b67af1c4', 'c18cecbe', 'd465c6d7', '8d92a2c3', '1b7c4d69', '09a9d073', '5791b840', 'ff13b10c', 'f986ca1a', '7210d461', '0f72f582', '6834d1f2', 'b3bad3f2', '4025c262', 'e186f49c', 'fecd828e', 'b410bd3d', '6f4c73ce', '9dbc77b3', '52cb36a0', '2090e94d', '07ff32f5']\n",
            "Universal features shape: torch.Size([128, 25])\n",
            "Context matches shape: torch.Size([128, 16, 23])\n",
            "Target scores shape: torch.Size([128, 16])\n",
            "Number of matches in team1_players (per sample): torch.Size([128, 17, 11, 25])\n",
            "Number of matches in team2_players (per sample): torch.Size([128, 17, 11, 25])\n"
          ]
        }
      ],
      "source": [
        "for batch in dataloader:\n",
        "    print(\"Player IDs:\", batch['player_id'])  # list of player_ids (length = batch_size)\n",
        "    \n",
        "    print(\"Universal features shape:\", (batch['univ_features']).shape)  # e.g., (batch_size, num_features)\n",
        "    \n",
        "    print(\"Context matches shape:\", batch['context_matches'].shape)  # e.g., (batch_size, context_len, performance_feature_dim)\n",
        "    \n",
        "    print(\"Target scores shape:\", batch['target_scores'].shape)      # e.g., (batch_size, context_len)\n",
        "    \n",
        "    # The following are lists of length (context_len+1); each element is a numpy array.\n",
        "    # team1_players_tensor = list_of_tensors_to_3d(batch['team1_players'])\n",
        "    # team1_players_tensor = team1_players_tensor.squeeze(2)\n",
        "    print(\"Number of matches in team1_players (per sample):\", batch['team1_players'].shape)\n",
        "    \n",
        "    # team2_players_tensor = list_of_tensors_to_3d(batch['team2_players'])\n",
        "    # team2_players_tensor = team2_players_tensor.squeeze(2)\n",
        "    print(\"Number of matches in team2_players (per sample):\", batch['team2_players'].shape)\n",
        "    break\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_of_tensors_to_3d(tensor_list):\n",
        "    \"\"\"\n",
        "    Converts a list of tensors into a 3D tensor with shape:\n",
        "    (1, number_of_tensors, *inner_tensor_shape)\n",
        "    \n",
        "    Args:\n",
        "        tensor_list (list of torch.Tensor): List of tensors with identical shapes.\n",
        "        \n",
        "    Returns:\n",
        "        torch.Tensor: A tensor with the new shape (1, len(tensor_list), inner dims...).\n",
        "    \"\"\"\n",
        "    # First stack the tensors along dimension 0: shape becomes (number_of_tensors, inner dims...)\n",
        "    stacked = torch.stack(tensor_list, dim=0)\n",
        "    # Add a leading dimension to obtain the final shape (1, number_of_tensors, inner dims...)\n",
        "    return stacked.unsqueeze(0)\n",
        "\n",
        "\n",
        "def list_of_dicts_to_tensor(data, key_order=None):\n",
        "    \"\"\"\n",
        "    Convert a list of dictionaries (each with tensor or numeric values) \n",
        "    into a 3D tensor of shape (1, number_of_dicts, number_of_keys).\n",
        "\n",
        "    Args:\n",
        "        data (list): List of dictionaries where each dictionary contains the same keys.\n",
        "        key_order (list, optional): Specific order of keys to extract from each dictionary.\n",
        "                                    If None, keys from the first dictionary are used.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A 3D tensor with shape (1, len(data), len(key_order)).\n",
        "    \"\"\"\n",
        "    if not data:\n",
        "        raise ValueError(\"The input data list is empty.\")\n",
        "    \n",
        "    # Use keys from the first dictionary if no order is specified.\n",
        "    if key_order is None:\n",
        "        key_order = list(data[0].keys())\n",
        "    \n",
        "    values_list = []\n",
        "    for d in data:\n",
        "        # Extract values in the specified order. Convert tensor values to scalar if necessary.\n",
        "        values = []\n",
        "        for key in key_order:\n",
        "            value = d[key]\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                # Assuming tensor is of shape (1,)\n",
        "                values.append(value.item())\n",
        "            else:\n",
        "                values.append(value)\n",
        "        values_list.append(values)\n",
        "    \n",
        "    # Convert the list of lists to a 2D tensor.\n",
        "    tensor_2d = torch.tensor(values_list)\n",
        "    # Add a new dimension at the beginning to make it 3D.\n",
        "    tensor_3d = tensor_2d.unsqueeze(0)\n",
        "    return tensor_3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: c:\\Users\\umbar\\OneDrive\\Documents\\Fantasy Point Prediction\\IPL_Fantasy_Score_Prediction\\Ashu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Current working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C_p0nkp293u"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sJwwrG3n1nOp"
      },
      "outputs": [],
      "source": [
        "class PlayerEmbedding(nn.Module):\n",
        "  \"\"\" HEre we are doing the Proj of the raw Player embedding into the PERFORMANCE_EMBD_DIM \"\"\"\n",
        "  def __init__(self, in_channels=config.PLAYER_INPUT_DIM, out_channels=config.PERFORMANCE_EMBD_DIM):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(f\"In Player Embd  {x.shape}\")\n",
        "    # x: (B, PLAYER_INPUT_DIM) or flattened (B*T, PLAYER_INPUT_DIM)\n",
        "    return self.proj(x)   ## ( B/B*T, PERFORMANCE_EMBD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eHYYk7B4IDNF"
      },
      "outputs": [],
      "source": [
        "class MatchEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes a match-level embedding from team and match information.\n",
        "    Expected input shapes (for T matches):\n",
        "      team1_players: (B, T, num_team1, PLAYER_INPUT_DIM)\n",
        "      team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
        "      match_info: (B, T, match_info_dim)\n",
        "    Output:\n",
        "      (B, T, PERFORMANCE_EMBD_DIM)\n",
        "    \"\"\"\n",
        "    def __init__(self, player_embedding_module, in_channels=(2*config.PERFORMANCE_EMBD_DIM + config.MATCH_INPUT_EMBD), out_channels=config.PERFORMANCE_EMBD_DIM):\n",
        "        super().__init__()\n",
        "        self.player = player_embedding_module\n",
        "        self.proj = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, team1_players, team2_players, match_info):\n",
        "        \"\"\"\n",
        "          1. Get player embeddings using self.player.\n",
        "          2. Sum (or pool) embeddings for each team.\n",
        "          3. Concatenate team representations with match_info.\n",
        "          4. Project the concatenated vector to obtain the final match embedding.\n",
        "        \"\"\"\n",
        "        B, T, num_team1, _ = team1_players.shape\n",
        "        # print(f\"In match embd {team1_players.shape} & {match_info.shape}, B {B}, T, {T}, nums_team1 {num_team1}\")\n",
        "        # Compute player embeddings\n",
        "        team1_flat = team1_players.reshape(B * T, num_team1, -1)  # (B*T, num_team1, PLAYER_INPUT_DIM)\n",
        "        team1_embeds = self.player(team1_flat)  # (B*T, num_team1, PERFORMANCE_EMBD_DIM)\n",
        "        team1_sum = team1_embeds.sum(dim=1)  # (B*T, PERFORMANCE_EMBD_DIM)\n",
        "        team1_sum = team1_sum.reshape(B, T, -1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "\n",
        "        B, T, num_team2, _ = team2_players.shape\n",
        "        team2_flat = team2_players.reshape(B * T, num_team2, -1)\n",
        "        team2_embeds = self.player(team2_flat)  # (B*T, num_team2, PERFORMANCE_EMBD_DIM)\n",
        "        team2_sum = team2_embeds.sum(dim=1)  # (B*T, PERFORMANCE_EMBD_DIM)\n",
        "        team2_sum = team2_sum.reshape(B, T, -1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "\n",
        "        # Concatenate team summaries with match-level info along last dimension.\n",
        "        # match_info: (B, T, match_info_dim)\n",
        "        fused = torch.cat([team1_sum, team2_sum, match_info], dim=-1)  # (B, T, 2*PERFORMANCE_EMBD_DIM + match_info_dim)\n",
        "        match_embedding = self.proj(fused)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "\n",
        "        return match_embedding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UndNbo4bQ0Tr"
      },
      "outputs": [],
      "source": [
        "class PerformanceEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Combines a sequence of player embeddings with a sequence of match embeddings to produce a sequence\n",
        "    of performance embeddings. For each time step:\n",
        "      1. Obtain the player's embedding from player_input (B, T, PLAYER_INPUT_DIM).\n",
        "      2. Obtain the match embedding using the MatchEmbedding module, which now expects team-level\n",
        "         inputs with a time dimension (B, T, ...).\n",
        "      3. Concatenate these embeddings (+moresulting in a vector of dimension 2 * PERFORMANCE_EMBD_DIM).\n",
        "      4. Project the concatenated vector to PERFORMANCE_EMBD_DIM.\n",
        "\n",
        "    Expected input shapes:\n",
        "      player_input: (B, T, PLAYER_INPUT_DIM)\n",
        "      team1_players: (B, T, num_team1, PLAYER_INPUT_DIM)\n",
        "      team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
        "      match_info: (B, T, match_info_dim)\n",
        "\n",
        "    Output:\n",
        "      (B, T, PERFORMANCE_EMBD_DIM)\n",
        "    \"\"\"\n",
        "    def __init__(self, player_embedding_module, match_embedding_module, out_channels=config.PERFORMANCE_EMBD_DIM):\n",
        "        super().__init__()\n",
        "        self.player_embedding_module = player_embedding_module\n",
        "        self.match_embedding_module = match_embedding_module\n",
        "        self.performance_proj = nn.Linear(config.PERFORMANCE_INPUT_DIM, out_channels)\n",
        "        # Linear layer to map concatenated [player_emb; match_emb] (dimension 2*PERFORMANCE_EMBD_DIM)\n",
        "        # to PERFORMANCE_EMBD_DIM.\n",
        "        self.proj = nn.Linear(3 * config.PERFORMANCE_EMBD_DIM, out_channels)\n",
        "\n",
        "    def forward(self, player_input, player_performance_input, team1_players, team2_players, match_info):\n",
        "        \"\"\"\n",
        "        player_input: (B, T, PLAYER_INPUT_DIM) - raw features for a specific player across T matches.\n",
        "        team1_players: (B, T, num_team1, PLAYER_INPUT_DIM) - raw features for team1 players per match.\n",
        "        team2_players: (B, T, num_team2, PLAYER_INPUT_DIM) - raw features for team2 players per match.\n",
        "        match_info: (B, T, match_info_dim) - extra normalized match information per match.\n",
        "        \"\"\"\n",
        "        B, T, _ = player_performance_input.shape\n",
        "        # print(f\"In performance embedding {player_input.shape} && {player_performance_input.shape}\")\n",
        "        # Compute player's embedding for each match time step.\n",
        "        # Reshape to (B*T, PLAYER_INPUT_DIM) so that the player_embedding_module can be applied, then reshape back.\n",
        "        player_emb = self.player_embedding_module(player_input.reshape(B, -1))  # (B, PERFORMANCE_EMBD_DIM)\n",
        "        player_emb = player_emb.unsqueeze(1).repeat(1, T, 1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "        #!why we are repeating this.....\n",
        "        player_performance_emb = self.performance_proj(player_performance_input.reshape(B * T, -1))  # (B*T, PERFORMANCE_INPUT_DIM) => (B*T, PERFORMANCE_EMBD_DIM)\n",
        "        player_performance_emb = player_performance_emb.reshape(B, T, -1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "\n",
        "        # Compute match embedding across T time steps.\n",
        "        # Ensure that the match_embedding_module expects inputs with a time dimension.\n",
        "        match_emb = self.match_embedding_module(team1_players, team2_players, match_info)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "\n",
        "        # Concatenate the player's embedding and match embedding for each time step.\n",
        "        combined = torch.cat([player_emb, player_performance_emb, match_emb], dim=-1)  # (B, T, 3 * PERFORMANCE_EMBD_DIM)\n",
        "        #! what is player_emb, and player_performance_emb (diffence....)\n",
        "        # Project the concatenated vector back to PERFORMANCE_EMBD_DIM.\n",
        "        performance_emb = self.proj(combined)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "        #! performance_proj -> performance_embd_dim  and the combined with the match_ifo is also project to the performance_embd_dim \n",
        "        #TODO should we use another higher dimesion \n",
        "        return performance_emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc4qJqR9ZVCU"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n",
        "        super().__init__()\n",
        "        # This combines the Wq, Wk and Wv matrices into one matrix\n",
        "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
        "        # This one represents the Wo matrix\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "        self.n_heads = n_heads   ## how many heads u want ?\n",
        "        self.d_head = d_embed // n_heads   ## the original embedding get divided in the all heads equally\n",
        "   \n",
        "\n",
        "    def forward(self, x, causal_mask=False):\n",
        "\n",
        "        # x: # (Batch_Size, Seq_Len, Dim)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        input_shape = x.shape\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        batch_size, sequence_length, d_embed = input_shape\n",
        "\n",
        "        # (Batch_Size, Seq_Len, H, Dim / H)\n",
        "        qkv_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim * 3) -> 3 tensor of shape (Batch_Size, Seq_Len, Dim)\n",
        "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
        "        q = q.view(qkv_shape).transpose(1, 2)\n",
        "        k = k.view(qkv_shape).transpose(1, 2)\n",
        "        v = v.view(qkv_shape).transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Dim / H) @ (Batch_Size, H, Dim / H, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight = q @ k.transpose(-1, -2)\n",
        "\n",
        "        if causal_mask:\n",
        "            # Mask where the upper triangle (above the principal diagonal) is 1\n",
        "            mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
        "            # Fill the upper triangle with -inf\n",
        "            weight.masked_fill_(mask, -torch.inf)\n",
        "\n",
        "        # Divide by d_k (Dim / H).\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight /= math.sqrt(self.d_head)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) @ (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
        "        output = weight @ v\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, Seq_Len, H, Dim / H)\n",
        "        output = output.transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, Seq_Len, Dim)\n",
        "        output = output.reshape(input_shape)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XQaKP85sZU6l"
      },
      "outputs": [],
      "source": [
        "class a_layer(nn.Module):\n",
        "    \"\"\" A Single Transformer Layer/Block \"\"\"\n",
        "\n",
        "    def __init__(self, n_head: int, n_embd: int):\n",
        "        super().__init__()\n",
        "        # Pre-attention norm\n",
        "        self.layernorm_1 = nn.LayerNorm(n_embd)\n",
        "        # Self attention\n",
        "        self.attention = SelfAttention(n_head, n_embd)\n",
        "        # Pre-FNN norm\n",
        "        self.layernorm_2 = nn.LayerNorm(n_embd)\n",
        "        # Feedforward layer\n",
        "        self.linear_1 = nn.Linear(n_embd, 4 * n_embd)\n",
        "        self.linear_2 = nn.Linear(4 * n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        residue = x\n",
        "\n",
        "        ### SELF ATTENTION ###\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.layernorm_1(x)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.attention(x, causal_mask=True)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) + (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x += residue\n",
        "\n",
        "        ### FEEDFORWARD LAYER ###\n",
        "        # Apply a feedforward layer where the hidden dimension is 4 times the embedding dimension.\n",
        "\n",
        "        residue = x\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.layernorm_2(x)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, 4 * Dim)\n",
        "        x = self.linear_1(x)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, 4 * Dim) -> (Batch_Size, Seq_Len, 4 * Dim)\n",
        "        x = x * torch.sigmoid(1.702 * x)   # QuickGELU activation function found best for this work\n",
        "\n",
        "        # (Batch_Size, Seq_Len, 4 * Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.linear_2(x)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) + (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x += residue\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AcPxIGOTKMSz"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, d_embed=config.PERFORMANCE_EMBD_DIM, d_cross=config.PERFORMANCE_EMBD_DIM, n_heads=8, in_proj_bias=True, out_proj_bias=True):\n",
        "        super().__init__()\n",
        "        # Initialize linear layers for query, key, and value projections.\n",
        "        # q_proj: projects the query input with shape (batch_size, seq_length_q=1, d_embed)\n",
        "        self.q_proj   = nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n",
        "        # k_proj: projects the key input with shape (batch_size, seq_length_kv=11, d_cross)\n",
        "        self.k_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
        "        # v_proj: projects the value input with shape (batch_size, seq_length_kv=11, d_cross)\n",
        "        self.v_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
        "        # out_proj: projects the concatenated multi-head outputs back to the original embedding dimension.\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "        self.n_heads = n_heads                     # Number of attention heads.\n",
        "        self.d_head = d_embed // n_heads           # Dimensionality per head.\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Compute cross-attention between two sequences.\n",
        "\n",
        "        Parameters:\n",
        "            x (Tensor): Query input tensor of shape (batch_size, seq_length_q=1, d_embed)\n",
        "            y (Tensor): Key/Value input tensor of shape (batch_size, seq_length_kv=11, d_cross)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor after applying cross-attention, with shape (batch_size, seq_length_q=1, d_embed)\n",
        "        \"\"\"\n",
        "        # Save original input shape.\n",
        "        input_shape = x.shape  # (batch_size, seq_length_q=1, d_embed)\n",
        "        batch_size, sequence_length, d_embed = input_shape\n",
        "\n",
        "        # Define intermediate shape for splitting into multiple heads.\n",
        "        # New shape: (batch_size, seq_length, n_heads, d_head)\n",
        "        interim_shape = (batch_size, -1, self.n_heads, self.d_head)\n",
        "\n",
        "        # Apply linear projections:\n",
        "        # q: project queries from x.\n",
        "        q = self.q_proj(x)  # (batch_size, seq_length_q=1, d_embed)\n",
        "        # k: project keys from y.\n",
        "        k = self.k_proj(y)  # (batch_size, seq_length_kv=11, d_embed)\n",
        "        # v: project values from y.\n",
        "        v = self.v_proj(y)  # (batch_size, seq_length_kv=11, d_embed)\n",
        "\n",
        "        # Reshape projections to separate attention heads.\n",
        "        # Transform shape to (batch_size, n_heads, seq_length, d_head)\n",
        "        q = q.view(interim_shape).transpose(1, 2)\n",
        "        k = k.view(interim_shape).transpose(1, 2)\n",
        "        v = v.view(interim_shape).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores using scaled dot-product:\n",
        "        # Multiply queries with transposed keys.\n",
        "        # Resulting shape: (batch_size, n_heads, seq_length_q=1, seq_length_kv=11)\n",
        "        weight = q @ k.transpose(-1, -2)\n",
        "\n",
        "        # Scale the scores by the square root of head dimension to stabilize gradients.\n",
        "        weight /= math.sqrt(self.d_head)\n",
        "\n",
        "        # Normalize the attention weights using softmax along the key dimension.\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "\n",
        "        # Compute the weighted sum over the values.\n",
        "        # Output shape: (batch_size, n_heads, seq_length_q=1, d_head)\n",
        "        output = weight @ v\n",
        "\n",
        "        # Reorder dimensions to combine multiple heads:\n",
        "        # Transpose back to (batch_size, seq_length_q=1, n_heads, d_head) and make contiguous.\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "\n",
        "        # Merge the multi-head outputs to recover original embedding dimension.\n",
        "        # Final shape: (batch_size, seq_length_q=1, d_embed)\n",
        "        output = output.view(input_shape)\n",
        "\n",
        "        # Apply the final output projection.\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        return output  # Return the final cross-attention output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "XziRltvu3GM8",
        "outputId": "7b7a1af9-2edd-4e36-a48f-519c27a74e45"
      },
      "outputs": [],
      "source": [
        "class NextFormPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Predicts a sequence of performance embeddings autoregressively.\n",
        "    For loss, for each time step t (from 0 to T-2), the predicted performance embedding at t is used,\n",
        "    together with the ground-truth player and match embeddings at time t+1, to predict the fantasy score.\n",
        "    The loss is then computed over all such time steps by flattening the predictions and targets,\n",
        "    mimicking next-word prediction loss calculation.\n",
        "\n",
        "    Expected input shapes:\n",
        "      player_input: (B, PLAYER_INPUT_DIM)\n",
        "      player_performance_input: (B, T, PLAYER_INPUT_DIM)\n",
        "      team1_players: (B, T, num_team1, PLAYER_INPUT_DIM)\n",
        "      team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
        "      match_info: (B, T, match_info_dim)\n",
        "      target: (B, T) fantasy scores for matches 2..T\n",
        "    Output:\n",
        "      perf_emb: (B, T, PERFORMANCE_EMBD_DIM) -- the performance embedding sequence\n",
        "      loss: scalar loss computed over all predicted fantasy scores\n",
        "    \"\"\"\n",
        "    def __init__(self, player_embedding_module, match_embedding_module, fantasy_score_prediction_module, custom_loss, embedding_dim=config.PERFORMANCE_EMBD_DIM, num_layers=6, n_head=8):\n",
        "        super().__init__()\n",
        "        self.player_embedding_module = player_embedding_module\n",
        "        self.match_embedding = match_embedding_module(player_embedding_module)\n",
        "        self.token_embedding = PerformanceEmbedding(\n",
        "            player_embedding_module,\n",
        "            self.match_embedding\n",
        "        )       \n",
        "        # Positional embedding for T tokens (no CLS token)\n",
        "        # self.cross_attention_layers = nn.ModuleList([CrossAttention(n_heads, d_embed, d_cross) for _ in range(2)])\n",
        "        self.pos_embedding = nn.Embedding(config.CONTEXT_LEN, embedding_dim)\n",
        "        self.layers = nn.ModuleList([a_layer(n_head=n_head, n_embd=embedding_dim) for _ in range(num_layers)])\n",
        "        self.layernorm = nn.LayerNorm(embedding_dim)\n",
        "        self.out_proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.fantasy_score_prediction_module = fantasy_score_prediction_module\n",
        "        self.custom_loss = custom_loss\n",
        "\n",
        "    def forward(self, player_input, player_performance, team1_players, team2_players, match_info, target=None):\n",
        "        \"\"\"\n",
        "        player_input: (B, PLAYER_INPUT_DIM)\n",
        "        player_performance: (B, T, PLAYER_PERFORMANCE_INPUT)\n",
        "        team1_players: (B, T, num_team1, PLAYER_INPUT_DIM)\n",
        "        team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
        "        match_info: (B, T, match_info_dim)\n",
        "        target: (B, T-1) fantasy scores for matches 2..T + 1 (if provided) // it should be (B,T) .. 2-->T + 1\n",
        "        \"\"\"\n",
        "        B, T, _ = player_performance.shape  # T = context length\n",
        "        # team1_players = team1_players[:, :config.CONTEXT_LEN, :11, :]\n",
        "        # team2_players = team2_players[:, :config.CONTEXT_LEN, :11, :]\n",
        "        # Obtain performance embeddings sequence (B, T, embedding_dim)\n",
        "        # print(f\"In next form 1 team1 {team1_players[:, :config.CONTEXT_LEN, :, :].shape}, team2 {team2_players[:, :config.CONTEXT_LEN, :, :].shape}, \") \n",
        "        \n",
        "        x = self.token_embedding(player_input, player_performance, team1_players[:, :config.CONTEXT_LEN, :, :], team2_players[:, :config.CONTEXT_LEN, :, :], match_info[:, :config.CONTEXT_LEN, :])\n",
        "\n",
        "\n",
        "        # Add positional embeddings (using indices 0..T-1)\n",
        "        pos_ids = torch.arange(T, device=config.DEVICE).unsqueeze(0).expand(B, T)  # (B, T)\n",
        "        x = x + self.pos_embedding(pos_ids)\n",
        "\n",
        "        # Process through Transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.layernorm(x)\n",
        "        perf_emb = self.out_proj(x)  # (B, T, embedding_dim)\n",
        "\n",
        "        loss = None\n",
        "        if target is not None:\n",
        "            # Shift sequence: for each time step t (0...T-2), use predicted perf_emb at time t to predict\n",
        "            # the fantasy score for match t+1.\n",
        "            pred_perf = perf_emb[:, :, :]  # (B, T, embedding_dim)\n",
        "            Bp, Tp, D = pred_perf.shape\n",
        "            pred_perf = pred_perf.reshape(B * Tp, D)\n",
        "            \n",
        "            player_embd = self.player_embedding_module(player_input)  # (B, 25)\n",
        "            player_embd = player_embd.unsqueeze(1).repeat(1, Tp, 1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "            player_embd = player_embd.reshape(B * Tp, D)\n",
        "            \n",
        "            \n",
        "            # Get ground-truth match embedding for match t+1:\n",
        "            # print(f\"In next form 2 team1 {team1_players[:, 1:, :, :].shape}, team2 {team2_players[:, 1:, :, :].shape}\") \n",
        "            \n",
        "            target_match_embd = self.match_embedding(\n",
        "                team1_players[:, 1:, :, :],\n",
        "                team2_players[:, 1:, :, :],\n",
        "                match_info[:, 1:, :]\n",
        "            )  # (B, T-1, embedding_dim)\n",
        "            target_match_embd = target_match_embd.reshape(B * Tp, D)\n",
        "\n",
        "            # Predict fantasy scores from the predicted next form, combined with target player and match embeddings.\n",
        "            # fantasy_score_prediction_module should output shape (B, T-1, 1)\n",
        "            pred_fantasy = self.fantasy_score_prediction_module(pred_perf, player_embd, target_match_embd)\n",
        "\n",
        "            # Flatten predictions and targets to compute loss similarly to next-word prediction.\n",
        "            pred_fantasy_flat = pred_fantasy.reshape(-1)      # (B*(T-1),)\n",
        "            target_flat = target.reshape(-1)                  # (B*(T-1),)\n",
        "            loss = self.custom_loss(pred_fantasy_flat, target_flat)\n",
        "        return perf_emb, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmlcDyxbYic2"
      },
      "outputs": [],
      "source": [
        "# class NextFormPredictor(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Predicts a sequence of performance embeddings autoregressively.\n",
        "#     Then, for loss calculation, for each time step t (from 0 to T-2), it uses the predicted\n",
        "#     performance embedding at time t to predict the fantasy score for match t+1.\n",
        "#     The prediction is performed by combining:\n",
        "#       - The predicted performance embedding from time t (acting as the 'next form'),\n",
        "#       - The ground-truth player embedding for time t+1, and\n",
        "#       - The ground-truth match embedding for time t+1.\n",
        "#     These three are passed through an MLP (fantasy_score_prediction_module) to yield the predicted fantasy score.\n",
        "#     The loss is computed (e.g., via MSE) between these predictions and the provided target fantasy scores.\n",
        "\n",
        "#     Expected input shapes:\n",
        "\n",
        "\n",
        "\n",
        "#       player_input: (B, T, PLAYER_INPUT_DIM)\n",
        "\n",
        "#       team1_players: (B, T, num_team1, PLAYER_INPUT_DIM)\n",
        "#       team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
        "#       match_info: (B, T, match_info_dim)\n",
        "#       target: (B, T-1)  (fantasy scores for matches 2..T)\n",
        "#     Output:\n",
        "#       performance_embeddings: (B, T, PERFORMANCE_EMBD_DIM)\n",
        "#       loss: computed fantasy score prediction loss\n",
        "#     \"\"\"\n",
        "#     def __init__(self, player_embedding_module, match_embedding_module, fantasy_score_prediction_module, embedding_dim=config.PERFORMANCE_EMBD_DIM, num_layers=6, n_head=8):\n",
        "#         super().__init__()\n",
        "#         self.player_embedding_module = player_embedding_module\n",
        "#         self.match_embedding_module = match_embedding_module\n",
        "#         self.token_embedding = PerformanceEmbedding(player_embedding_module, match_embedding_module)\n",
        "#         self.pos_embedding = nn.Embedding(config.CONTEXT_LEN, embedding_dim)\n",
        "#         self.layers = nn.ModuleList([a_layer(n_head=n_head, n_embd=embedding_dim) for _ in range(num_layers)])\n",
        "#         self.layernorm = nn.LayerNorm(embedding_dim)\n",
        "#         self.out_proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "#         self.fantasy_score_prediction_module = fantasy_score_prediction_module\n",
        "\n",
        "#     def forward(self, player_input, team1_players, team2_players, match_info, target=None):\n",
        "#         \"\"\"\n",
        "#         Forward pass.\n",
        "#           player_input: (B, T, PLAYER_INPUT_DIM)\n",
        "#           team1_players: (B, T, num_team1, PLAYER_INPUT_DIM)\n",
        "#           team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
        "#           match_info: (B, T, match_info_dim)\n",
        "#           target: (B, T-1) fantasy scores for matches 2..T (if provided)\n",
        "#         \"\"\"\n",
        "#         B, T, _ = player_input.shape\n",
        "#         # Obtain a sequence of performance embeddings (B, T, embedding_dim)\n",
        "#         x = self.token_embedding(player_input, team1_players, team2_players, match_info)\n",
        "#         # Add positional embeddings: generate indices [0, 1, ..., T-1] and add\n",
        "#         pos_ids = torch.arange(T, device=player_input.device).unsqueeze(0).expand(B, T)\n",
        "#         x = x + self.pos_embedding(pos_ids)\n",
        "\n",
        "#         # Process sequence through transformer layers.\n",
        "#         for layer in self.layers:\n",
        "#             x = layer(x)\n",
        "#         x = self.layernorm(x)\n",
        "#         # Optionally, project outputs if needed:\n",
        "#         perf_emb = self.out_proj(x)  # (B, T, embedding_dim)\n",
        "\n",
        "#         loss = None\n",
        "#         if target is not None:\n",
        "#             # For autoregressive fantasy score prediction, we use the performance embedding at time t to predict\n",
        "#             # the fantasy score for match t+1. Therefore, we shift the sequence:\n",
        "#             pred_perf = perf_emb[:, :-1, :]  # (B, T-1, embedding_dim) predicted \"next form\" embeddings.\n",
        "\n",
        "#             # Get the ground truth player embedding for match t+1:\n",
        "#             target_player_embd = self.player_embedding_module(player_input[:, 1:, :])  # (B, T-1, embedding_dim)\n",
        "\n",
        "#             # Get the ground truth match embedding for match t+1:\n",
        "#             target_match_embd = self.match_embedding_module(\n",
        "#                 team1_players[:, 1:, :, :],\n",
        "#                 team2_players[:, 1:, :, :],\n",
        "#                 match_info[:, 1:, :]\n",
        "#             )  # (B, T-1, embedding_dim)\n",
        "\n",
        "#             # Predict fantasy scores from the predicted next form combined with the target player and match embeddings.\n",
        "#             # The fantasy score prediction module expects three tensors of shape (B, T-1, embedding_dim) and outputs (B, T-1, 1)\n",
        "#             pred_fantasy = self.fantasy_score_prediction_module(pred_perf, target_player_embd, target_match_embd)\n",
        "#             # Compute loss (e.g., MSE loss) between predicted fantasy scores and target.\n",
        "#             loss = F.mse_loss(pred_fantasy.squeeze(-1), target)\n",
        "#         return perf_emb, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VvX9Ucc3GJy"
      },
      "outputs": [],
      "source": [
        "# experiment with the dropout layers\n",
        "class FantasyScorePrediction(nn.Module):\n",
        "    \"\"\"Predict the Fantasy Score by the <CLS> embedding, the next match_embedding, and the player_embedding.\"\"\"\n",
        "    def __init__(self, embedding_dim=config.PERFORMANCE_EMBD_DIM):\n",
        "        super().__init__()\n",
        "        self.proj1 = nn.Linear(3 * config.PERFORMANCE_EMBD_DIM, 2048)\n",
        "        self.proj2 = nn.Linear(2048, 512)\n",
        "        self.proj3 = nn.Linear(512, 512)\n",
        "        self.proj4 = nn.Linear(512, 256)\n",
        "        self.proj5 = nn.Linear(256, 256)\n",
        "        self.proj6 = nn.Linear(256, 128)\n",
        "        self.proj7 = nn.Linear(128, 128)\n",
        "        self.proj8 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x, target_player_embd, target_match_embd): \n",
        "        # Concatenate the embeddings along the last dimension\n",
        "        # print(f\"NI fantasy {x.shape} & {target_player_embd.shape} & {target_match_embd.shape}\")\n",
        "        x = torch.cat([x, target_player_embd, target_match_embd], dim=-1)\n",
        "\n",
        "        x = F.gelu(self.proj1(x))\n",
        "        x = F.gelu(self.proj2(x))\n",
        "        x = F.gelu(self.proj3(x))\n",
        "        x = F.gelu(self.proj4(x))\n",
        "        x = F.gelu(self.proj5(x))\n",
        "        x = F.gelu(self.proj6(x))\n",
        "        x = F.gelu(self.proj7(x))\n",
        "\n",
        "\n",
        "        x = self.proj8(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Generalization: \n",
        "If your evaluation metric or deployment scenario is more \n",
        "sensitive to final outcomes rather than intermediate steps, training with a \n",
        "weighted loss that emphasizes these outcomes can enhance generalization.\n",
        "\"\"\"\n",
        "def custom_loss2(pred_fantasy,target,weights = None):\n",
        "    \"\"\" \n",
        "        Auto regressive loss over T time steps.\n",
        "        \n",
        "        Inputs:\n",
        "        - pred_fantasy: Predicted outputs of shape (B, T)\n",
        "        - target: Ground truth values of shape (B, T)\n",
        "        - weights : to each time step loss... (we can pre-compute them instead of computing them again -2)\n",
        "        \n",
        "        Output:\n",
        "        - total_loss: A scalar tensor representing the weighted MSE loss\n",
        "        \n",
        "        We use a piecewise linear weighting scheme:\n",
        "        - For t <= t0: weight = alpha\n",
        "        - For t >  t0: weight = alpha + (1 - alpha) * (t - t0) / (T - t0)\n",
        "        \n",
        "        This formulation emphasizes later time steps (after a certain context is available)\n",
        "        and can enhance generalization when final outcomes are more important.\n",
        "    \"\"\"\n",
        "    B,T,_ = pred_fantasy.shape # shape (B,T,)\n",
        "    alpha = 0.5\n",
        "    t0 = 5  #TODO for time steps less then t0 , loss will have weights 0.5\n",
        "    if weights is None:\n",
        "        weights = []\n",
        "        for t in range(1,T + 1):\n",
        "            if t <=t0:\n",
        "                weights.append(alpha)\n",
        "            else:\n",
        "                weight = alpha + (1 - alpha)*(t -t0)/(T - t0)\n",
        "                weights.append(weight)\n",
        "    weights = torch.tensor(weights,device=pred_fantasy.device,dtype=pred_fantasy.dtype)\n",
        "    weights = weights.unsqueeze(0) # shape is (1,T)\n",
        "    squared_error = (pred_fantasy - target)**2\n",
        "    weighted_error = squared_error*weights\n",
        "    loss = weighted_error.mean()\n",
        "    return loss\n",
        "        \n",
        "        \n",
        "\n",
        "class LogisticWeightedMSELoss(nn.Module):\n",
        "    def __init__(self,T:int,W_MIN = 0.25,W_MAX = 1,k= 0.5):\n",
        "        super(LogisticWeightedMSELoss,self).__init__()\n",
        "        t = np.arange(1,T + 1)\n",
        "        self.weights = W_MIN + (W_MAX - W_MIN)/ (1 + np.exp(-k * (t - T/2)))\n",
        "        self.weights =  torch.tensor(weights, dtype=torch.float32)\n",
        "        \n",
        "    def forward(self,pred_fantasy,targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pred_fantasy (torch.tensor): shape is (B,T)\n",
        "            targets (torch.tensor): shape is (B,T)\n",
        "        \"\"\"\n",
        "        squared_error = (pred_fantasy - targets)**2\n",
        "        weighted_errors = squared_error*self.weights\n",
        "        return weighted_errors\n",
        "    \n",
        "        \n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "grCh7qwMqBAD"
      },
      "outputs": [],
      "source": [
        "def custom_loss(pred_fantasy, target):\n",
        "    # \"\"\"\n",
        "    # Custom Upper-Lower Bound Loss for integer fantasy scores.\n",
        "\n",
        "    # For each example:\n",
        "    #   - Define a lower bound = target - 0.5\n",
        "    #   - Define an upper bound = target + 0.5\n",
        "    #   - If pred_fantasy is within [lower, upper], loss = 0\n",
        "    #   - If pred_fantasy < lower, loss = lower - pred_fantasy\n",
        "    #   - If pred_fantasy > upper, loss = pred_fantasy - upper\n",
        "\n",
        "    # Args:\n",
        "    #     pred_fantasy (Tensor): Predicted fantasy scores, shape (B, ...), floats.\n",
        "    #     target       (Tensor): Ground-truth fantasy scores, shape (B, ...), integers or floats.\n",
        "\n",
        "    # Returns:\n",
        "    #     Tensor: Scalar mean loss.\n",
        "    # \"\"\"\n",
        "    # # convert target to float tensor\n",
        "    # target = target.to(pred_fantasy.dtype)\n",
        "\n",
        "    # # compute bounds\n",
        "    # lower = target - 0.5\n",
        "    # upper = target + 0.5\n",
        "\n",
        "    # # zero tensor for comparison\n",
        "    # zeros = torch.zeros_like(pred_fantasy)\n",
        "\n",
        "    # # penalty for being below lower bound\n",
        "    # loss_low = torch.where(pred_fantasy < lower,\n",
        "    #                        lower - pred_fantasy,\n",
        "    #                        zeros)\n",
        "\n",
        "    # # penalty for being above upper bound\n",
        "    # loss_high = torch.where(pred_fantasy > upper,\n",
        "    #                         pred_fantasy - upper,\n",
        "    #                         zeros)\n",
        "\n",
        "    # # total loss per example\n",
        "    # loss = loss_low + loss_high\n",
        "\n",
        "    # return average\n",
        "    # return loss.mean()\n",
        "    \n",
        "    # print(f\"custom loss {pred_fantasy.shape} & {target.shape}\")\n",
        "    return nn.functional.mse_loss(pred_fantasy, target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "player_embedding_module = PlayerEmbedding(\n",
        "    in_channels=config.PLAYER_INPUT_DIM,\n",
        "    out_channels=config.PERFORMANCE_EMBD_DIM\n",
        ")\n",
        "\n",
        "# Note: NextFormPredictor expects the *class* MatchEmbedding so that inside it\n",
        "# it can do `match_embedding_module(player_embedding_module)`\n",
        "match_embedding_module = MatchEmbedding\n",
        "\n",
        "fantasy_score_prediction_module = FantasyScorePrediction(\n",
        "    embedding_dim=config.PERFORMANCE_EMBD_DIM\n",
        ")\n",
        "\n",
        "model = NextFormPredictor(\n",
        "    player_embedding_module=player_embedding_module,\n",
        "    match_embedding_module=match_embedding_module,\n",
        "    fantasy_score_prediction_module=fantasy_score_prediction_module,\n",
        "    custom_loss=custom_loss,\n",
        "    embedding_dim=config.PERFORMANCE_EMBD_DIM,\n",
        "    num_layers=6,\n",
        "    n_head=8\n",
        ").to(config.DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo9h_edb3G1C"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6k9yapLG3JyY"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "def train(model, dataloader, start_epoch=0) -> List[torch.Tensor]:\n",
        "    model.to(config.DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
        "    epoch_losses = []\n",
        "\n",
        "    for epoch in range(start_epoch, config.NUM_EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Wrap dataloader with tqdm for progress bar\n",
        "        loop = tqdm(dataloader, leave=True)\n",
        "        loop.set_description(f\"Epoch [{epoch+1}/{config.NUM_EPOCHS}]\")\n",
        "\n",
        "        for batch in loop:\n",
        "            # Move tensors to device\n",
        "            for k, v in batch.items():\n",
        "                if torch.is_tensor(v):\n",
        "                    batch[k] = v.to(config.DEVICE)\n",
        "\n",
        "            # Unpack batch\n",
        "            player_univ   = batch['univ_features']\n",
        "            context_feats = batch['context_matches']\n",
        "            team1_players = batch['team1_players']\n",
        "            team2_players = batch['team2_players']\n",
        "            match_info    = batch['match_info']\n",
        "            target_scores = batch['target_scores']\n",
        "            \n",
        "            # print(player_univ.shape)\n",
        "            # print(context_feats.shape)\n",
        "            # print(team1_players.shape)\n",
        "            # print(team2_players.shape)\n",
        "            # print(match_info.shape)\n",
        "            # print(target_scores.shape)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            perf_emb, loss = model(\n",
        "                player_univ,\n",
        "                context_feats,\n",
        "                team1_players,\n",
        "                team2_players,\n",
        "                match_info,\n",
        "                target_scores\n",
        "            )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Update tqdm progress bar with current loss\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        epoch_losses.append(avg_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config.NUM_EPOCHS} — Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return epoch_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/100]: 100%|██████████| 19/19 [02:19<00:00,  7.35s/it, loss=2.88e+3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 — Avg Loss: 3117.3702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [2/100]: 100%|██████████| 19/19 [01:54<00:00,  6.04s/it, loss=2.06e+3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/100 — Avg Loss: 2156.3024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [3/100]: 100%|██████████| 19/19 [06:25<00:00, 20.27s/it, loss=1.74e+3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/100 — Avg Loss: 1678.3442\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [4/100]: 100%|██████████| 19/19 [04:50<00:00, 15.29s/it, loss=1.73e+3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/100 — Avg Loss: 1636.9903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [5/100]:  32%|███▏      | 6/19 [01:33<03:07, 14.45s/it, loss=1.73e+3]"
          ]
        }
      ],
      "source": [
        "train(model, dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### IS CONTEXT_SIZE too big check how many players remain when we remove all the players having total matches less that of config.CONTEXT_LEN, check the input dim are as expected in class's forward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3cyUlh63LhQ"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"next_form_predictor.pth\")\n",
        "In match embd torch.Size([16, 16, 11, 25]) & torch.Size([16, 16, 14])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBst2VcF3L9C"
      },
      "source": [
        "## Evalutaion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImfSP6DS3K9C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGPWDOqR3PfK"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCODw7UP3POm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kC35Lrh3PLY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc5K2NOvXSlv",
        "outputId": "006e2c47-128c-441e-913a-2bb9da6d7d86"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer for a pretrained model (e.g., BERT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "text = \"The patient was diagnosed with myocarditis and cardiomyopathy.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "candidate_terms = []\n",
        "i = 0\n",
        "while i < len(tokens):\n",
        "    token = tokens[i]\n",
        "    # Check if the next token exists and starts with '##'\n",
        "    if i < len(tokens) - 1 and tokens[i+1].startswith(\"##\"):\n",
        "        # Start forming the candidate term from the current token\n",
        "        current_term = token\n",
        "        # Advance index and merge subsequent subword tokens\n",
        "        while i + 1 < len(tokens) and tokens[i+1].startswith(\"##\"):\n",
        "            i += 1\n",
        "            current_term += tokens[i][2:]\n",
        "        candidate_terms.append(current_term)\n",
        "    i += 1\n",
        "\n",
        "print(\"Candidate Terms:\", candidate_terms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4lpvZy4XXGo",
        "outputId": "7fbf36fd-f540-46f2-960a-9dd55d330fbc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from transformers import AutoTokenizer\n",
        "from nltk import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Tokenize and merge subword tokens to extract candidate unknown words.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "text = (\"The patient was diagnosed with myocarditis and cardiomyopathy also pheeiong simenowon. \"\n",
        "        \"The patient had chronic kidney disease and myocarditis symptoms along with the severe symptoms of the pheeiong simenowon.\")\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Extract candidate unknown words using subword merging.\n",
        "candidate_terms = []\n",
        "i = 0\n",
        "while i < len(tokens):\n",
        "    token = tokens[i]\n",
        "    if i < len(tokens) - 1 and tokens[i+1].startswith(\"##\"):\n",
        "        current_term = token\n",
        "        while i + 1 < len(tokens) and tokens[i+1].startswith(\"##\"):\n",
        "            i += 1\n",
        "            current_term += tokens[i][2:]\n",
        "        candidate_terms.append(current_term)\n",
        "    i += 1\n",
        "\n",
        "print(\"Candidate Terms from Subwords:\", candidate_terms)\n",
        "\n",
        "# Remove duplicates to get the unique set of new words.\n",
        "new_words = list(set(candidate_terms))\n",
        "print(\"Unique New Words:\", new_words)\n",
        "\n",
        "# Step 2: Re-examine the token stream to capture only those tokens (or merged tokens)\n",
        "# that are in the new words set, preserving their order of occurrence.\n",
        "unknown_sequence = []\n",
        "i = 0\n",
        "while i < len(tokens):\n",
        "    token = tokens[i]\n",
        "    # If token is the start of a merged word (has a following subword)\n",
        "    if i < len(tokens) - 1 and tokens[i+1].startswith(\"##\"):\n",
        "        current_term = token\n",
        "        while i + 1 < len(tokens) and tokens[i+1].startswith(\"##\"):\n",
        "            i += 1\n",
        "            current_term += tokens[i][2:]\n",
        "        # Append only if the merged token is one of the new words.\n",
        "        if current_term in new_words:\n",
        "            unknown_sequence.append(current_term)\n",
        "    else:\n",
        "        # For standalone tokens, check if they are in new words.\n",
        "        if token in new_words:\n",
        "            unknown_sequence.append(token)\n",
        "    i += 1\n",
        "\n",
        "print(\"Sequence of New Words in Order:\", unknown_sequence)\n",
        "\n",
        "# Step 3: Perform n-gram analysis on the sequence of new words.\n",
        "# This will help detect if 2 or 3 new words are repeatedly appearing together.\n",
        "# You can adjust the n-gram size and the frequency threshold as needed.\n",
        "for n in [2, 3]:\n",
        "    n_grams = list(ngrams(unknown_sequence, n))\n",
        "    ngram_counts = Counter(n_grams)\n",
        "    # For example, only consider phrases that appear at least twice.\n",
        "    frequent_ngrams = [' '.join(gram) for gram, count in ngram_counts.items() if count >= 2]\n",
        "    print(f\"Frequent {n}-grams among new words:\", frequent_ngrams)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyThyy25vYoX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
