{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The BASELINE model"
      ],
      "metadata": {
        "id": "-ckUw9JF0JC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "70CeiU1w0QDJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4YEOS3nzBVQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from typing import List\n",
        "from torch import Tensor\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "Ve_zbfbzfLo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    PLAYERS_SIZE: int = 0\n",
        "    CONTEXT_LEN: int = 20  ## predict the 21st match by the 20\n",
        "    PERFORMANCE_EMBD_DIM: int = 128\n",
        "    PLAYER_INPUT_DIM: int = 25\n",
        "    MATCH_INPUT_EMBD: int = 25\n",
        "\n",
        "    NUM_EPOCHS: int = 100\n",
        "    LEARNING_RATE: float = 1e-3\n",
        "    BATCH_SIZE: int = 16\n",
        "    DEVICE: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # MODEL_SAVE_PATH: str = \"/kaggle/working\"\n",
        "    # BASE_DIR: str = '/kaggle/input/flickr8k/'\n",
        "    # CROSSATT_NUM_HEADS: int = 8\n",
        "    CLS_INIT_STD: float = 0.02    ## <CLS> token initialized with std 0.02 from the mean=0\n",
        "    TEST_DATASET_SIZE: int = 180\n",
        "    IDLE_DEVICE: str = 'cpu'\n",
        "    ACCUMULATION_STEPS = 4\n",
        "\n",
        "    @property\n",
        "    # def transform(self):\n",
        "\n",
        "    @property\n",
        "    # def reverse_transform(self):"
      ],
      "metadata": {
        "id": "G9htvclHfODH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()"
      ],
      "metadata": {
        "id": "grAYxNeZSjLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defination of **Losses**"
      ],
      "metadata": {
        "id": "mFI4JDxB0VTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "62S8Cr5Tol_7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J0fTYTZmzgtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "48MZrwcv0luw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "1CO-J50z0mWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## INPUT\n",
        "# Universal Player embedding\n",
        "# Match Situation Embedding\n",
        "# Form embedding --> attention to be performed on"
      ],
      "metadata": {
        "id": "5XptNBgk0lr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-2Ivtrd-IDJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bJEzVOF7IDG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KF-_1qfF1mw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Processing (Normalization)"
      ],
      "metadata": {
        "id": "HMwXN5x41ycH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbmPp-tF1ms8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YM9gq79B169b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Consolidation"
      ],
      "metadata": {
        "id": "d8J9yahl18-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Concatation of the Input Embedding"
      ],
      "metadata": {
        "id": "6wWrj0-o1655"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eljnjnGk2VDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generated Training Sequences"
      ],
      "metadata": {
        "id": "HoCbNz0l2aP_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wFOJQcoY2Z6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create the DataLoader"
      ],
      "metadata": {
        "id": "ZB5JsauI219m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZZm6eBO2lsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s-TlMJET2-Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "5C_p0nkp293u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PlayerEmbedding(nn.Module):\n",
        "  \"\"\" HEre we are doing the Proj of the raw Player embedding into the PERFORMANCE_EMBD_DIM \"\"\"\n",
        "  def __init__(self, in_channels=config.PLAYER_INPUT_DIM, out_channels=config.PERFORMANCE_EMBD_DIM):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (B, PLAYER_INPUT_DIM) or flattened (B*T, PLAYER_INPUT_DIM)\n",
        "    return self.proj(x)   ## ( B/B*T, PERFORMANCE_EMBD)"
      ],
      "metadata": {
        "id": "sJwwrG3n1nOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MatchEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes a match-level embedding from team and match information.\n",
        "    Expected input shapes (for T matches):\n",
        "      team1_players: (B, T, num_team1, PLAYER_INPUT_DIM)\n",
        "      team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
        "      match_info: (B, T, match_info_dim)\n",
        "    Output:\n",
        "      (B, T, PERFORMANCE_EMBD_DIM)\n",
        "    \"\"\"\n",
        "    def __init__(self, player_embedding_module, in_channels=config.MATCH_INPUT_EMBD, out_channels=config.PERFORMANCE_EMBD_DIM):\n",
        "        super().__init__()\n",
        "        self.player = player_embedding_module\n",
        "        self.proj = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, team1_players, team2_players, match_info):\n",
        "        \"\"\"\n",
        "          1. Get player embeddings using self.player.\n",
        "          2. Sum (or pool) embeddings for each team.\n",
        "          3. Concatenate team representations with match_info.\n",
        "          4. Project the concatenated vector to obtain the final match embedding.\n",
        "        \"\"\"\n",
        "\n",
        "        B, T, num_team1, _ = team1_players.shape\n",
        "        # Compute player embeddings\n",
        "        team1_flat = team1_players.view(B * T, num_team1, -1)  # (B*T, num_team1, PLAYER_INPUT_DIM)\n",
        "        team1_embeds = self.player(team1_flat)  # (B*T, num_team1, PERFORMANCE_EMBD_DIM)\n",
        "        team1_sum = team1_embeds.sum(dim=1)  # (B*T, PERFORMANCE_EMBD_DIM)\n",
        "        team1_sum = team1_sum.view(B, T, -1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "\n",
        "        B, T, num_team2, _ = team2_players.shape\n",
        "        team2_flat = team2_players.view(B * T, num_team2, -1)\n",
        "        team2_embeds = self.player(team2_flat)  # (B*T, num_team2, PERFORMANCE_EMBD_DIM)\n",
        "        team2_sum = team2_embeds.sum(dim=1)  # (B*T, PERFORMANCE_EMBD_DIM)\n",
        "        team2_sum = team2_sum.view(B, T, -1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "\n",
        "        # Concatenate team summaries with match-level info along last dimension.\n",
        "        # match_info: (B, T, match_info_dim)\n",
        "        fused = torch.cat([team1_sum, team2_sum, match_info], dim=-1)  # (B, T, 2*PERFORMANCE_EMBD_DIM + match_info_dim)\n",
        "        match_embedding = self.proj(fused)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "\n",
        "        return match_embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "eHYYk7B4IDNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PerformanceEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Combines a sequence of player embeddings with a sequence of match embeddings to produce a sequence\n",
        "    of performance embeddings. For each time step:\n",
        "      1. Obtain the player's embedding from player_input (B, T, PLAYER_INPUT_DIM).\n",
        "      2. Obtain the match embedding using the MatchEmbedding module, which now expects team-level\n",
        "         inputs with a time dimension (B, T, ...).\n",
        "      3. Concatenate these embeddings (resulting in a vector of dimension 2 * PERFORMANCE_EMBD_DIM).\n",
        "      4. Project the concatenated vector to PERFORMANCE_EMBD_DIM.\n",
        "\n",
        "    Expected input shapes:\n",
        "      player_input: (B, T, PLAYER_INPUT_DIM)\n",
        "      team1_players: (B, T, num_team1, PLAYER_INPUT_DIM)\n",
        "      team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
        "      match_info: (B, T, match_info_dim)\n",
        "\n",
        "    Output:\n",
        "      (B, T, PERFORMANCE_EMBD_DIM)\n",
        "    \"\"\"\n",
        "    def __init__(self, player_embedding_module, match_embedding_module, out_channels=config.PERFORMANCE_EMBD_DIM):\n",
        "        super().__init__()\n",
        "        self.player_embedding_module = player_embedding_module\n",
        "        self.match_embedding_module = match_embedding_module\n",
        "        # Linear layer to map concatenated [player_emb; match_emb] (dimension 2*PERFORMANCE_EMBD_DIM)\n",
        "        # to PERFORMANCE_EMBD_DIM.\n",
        "        self.proj = nn.Linear(2 * config.PERFORMANCE_EMBD_DIM, out_channels)\n",
        "\n",
        "    def forward(self, player_input, team1_players, team2_players, match_info):\n",
        "        \"\"\"\n",
        "        player_input: (B, T, PLAYER_INPUT_DIM) - raw features for a specific player across T matches.\n",
        "        team1_players: (B, T, num_team1, PLAYER_INPUT_DIM) - raw features for team1 players per match.\n",
        "        team2_players: (B, T, num_team2, PLAYER_INPUT_DIM) - raw features for team2 players per match.\n",
        "        match_info: (B, T, match_info_dim) - extra normalized match information per match.\n",
        "        \"\"\"\n",
        "        B, T, _ = player_input.shape\n",
        "\n",
        "        # Compute player's embedding for each match time step.\n",
        "        # Reshape to (B*T, PLAYER_INPUT_DIM) so that the player_embedding_module can be applied, then reshape back.\n",
        "        player_emb = self.player_embedding_module(player_input.view(B * T, -1))  # (B*T, PERFORMANCE_EMBD_DIM)\n",
        "        player_emb = player_emb.view(B, T, -1)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "\n",
        "        # Compute match embedding across T time steps.\n",
        "        # Ensure that the match_embedding_module expects inputs with a time dimension.\n",
        "        match_emb = self.match_embedding_module(team1_players, team2_players, match_info)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "\n",
        "        # Concatenate the player's embedding and match embedding for each time step.\n",
        "        combined = torch.cat([player_emb, match_emb], dim=-1)  # (B, T, 2 * PERFORMANCE_EMBD_DIM)\n",
        "\n",
        "        # Project the concatenated vector back to PERFORMANCE_EMBD_DIM.\n",
        "        performance_emb = self.proj(combined)  # (B, T, PERFORMANCE_EMBD_DIM)\n",
        "        return performance_emb\n"
      ],
      "metadata": {
        "id": "UndNbo4bQ0Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n",
        "        super().__init__()\n",
        "        # This combines the Wq, Wk and Wv matrices into one matrix\n",
        "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
        "        # This one represents the Wo matrix\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "        self.n_heads = n_heads   ## how many heads u want ?\n",
        "        self.d_head = d_embed // n_heads   ## the original embedding get divided in the all heads equally\n",
        "\n",
        "\n",
        "    def forward(self, x, causal_mask=False):\n",
        "\n",
        "        # x: # (Batch_Size, Seq_Len, Dim)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        input_shape = x.shape\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        batch_size, sequence_length, d_embed = input_shape\n",
        "\n",
        "        # (Batch_Size, Seq_Len, H, Dim / H)\n",
        "        qkv_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim * 3) -> 3 tensor of shape (Batch_Size, Seq_Len, Dim)\n",
        "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
        "        q = q.view(qkv_shape).transpose(1, 2)\n",
        "        k = k.view(qkv_shape).transpose(1, 2)\n",
        "        v = v.view(qkv_shape).transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Dim / H) @ (Batch_Size, H, Dim / H, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight = q @ k.transpose(-1, -2)\n",
        "\n",
        "        if causal_mask:\n",
        "            # Mask where the upper triangle (above the principal diagonal) is 1\n",
        "            mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
        "            # Fill the upper triangle with -inf\n",
        "            weight.masked_fill_(mask, -torch.inf)\n",
        "\n",
        "        # Divide by d_k (Dim / H).\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight /= math.sqrt(self.d_head)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) @ (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
        "        output = weight @ v\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, Seq_Len, H, Dim / H)\n",
        "        output = output.transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, Seq_Len, Dim)\n",
        "        output = output.reshape(input_shape)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "mc4qJqR9ZVCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class a_layer(nn.Module):\n",
        "    \"\"\" A Single Transformer Layer/Block \"\"\"\n",
        "\n",
        "    def __init__(self, n_head: int, n_embd: int):\n",
        "        super().__init__()\n",
        "        # Pre-attention norm\n",
        "        self.layernorm_1 = nn.LayerNorm(n_embd)\n",
        "        # Self attention\n",
        "        self.attention = SelfAttention(n_head, n_embd)\n",
        "        # Pre-FNN norm\n",
        "        self.layernorm_2 = nn.LayerNorm(n_embd)\n",
        "        # Feedforward layer\n",
        "        self.linear_1 = nn.Linear(n_embd, 4 * n_embd)\n",
        "        self.linear_2 = nn.Linear(4 * n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        residue = x\n",
        "\n",
        "        ### SELF ATTENTION ###\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.layernorm_1(x)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.attention(x, causal_mask=True)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) + (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x += residue\n",
        "\n",
        "        ### FEEDFORWARD LAYER ###\n",
        "        # Apply a feedforward layer where the hidden dimension is 4 times the embedding dimension.\n",
        "\n",
        "        residue = x\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.layernorm_2(x)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, 4 * Dim)\n",
        "        x = self.linear_1(x)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, 4 * Dim) -> (Batch_Size, Seq_Len, 4 * Dim)\n",
        "        x = x * torch.sigmoid(1.702 * x)   # QuickGELU activation function found best for this work\n",
        "\n",
        "        # (Batch_Size, Seq_Len, 4 * Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.linear_2(x)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) + (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x += residue\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "XQaKP85sZU6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NextFormPredictor(nn.Module):\n",
        "    \"\"\" Predict the next Form embedding by previous Form embedding autoregressively with Transformer Decoder \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Processes a sequence of T performance embeddings (one per match) via a Transformer-based decoder.\n",
        "    A learnable [CLS] token is prepended and positional embeddings are added. The final [CLS] token output\n",
        "    serves as the aggregated representation.\n",
        "\n",
        "    Expected input shapes:\n",
        "      player_input: (B, T, PLAYER_INPUT_DIM)\n",
        "      team1_players: (B, T, num_team1, PLAYER_INPUT_DIM)\n",
        "      team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
        "      match_info: (B, T, match_info_dim)\n",
        "\n",
        "    Output:\n",
        "      (B, PERFORMANCE_EMBD_DIM)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, player_embedding_module, match_embedding_module, fantasy_score_prediction_module, custom_loss, embedding_dim=config.PERFORMANCE_EMBD_DIM):\n",
        "        super().__init__()\n",
        "        self.player_embedding_module = player_embedding_module\n",
        "        self.match_embedding_module = match_embedding_module\n",
        "        # cls_token = nn.Parameter(torch.normal(mean=0.0, std=config.CLS_INIT_STD, size=(1, 1, embedding_dim)))\n",
        "        self.token_embedding = PerformanceEmbedding(\n",
        "            player_embedding_module,\n",
        "            match_embedding_module(player_embedding_module)\n",
        "        )\n",
        "        self.pos_embedding = nn.Embedding(config.CONTEXT_LEN+1, embedding_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([a_layer(n_head=8, n_embd=embedding_dim) for _ in range(6)])\n",
        "        self.layernorm = nn.LayerNorm(embedding_dim)\n",
        "        self.out_proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.fantasy_score_prediction_module = fantasy_score_prediction_module\n",
        "        self.custom_loss = custom_loss\n",
        "\n",
        "    def forward(self, player_input, team1_players, team2_players, match_info, target=None):\n",
        "      \"\"\" here we are aussming that the each of the arg - player_input, team1&2_players ,macth_info have\n",
        "      size [batch, seq_len]\"\"\"\n",
        "      B, T, _ = player_input.shape     ## B=batch_size, T=Context_len\n",
        "\n",
        "      ## (B, T) => (B, T, embd)\n",
        "      x = self.token_embedding(player_input, team1_players, team2_players, match_info)\n",
        "\n",
        "      # cls_tokens = self.cls_token.expand(B, -1, -1)  ## (batch, 1, embeddign_dim)\n",
        "\n",
        "      # x = torch.cat((cls_tokens, x), dim=1)  ## (batch, T+1, embedding_dim)\n",
        "\n",
        "      x = x + self.pos_embedding(torch.arange(T, device=config.DEVICE))  ## (batch, T+1, embedding_dim)\n",
        "\n",
        "      for layer in self.layers:\n",
        "        x = layer(x)\n",
        "\n",
        "      x = self.layernorm(x)\n",
        "      perf_emb = self.out_proj(x)  # (B, T, embedding_dim)\n",
        "      # cls_val = x[:, 0, :]  ## (batch, embedding_dim) return the <CLS>\n",
        "\n",
        "      loss = None\n",
        "      if target is not None:\n",
        "          # For autoregressive fantasy score prediction, we use the performance embedding at time t to predict\n",
        "          # the fantasy score for match t+1. Therefore, we shift the sequence:\n",
        "          pred_perf = perf_emb[:, :-1, :]  # (B, T-1, embedding_dim) predicted \"next form\" embeddings.\n",
        "\n",
        "          # Get the ground truth player embedding for match t+1:\n",
        "          target_player_embd = self.player_embedding_module(player_input[:, 1:, :])  # (B, T-1, embedding_dim)\n",
        "\n",
        "          # Get the ground truth match embedding for match t+1:\n",
        "          target_match_embd = self.match_embedding_module(\n",
        "              team1_players[:, 1:, :, :],\n",
        "              team2_players[:, 1:, :, :],\n",
        "              match_info[:, 1:, :]\n",
        "          )  # (B, T-1, embedding_dim)\n",
        "\n",
        "          # Predict fantasy scores from the predicted next form combined with the target player and match embeddings.\n",
        "          # The fantasy score prediction module expects three tensors of shape (B, T-1, embedding_dim) and outputs (B, T-1, 1)\n",
        "          pred_fantasy = self.fantasy_score_prediction_module(pred_perf, target_player_embd, target_match_embd)\n",
        "          # Compute loss (e.g., MSE loss) between predicted fantasy scores and target.\n",
        "          loss = self.custom_loss(pred_fantasy.squeeze(-1), target)\n",
        "      return perf_emb, loss\n"
      ],
      "metadata": {
        "id": "XziRltvu3GM8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "7b7a1af9-2edd-4e36-a48f-519c27a74e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4c267ca5dea3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNextFormPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\" Predict the next Form embedding by previous Form embedding autoregressively with Transformer Decoder \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_embedding_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_embedding_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPERFORMANCE_EMBD_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class NextFormPredictor(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Predicts a sequence of performance embeddings autoregressively.\n",
        "#     Then, for loss calculation, for each time step t (from 0 to T-2), it uses the predicted\n",
        "#     performance embedding at time t to predict the fantasy score for match t+1.\n",
        "#     The prediction is performed by combining:\n",
        "#       - The predicted performance embedding from time t (acting as the 'next form'),\n",
        "#       - The ground-truth player embedding for time t+1, and\n",
        "#       - The ground-truth match embedding for time t+1.\n",
        "#     These three are passed through an MLP (fantasy_score_prediction_module) to yield the predicted fantasy score.\n",
        "#     The loss is computed (e.g., via MSE) between these predictions and the provided target fantasy scores.\n",
        "\n",
        "#     Expected input shapes:\n",
        "#       player_input: (B, T, PLAYER_INPUT_DIM)\n",
        "#       team1_players: (B, T, num_team1, PLAYER_INPUT_DIM)\n",
        "#       team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
        "#       match_info: (B, T, match_info_dim)\n",
        "#       target: (B, T-1)  (fantasy scores for matches 2..T)\n",
        "#     Output:\n",
        "#       performance_embeddings: (B, T, PERFORMANCE_EMBD_DIM)\n",
        "#       loss: computed fantasy score prediction loss\n",
        "#     \"\"\"\n",
        "#     def __init__(self, player_embedding_module, match_embedding_module, fantasy_score_prediction_module, embedding_dim=config.PERFORMANCE_EMBD_DIM, num_layers=6, n_head=8):\n",
        "#         super().__init__()\n",
        "#         self.player_embedding_module = player_embedding_module\n",
        "#         self.match_embedding_module = match_embedding_module\n",
        "#         self.token_embedding = PerformanceEmbedding(player_embedding_module, match_embedding_module)\n",
        "#         self.pos_embedding = nn.Embedding(config.CONTEXT_LEN, embedding_dim)\n",
        "#         self.layers = nn.ModuleList([a_layer(n_head=n_head, n_embd=embedding_dim) for _ in range(num_layers)])\n",
        "#         self.layernorm = nn.LayerNorm(embedding_dim)\n",
        "#         self.out_proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "#         self.fantasy_score_prediction_module = fantasy_score_prediction_module\n",
        "\n",
        "#     def forward(self, player_input, team1_players, team2_players, match_info, target=None):\n",
        "#         \"\"\"\n",
        "#         Forward pass.\n",
        "#           player_input: (B, T, PLAYER_INPUT_DIM)\n",
        "#           team1_players: (B, T, num_team1, PLAYER_INPUT_DIM)\n",
        "#           team2_players: (B, T, num_team2, PLAYER_INPUT_DIM)\n",
        "#           match_info: (B, T, match_info_dim)\n",
        "#           target: (B, T-1) fantasy scores for matches 2..T (if provided)\n",
        "#         \"\"\"\n",
        "#         B, T, _ = player_input.shape\n",
        "#         # Obtain a sequence of performance embeddings (B, T, embedding_dim)\n",
        "#         x = self.token_embedding(player_input, team1_players, team2_players, match_info)\n",
        "#         # Add positional embeddings: generate indices [0, 1, ..., T-1] and add\n",
        "#         pos_ids = torch.arange(T, device=player_input.device).unsqueeze(0).expand(B, T)\n",
        "#         x = x + self.pos_embedding(pos_ids)\n",
        "\n",
        "#         # Process sequence through transformer layers.\n",
        "#         for layer in self.layers:\n",
        "#             x = layer(x)\n",
        "#         x = self.layernorm(x)\n",
        "#         # Optionally, project outputs if needed:\n",
        "#         perf_emb = self.out_proj(x)  # (B, T, embedding_dim)\n",
        "\n",
        "#         loss = None\n",
        "#         if target is not None:\n",
        "#             # For autoregressive fantasy score prediction, we use the performance embedding at time t to predict\n",
        "#             # the fantasy score for match t+1. Therefore, we shift the sequence:\n",
        "#             pred_perf = perf_emb[:, :-1, :]  # (B, T-1, embedding_dim) predicted \"next form\" embeddings.\n",
        "\n",
        "#             # Get the ground truth player embedding for match t+1:\n",
        "#             target_player_embd = self.player_embedding_module(player_input[:, 1:, :])  # (B, T-1, embedding_dim)\n",
        "\n",
        "#             # Get the ground truth match embedding for match t+1:\n",
        "#             target_match_embd = self.match_embedding_module(\n",
        "#                 team1_players[:, 1:, :, :],\n",
        "#                 team2_players[:, 1:, :, :],\n",
        "#                 match_info[:, 1:, :]\n",
        "#             )  # (B, T-1, embedding_dim)\n",
        "\n",
        "#             # Predict fantasy scores from the predicted next form combined with the target player and match embeddings.\n",
        "#             # The fantasy score prediction module expects three tensors of shape (B, T-1, embedding_dim) and outputs (B, T-1, 1)\n",
        "#             pred_fantasy = self.fantasy_score_prediction_module(pred_perf, target_player_embd, target_match_embd)\n",
        "#             # Compute loss (e.g., MSE loss) between predicted fantasy scores and target.\n",
        "#             loss = F.mse_loss(pred_fantasy.squeeze(-1), target)\n",
        "#         return perf_emb, loss\n"
      ],
      "metadata": {
        "id": "UmlcDyxbYic2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment with the dropout layers\n",
        "class FantasyScorePrediction(nn.Module):\n",
        "    \"\"\"Predict the Fantasy Score by the <CLS> embedding, the next match_embedding, and the player_embedding.\"\"\"\n",
        "    def __init__(self, embedding_dim=config.PERFORMANCE_EMBD_DIM):\n",
        "        super().__init__()\n",
        "        self.proj1 = nn.Linear(3 * config.PERFORMANCE_EMBD_DIM, 2048)\n",
        "        self.proj2 = nn.Linear(2048, 512)\n",
        "        self.proj3 = nn.Linear(512, 512)\n",
        "        self.proj4 = nn.Linear(512, 256)\n",
        "        self.proj5 = nn.Linear(256, 256)\n",
        "        self.proj6 = nn.Linear(256, 128)\n",
        "        self.proj7 = nn.Linear(128, 128)\n",
        "        self.proj8 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x, target_player_embd, target_match_embd):  # x = <CLS> token\n",
        "        # Concatenate the embeddings along the last dimension\n",
        "        x = torch.cat([x, target_player_embd, target_match_embd], dim=-1)\n",
        "\n",
        "        x = F.gelu(self.proj1(x))\n",
        "        x = F.gelu(self.proj2(x))\n",
        "        x = F.gelu(self.proj3(x))\n",
        "        x = F.gelu(self.proj4(x))\n",
        "        x = F.gelu(self.proj5(x))\n",
        "        x = F.gelu(self.proj6(x))\n",
        "        x = F.gelu(self.proj7(x))\n",
        "\n",
        "\n",
        "        x = self.proj8(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "-VvX9Ucc3GJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(pred_fantasy, target):\n",
        "    \"\"\" Write the code of the our custom Upper-Lower Bound Loss function here properly ... \"\"\"\n",
        ""
      ],
      "metadata": {
        "id": "grCh7qwMqBAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Vo9h_edb3G1C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6k9yapLG3JyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I3cyUlh63LhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evalutaion"
      ],
      "metadata": {
        "id": "tBst2VcF3L9C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ImfSP6DS3K9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "fGPWDOqR3PfK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sCODw7UP3POm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6kC35Lrh3PLY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}